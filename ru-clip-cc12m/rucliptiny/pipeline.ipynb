{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm import create_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import transformers\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer,\\\n",
    "        get_linear_schedule_with_warmup\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        tokenizer_load = \"DeepPavlov/distilrubert-tiny-cased-conversational-v1\"\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(tokenizer_load)\n",
    "\n",
    "    def tokenize(self, texts, max_len=77):\n",
    "        tokenized = self.tokenizer.batch_encode_plus(texts,\n",
    "                                                     truncation=True,\n",
    "                                                     add_special_tokens=True,\n",
    "                                                     max_length=max_len,\n",
    "                                                     padding='max_length',\n",
    "                                                     return_attention_mask=True,\n",
    "                                                     return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        _convert_image_to_rgb,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]), ])\n",
    "\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuCLIPTinyDataset(Dataset):\n",
    "    def __init__(self, dir, df_path, max_text_len=77):\n",
    "        self.df = pd.read_csv(df_path)\n",
    "        self.dir = dir\n",
    "        self.max_text_len = max_text_len\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.transform = get_transform()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # достаем имя изображения и ее лейбл\n",
    "        image_name = self.df['image_name'].iloc[idx]\n",
    "        text = self.df['text'].iloc[idx]\n",
    "        tokens = self.tokenizer.tokenize([text], max_len=self.max_text_len)\n",
    "        input_ids, attention_mask = tokens[0][0], tokens[1][0]\n",
    "        image = cv2.imread(os.path.join(self.dir, image_name))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        image = self.transform(image)\n",
    "        return image, input_ids, attention_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuCLIPtiny(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.visual = create_model('convnext_tiny',\n",
    "                                   pretrained=False,\n",
    "                                   num_classes=0,\n",
    "                                   in_chans=3)  # out 768\n",
    "        text_config = DistilBertConfig(**{\"vocab_size\": 30522,\n",
    "                                          \"max_position_embeddings\": 512,\n",
    "                                          \"n_layers\": 3,\n",
    "                                          \"n_heads\": 12,\n",
    "                                          \"dim\": 264,\n",
    "                                          \"hidden_dim\": 792,\n",
    "                                          \"model_type\": \"distilbert\"})\n",
    "        self.transformer = DistilBertModel(text_config)\n",
    "        self.final_ln = torch.nn.Linear(264, 768)\n",
    "        self.logit_scale = torch.nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.stem[0].weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        x = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = x.last_hidden_state[:, 0, :]\n",
    "        x = self.final_ln(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(input_ids, attention_mask)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, train_dataframe, train_dir,\n",
    "                 val_dataframe=None, val_dir=None, learning_rate=1e-4,\n",
    "                 freeze_image_encoder=True, freeze_text_encoder=False, max_text_len=77,\n",
    "                 train_batch_size=64, val_batch_size=64, num_workers=2,\n",
    "                 weight_decay=1e-4, grad_accum=8):\n",
    "        self.train_dataframe = train_dataframe\n",
    "        self.train_dir = train_dir\n",
    "        self.val_dataframe = val_dataframe\n",
    "        self.val_dir = val_dir\n",
    "        self.learning_rate = learning_rate\n",
    "        self.freeze_image_encoder = freeze_image_encoder\n",
    "        self.freeze_text_encoder = freeze_text_encoder\n",
    "        self.max_text_len = max_text_len\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.weight_decay = weight_decay\n",
    "        self.grad_accum = grad_accum\n",
    "        print(f\"train batch size = {self.train_batch_size * self.grad_accum}\")\n",
    "\n",
    "    def train_model(self, model, epochs_num=1, device='cuda', verbose=10):\n",
    "\n",
    "        is_val = self.val_dataframe is not None and self.val_dir is not None\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        train_dataset = RuCLIPTinyDataset(self.train_dir, self.train_dataframe, self.max_text_len)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                    batch_size=self.train_batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=self.num_workers)\n",
    "\n",
    "        if is_val:\n",
    "            val_dataset = RuCLIPTinyDataset(self.val_dir, self.val_dataframe, self.max_text_len)\n",
    "            val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                                     batch_size=self.val_batch_size,\n",
    "                                                     shuffle=False,\n",
    "                                                     pin_memory=True,\n",
    "                                                     num_workers=self.num_workers)\n",
    "\n",
    "        for i, child in enumerate(model.children()):\n",
    "            if (i == 0 and self.freeze_image_encoder) or (i == 1 and self.freeze_text_encoder):\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        loss_img = torch.nn.CrossEntropyLoss()\n",
    "        loss_txt = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.learning_rate, betas=(0.9, 0.98), eps=1e-8,\n",
    "                                          weight_decay=self.weight_decay)\n",
    "        total_steps = len(train_loader) * epochs_num\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                        num_warmup_steps=0,\n",
    "                                                        num_training_steps=total_steps)\n",
    "        for epoch in range(epochs_num):\n",
    "            model.train()\n",
    "            print(f'start training epoch {epoch}')\n",
    "            curr_batch = 0\n",
    "            X = []\n",
    "            Y = []\n",
    "            curr_batch = 0\n",
    "            for i, batch in enumerate(tqdm(train_loader)):\n",
    "                images = batch[0].cuda()\n",
    "                input_ids = batch[1].cuda()\n",
    "                attention_mask = batch[2].cuda()\n",
    "\n",
    "                image_features = model.encode_image(images)\n",
    "                text_features = model.encode_text(input_ids, attention_mask)\n",
    "\n",
    "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "                text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                X.append(image_features)\n",
    "                Y.append(text_features)\n",
    "\n",
    "                if ((i + 1) % self.grad_accum == 0) or (i + 1 == len(train_loader)):\n",
    "                    optimizer.zero_grad()\n",
    "                    X = torch.cat(X, axis=0).cuda()\n",
    "                    Y = torch.cat(Y, axis=0).cuda()\n",
    "                    logit_scale = model.logit_scale.exp()\n",
    "                    logits_per_image = logit_scale * X @ Y.t()\n",
    "                    logits_per_text = logits_per_image.t()\n",
    "                    ground_truth = torch.arange(X.shape[0], dtype=torch.long).cuda()\n",
    "                    img_l = loss_img(logits_per_image, ground_truth)\n",
    "                    text_l = loss_txt(logits_per_text, ground_truth)\n",
    "                    total_loss = (img_l + text_l) / 2\n",
    "                    if curr_batch % verbose == 0:\n",
    "                        print(f'{i}/{len(train_loader)} total_loss {total_loss}')\n",
    "                    total_loss.backward()   \n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    \n",
    "                    X = []\n",
    "                    Y = []\n",
    "                    curr_batch += 1\n",
    "            if is_val:\n",
    "                print(f'start val epoch {epoch}')\n",
    "                total_loss = 0\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for i, batch in enumerate(tqdm(val_loader)):\n",
    "                        images = batch[0].to(device)\n",
    "                        input_ids = batch[1].to(device)\n",
    "                        attention_mask = batch[2].to(device)\n",
    "\n",
    "                        logits_per_image, logits_per_text = model(images, input_ids, attention_mask)\n",
    "                        ground_truth = torch.arange(batch[1].shape[0], dtype=torch.long).to(device)\n",
    "                        img_l = loss_img(logits_per_image, ground_truth).item()\n",
    "                        text_l = loss_txt(logits_per_text, ground_truth).item()\n",
    "                        total_loss += (img_l + text_l) / 2\n",
    "                    print(f'val loss = {total_loss / len(val_loader)}')\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.transform = get_transform()\n",
    "\n",
    "    def prepare_images_features(self, model, images_path, device='cpu'):\n",
    "        images_features = []\n",
    "        for image_path in images_path:\n",
    "            image = Image.open(image_path)\n",
    "            image = self.transform(image)\n",
    "            with torch.no_grad():\n",
    "                image_features = model.encode_image(image.unsqueeze(0).to(device)).float().cpu()[0]\n",
    "            images_features.append(image_features)\n",
    "        images_features = torch.stack(images_features, axis=0)\n",
    "        images_features /= images_features.norm(dim=-1, keepdim=True)\n",
    "        return images_features.cpu()\n",
    "\n",
    "    def prepare_text_features(self, model, texts, max_len=77, device='cpu'):\n",
    "        texts_features = []\n",
    "        for text in texts:\n",
    "            tokens = self.tokenizer.tokenize([text], max_len)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(tokens[0].to(device), tokens[1].to(device)).float().cpu()[0]\n",
    "            texts_features.append(text_features)\n",
    "        texts_features = torch.stack(texts_features, axis=0)\n",
    "        texts_features /= texts_features.norm(dim=-1, keepdim=True)\n",
    "        return texts_features\n",
    "\n",
    "    def __call__(self, model, images_path, classes, get_probs=False, max_len=77, device='cpu'):\n",
    "        model.eval().to(device)\n",
    "        image_features = self.prepare_images_features(model, images_path, device)\n",
    "        texts_features = self.prepare_text_features(model, classes, max_len, device)\n",
    "        text_probs = (1 * image_features @ texts_features.T).softmax(dim=-1)\n",
    "        if get_probs:\n",
    "            return text_probs\n",
    "        else:\n",
    "            return text_probs.argmax(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_second",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

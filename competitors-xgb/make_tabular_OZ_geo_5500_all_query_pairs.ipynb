{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5iUcYdz5nAl"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz4PYdr05nAm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import requests\n",
        "import os\n",
        "\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "from datetime import date, timedelta\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from typing import List, Tuple\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import math\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "# import optuna\n",
        "\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yU41zzbg5nAn"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import ruclip\n",
        "except ModuleNotFoundError:\n",
        "    !pip install git+https://github.com/tony-pitchblack/ru-clip.git#egg=ruclip\n",
        "    import ruclip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dr3wtFSy5nAo"
      },
      "outputs": [],
      "source": [
        "import ruclip\n",
        "\n",
        "clip, processor = ruclip.load('ruclip-vit-base-patch32-384')\n",
        "sbert = SentenceTransformer('all-distilroberta-v1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnhX3Ll45nAp"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWwzkaUK5nAq"
      },
      "outputs": [],
      "source": [
        "IMG_DATASET_NAME = 'images_OZ_geo_5500'\n",
        "TABLE_DATASET_DIR = 'tables_OZ_geo_5500'\n",
        "TABLE_DATASET_FILES= [\n",
        "    'Ozon_Crawler_Latest_info2025-04-07-12-57-51.xlsx',\n",
        "    'Карты мира_озон.xlsx'\n",
        "]\n",
        "\n",
        "SUBSET_SIZE = None\n",
        "# SUBSET_SIZE = 1\n",
        "\n",
        "DATA_PATH = 'data'\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azE6qnUc5nAq"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh3ebtsU6hcf"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import dotenv\n",
        "except ImportError:\n",
        "    !pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSbSdBoj6kK_"
      },
      "outputs": [],
      "source": [
        "# Use tokens from .env\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import huggingface_hub\n",
        "import wandb\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "huggingface_hub.login(token=HF_TOKEN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gdeq0Llb5nAq"
      },
      "outputs": [],
      "source": [
        "# Download models' weights & text/image datasets\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_ID = \"INDEEPA/clip-siamese\"\n",
        "LOCAL_DIR = Path(\"data/train_results\")\n",
        "LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "snapshot_download(\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type='dataset',\n",
        "    local_dir='data',\n",
        "    allow_patterns=[\n",
        "        # \"train_results/siamese_fitted*.pt\",\n",
        "        *[str(Path(TABLE_DATASET_DIR) / file_name) for file_name in TABLE_DATASET_FILES],\n",
        "        f\"{IMG_DATASET_NAME}.zip\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "!unzip -n -q data/{IMG_DATASET_NAME}.zip -d data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKzV07Mg5nAr"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gvHqtDm5nAr"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = 'data'\n",
        "\n",
        "file_path = (\n",
        "    Path(DATA_PATH) /\n",
        "    Path('tables_OZ_geo_5500') /\n",
        "    'Ozon_Crawler_Latest_info2025-04-07-12-57-51.xlsx'\n",
        ")\n",
        "\n",
        "descr_source_df = pd.read_excel(file_path)\n",
        "descr_source_df.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQr2NqMm5nAs"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Function to extract Latin name and convert to snake_case\n",
        "def extract_and_convert(col_name):\n",
        "    match = re.search(r'\\(([^)]+)\\)', col_name)\n",
        "    if match:\n",
        "        camel = match.group(1)\n",
        "    else:\n",
        "        camel = col_name\n",
        "    # Convert CamelCase to snake_case\n",
        "    snake = re.sub(r'(?<!^)(?=[A-Z])', '_', camel).lower()\n",
        "    return snake\n",
        "\n",
        "# Apply renaming\n",
        "descr_source_df.rename(columns={col: extract_and_convert(col) for col in descr_source_df.columns}, inplace=True)\n",
        "\n",
        "# Check the result\n",
        "print(\"Renamed columns:\")\n",
        "descr_source_df.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2GqhqDA5nAs"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = 'data'\n",
        "\n",
        "file_path = (\n",
        "    Path(DATA_PATH) /\n",
        "    Path('tables_OZ_geo_5500') /\n",
        "    'Карты мира_озон.xlsx'\n",
        ")\n",
        "\n",
        "source_df = pd.read_excel(file_path)\n",
        "source_df.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUyzhLch5nAs"
      },
      "outputs": [],
      "source": [
        "all_required_cols = [\n",
        "    'balance_first',\n",
        "    'sales_first',\n",
        "    'rating_first',\n",
        "    'final_price_first',\n",
        "    'comments_first',\n",
        "    'description_first',\n",
        "    'name_first',\n",
        "    'options_first',\n",
        "    'sku_first',\n",
        "    'has_video_first',\n",
        "    'photo_count_first',\n",
        "\n",
        "    'balance_second', # Balance\n",
        "    'sales_second',\n",
        "    'rating_second', # AvgRating\n",
        "    'final_price_second', # DiscountPrice,\n",
        "    'comments_second', # Reviews\n",
        "    'description_second',\n",
        "    'name_second', # ProductName\n",
        "    'options_second',\n",
        "    'sku_second',\n",
        "    'has_video_second',\n",
        "    'photo_count_second',\n",
        "\n",
        "    # 'image_url_first',\n",
        "    # 'image_url_second',\n",
        "\n",
        "    'iseq_vendor', # 0\n",
        "    'iseq_color', # 0\n",
        "    'iseq_brand', # BrandName\n",
        "    'iseq_supp', # 0\n",
        "    'are_related', # 0\n",
        "\n",
        "    'desc_sim',\n",
        "    'opt_sim',\n",
        "    'name_sim',\n",
        "    'img_sim',\n",
        "\n",
        "    'label'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkaXnu7a5nAt"
      },
      "outputs": [],
      "source": [
        "new_source_df = source_df.rename(\n",
        "    columns={\n",
        "        col: col.lower().replace(\" \", \"_\")\n",
        "        for col in source_df.columns\n",
        "    }\n",
        ")\n",
        "\n",
        "required_cols = [\n",
        "    'balance',\n",
        "    'sales',\n",
        "    'final_price',\n",
        "    'rating',\n",
        "    'comments',\n",
        "    # 'description',\n",
        "    'name',\n",
        "    # 'options'\n",
        "    'sku',\n",
        "    'has_video',\n",
        "    'pics_count'\n",
        "]\n",
        "\n",
        "new_source_df = (\n",
        "    new_source_df[required_cols]\n",
        "    .rename(columns={'pics_count': 'photo_count'})\n",
        ")\n",
        "\n",
        "new_source_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnvGujCt5nAt"
      },
      "outputs": [],
      "source": [
        "# Extract image id from URL\n",
        "\n",
        "descr_source_df['image_id'] = descr_source_df['cover_image'].dropna().apply(\n",
        "    lambda s: re.search(r'/(\\d+)\\.jpg$', str(s)).group(1)\n",
        ")\n",
        "\n",
        "descr_source_df.dropna(subset='image_id', inplace=True)\n",
        "descr_source_df[['image_id', 'sku']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz7RhUDC5nAt"
      },
      "outputs": [],
      "source": [
        "new_source_df = new_source_df.merge(\n",
        "    descr_source_df[['sku', 'description', 'image_id']],\n",
        "    on='sku'\n",
        ")\n",
        "\n",
        "new_source_df['options'] = new_source_df['name']\n",
        "new_source_df.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXkXS3Nm91NI"
      },
      "outputs": [],
      "source": [
        "new_source_df['description'] = (\n",
        "    new_source_df['description']\n",
        "    .fillna(new_source_df['name'])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TX74SPy5nAt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_pairs(sku):\n",
        "    \"\"\"\n",
        "    Given a target SKU, return a paired DataFrame where:\n",
        "      - *_first columns correspond to the target SKU row.\n",
        "      - *_second columns correspond to all other SKU rows.\n",
        "      - Equality columns (iseq_vendor, iseq_color, iseq_brand, iseq_supp, are_related) are added (all set to 0).\n",
        "\n",
        "    Parameters:\n",
        "        sku (int or str): SKU identifier for the target row.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with paired rows.\n",
        "    \"\"\"\n",
        "    # Ensure new_source_df is available in the global scope\n",
        "    global new_source_df\n",
        "\n",
        "    # Select the target row and the remaining rows\n",
        "    target_df = new_source_df[new_source_df['sku'] == sku]\n",
        "    if target_df.empty:\n",
        "        raise ValueError(f\"SKU {sku} not found in new_source_df\")\n",
        "    rest_df = new_source_df[new_source_df['sku'] != sku]\n",
        "\n",
        "    # Create a cross join (cartesian product) between the target row and all others\n",
        "    paired_df_all = pd.merge(\n",
        "        target_df.assign(key=1),\n",
        "        rest_df.assign(key=1),\n",
        "        on='key',\n",
        "        suffixes=('_first', '_second')\n",
        "    ).drop('key', axis=1)\n",
        "\n",
        "    # Add equality columns and set them all to 0\n",
        "    eq_cols = ['iseq_vendor', 'iseq_color', 'iseq_brand', 'iseq_supp', 'are_related']\n",
        "    for col in eq_cols:\n",
        "        paired_df_all[col] = 0\n",
        "\n",
        "    # Define desired final order of columns\n",
        "    final_columns = [\n",
        "        'balance_first', 'sales_first', 'rating_first', 'final_price_first',\n",
        "        'comments_first', 'description_first', 'name_first', 'options_first',\n",
        "        'sku_first', 'has_video_first', 'photo_count_first',\n",
        "\n",
        "        'balance_second', 'sales_second', 'rating_second', 'final_price_second',\n",
        "        'comments_second', 'description_second', 'name_second', 'options_second',\n",
        "        'sku_second', 'has_video_second', 'photo_count_second',\n",
        "\n",
        "        'iseq_vendor', 'iseq_color', 'iseq_brand', 'iseq_supp', 'are_related',\n",
        "\n",
        "        'image_id_first', 'image_id_second'\n",
        "    ]\n",
        "\n",
        "    paired_df_all = paired_df_all[final_columns]\n",
        "    return paired_df_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOBCArLWAqBy"
      },
      "outputs": [],
      "source": [
        "# Subset Query SKU\n",
        "query_skus = source_df[source_df['seller'] == 'ИНТЕРТРЕЙД']['sku'].tolist()\n",
        "len(query_skus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOEoGsk1AjR3"
      },
      "outputs": [],
      "source": [
        "paired_df_all = pd.DataFrame()\n",
        "for sku in query_skus:\n",
        "    paired_df = get_pairs(sku)\n",
        "    paired_df.columns.tolist()\n",
        "    paired_df_all = pd.concat([paired_df_all, paired_df], ignore_index=True)\n",
        "\n",
        "paired_df_all.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laV5w22X5nAt"
      },
      "source": [
        "# Add embedding distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_gY4LE55nAu"
      },
      "outputs": [],
      "source": [
        "# # Take a subset\n",
        "\n",
        "subset_size = len(paired_df_all) if SUBSET_SIZE is None else SUBSET_SIZE\n",
        "\n",
        "paired_df = paired_df_all.sample(subset_size, random_state=42)\n",
        "len(paired_df), len(paired_df_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4Jqmp0I5nAu"
      },
      "outputs": [],
      "source": [
        "# Compute description and option similarities\n",
        "\n",
        "BATCH_SIZE = 768 if torch.cuda.is_available() else 8\n",
        "\n",
        "desc_first, opt_first = paired_df.description_first, paired_df.options_first\n",
        "desc_second, opt_second = paired_df.description_second, paired_df.options_second\n",
        "\n",
        "emb_first = sbert.encode(\n",
        "    desc_first.tolist(),\n",
        "    convert_to_tensor=True,\n",
        "    show_progress_bar=True,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "emb_second = sbert.encode(\n",
        "    desc_second.tolist(),\n",
        "    convert_to_tensor=True,\n",
        "    show_progress_bar=True,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "desc_sim = np.diag(util.cos_sim(emb_first, emb_second).cpu().numpy())\n",
        "\n",
        "emb_first = sbert.encode(\n",
        "    opt_first.tolist(),\n",
        "    convert_to_tensor=True,\n",
        "    show_progress_bar=True,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "emb_second = sbert.encode(\n",
        "    opt_second.tolist(),\n",
        "    convert_to_tensor=True,\n",
        "    show_progress_bar=True,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "opt_sim = np.diag(util.cos_sim(emb_first, emb_second).cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vBS1jMk5nAu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "def get_sku_image_offline(\n",
        "    sku_or_image_id,\n",
        "    img_dataset_dir='data/images_7k'\n",
        "):\n",
        "    \"\"\"\n",
        "    Load an image for a given SKU from the dataset path.\n",
        "    It tries .jpg first then .webp.\n",
        "\n",
        "    Parameters:\n",
        "        sku (int or str): The SKU number.\n",
        "        img_dataset_dir (str): Directory path where images are stored.\n",
        "\n",
        "    Returns:\n",
        "        Image object if found and opened; otherwise, None.\n",
        "    \"\"\"\n",
        "    for ext in ['.jpg', '.webp']:\n",
        "        img_path = os.path.join(\n",
        "            img_dataset_dir, f\"{sku_or_image_id}{ext}\"\n",
        "        )\n",
        "\n",
        "        if os.path.exists(img_path):\n",
        "            try:\n",
        "                with open(img_path, 'rb') as f:\n",
        "                    img_data = f.read()\n",
        "                image = Image.open(BytesIO(img_data))\n",
        "                # Ensure the image loads completely\n",
        "                image.load()\n",
        "                return image\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {img_path}: {e}\")\n",
        "    return None\n",
        "\n",
        "# get_sku_image_offline(\n",
        "#     paired_df['image_id_first'].sample(1).item(),\n",
        "#     'data/images_OZ_geo_5500'\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcIGQqMM5nAu"
      },
      "outputs": [],
      "source": [
        "# Load images & names\n",
        "\n",
        "def get_images_names(\n",
        "    df,\n",
        "    image_id_col_first = 'sku_first',\n",
        "    image_id_col_second = 'sku_second',\n",
        "    img_dataset_dir='../data/images_7k',\n",
        "    offline=True,\n",
        ") -> Tuple[List[Image.Image], List[object]]:\n",
        "\n",
        "    images, names, problems = list(), list(), list()\n",
        "    for row in df.iterrows():\n",
        "        row_num = row[0]\n",
        "        row = row[1]\n",
        "\n",
        "        if offline:\n",
        "            img1 = get_sku_image_offline(int(row[image_id_col_first]), img_dataset_dir)\n",
        "            img2 = get_sku_image_offline(int(row[image_id_col_second]), img_dataset_dir)\n",
        "        else:\n",
        "            img1 = get_sku_image(int(row[image_id_col_first]))\n",
        "            img2 = get_sku_image(int(row[image_id_col_second]))\n",
        "\n",
        "        name1, name2 = row.name_first, row.name_second\n",
        "        if img1 is not None and img2 is not None:\n",
        "            images.append(img1)\n",
        "            images.append(img2)\n",
        "            names.append(name1)\n",
        "            names.append(name2)\n",
        "        else:\n",
        "            problems.append(row_num)\n",
        "    images = images\n",
        "    return images, names, problems\n",
        "\n",
        "images, names, problems_ids = get_images_names(\n",
        "    paired_df,\n",
        "    image_id_col_first = 'image_id_first',\n",
        "    image_id_col_second = 'image_id_second',\n",
        "    img_dataset_dir='data/images_OZ_geo_5500'\n",
        ")\n",
        "\n",
        "print(f'Images loaded: {len(images)}')\n",
        "print(f'Images not loaded: {len(problems_ids)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKwtvAiG5nAv"
      },
      "outputs": [],
      "source": [
        "# Delete problematic ids\n",
        "paired_df = paired_df[~paired_df.index.isin(problems_ids)]\n",
        "desc_sim = np.delete(desc_sim, problems_ids)\n",
        "opt_sim = np.delete(opt_sim, problems_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_3jsGZs5nAv"
      },
      "outputs": [],
      "source": [
        "classes = list(names)\n",
        "\n",
        "templates = ['{}', 'это {}', 'на картинке {}', 'товар {}']\n",
        "\n",
        "predictor = ruclip.Predictor(clip, processor, DEVICE, bs=8, templates=templates)\n",
        "with torch.no_grad():\n",
        "    text_latents = predictor.get_text_latents(classes)\n",
        "    images_latents = predictor.get_image_latents(images)\n",
        "\n",
        "name_sim = []\n",
        "img_sim = []\n",
        "\n",
        "for ind in range(0, text_latents.shape[0], 2):\n",
        "    first = text_latents[ind]\n",
        "    second = text_latents[ind + 1]\n",
        "    name_sim.append(util.cos_sim(first, second).cpu().numpy().squeeze())\n",
        "\n",
        "    first = images_latents[ind]\n",
        "    second = images_latents[ind + 1]\n",
        "    img_sim.append(util.cos_sim(first, second).cpu().numpy().squeeze())\n",
        "\n",
        "print(len(name_sim))\n",
        "print(len(img_sim))\n",
        "\n",
        "scores = np.c_[desc_sim, opt_sim, name_sim, img_sim]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLAXw3iZ5nAv"
      },
      "outputs": [],
      "source": [
        "scores_df = pd.DataFrame(scores, columns=['desc_sim', 'opt_sim', 'name_sim', 'img_sim'])\n",
        "\n",
        "final_df = pd.concat(\n",
        "    [\n",
        "        paired_df.drop(columns=scores_df.columns, errors='ignore'),\n",
        "        scores_df\n",
        "    ],\n",
        "    axis=1\n",
        ")\n",
        "final_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZNmh97d5nAv"
      },
      "outputs": [],
      "source": [
        "final_df.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Mprnydy5nAv"
      },
      "outputs": [],
      "source": [
        "file_path = (\n",
        "    Path(DATA_PATH) /\n",
        "    'tables_OZ_geo_5500' /\n",
        "    'tabular_OZ_geo_5500.csv'\n",
        ")\n",
        "\n",
        "final_df.to_csv(file_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

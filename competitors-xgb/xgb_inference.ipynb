{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd55908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.753719340570969\n",
      "F1-score: 0.7431081339773574\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the checkpoint directory.\n",
    "CKPT_DIR = 'model_params_big_test'\n",
    "# CKPT_DIR = 'res_f1'\n",
    "# CKPT_DIR = 'res_balanced_accuracy'\n",
    "\n",
    "# Paths to the saved model and scaler.\n",
    "xgb_model_path = Path(CKPT_DIR) / 'xgboost_model.json'\n",
    "scaler_path = Path(CKPT_DIR) / 'std_scaler.bin'\n",
    "df = pd.read_csv(Path(CKPT_DIR) / 'data.csv')\n",
    "\n",
    "df.columns.tolist()\n",
    "\n",
    "# Load the scaler and the pre-trained XGBoost model.\n",
    "scaler = joblib.load(scaler_path)\n",
    "model = xgb.XGBClassifier()\n",
    "model.load_model(xgb_model_path)\n",
    "\n",
    "# Load the dataset that contains ground truth labels.\n",
    "\n",
    "# Separate the true labels from the dataset.\n",
    "y_true = df['label']\n",
    "\n",
    "# Define the columns that were not used as features during training.\n",
    "columns_to_drop = [\n",
    "    'sku_first', 'sku_second',\n",
    "    'name_first', 'description_first',\n",
    "    'name_second', 'description_second',\n",
    "    'options_first', 'options_second',\n",
    "    'image_url_first', 'image_url_second',\n",
    "    'label'\n",
    "]\n",
    "\n",
    "# Create a DataFrame for scaling by dropping the extra columns.\n",
    "# The original df remains unchanged.\n",
    "X = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Ensure the columns match exactly what the scaler was trained on.\n",
    "# The scaler's attribute 'feature_names_in_' holds the expected column names.\n",
    "if hasattr(scaler, 'feature_names_in_'):\n",
    "    expected_features = list(scaler.feature_names_in_)\n",
    "    X_for_scaler = df[expected_features]\n",
    "else:\n",
    "    X_for_scaler = df\n",
    "\n",
    "# print(\"Columns used for scaling:\", X_for_scaler.columns.tolist())\n",
    "\n",
    "# Scale the features using the loaded scaler.\n",
    "X_scaled = scaler.transform(X_for_scaler)\n",
    "\n",
    "# Run inference using the pre-trained model.\n",
    "predictions = model.predict(X_scaled)\n",
    "\n",
    "# Calculate metrics.\n",
    "accuracy = accuracy_score(y_true, predictions)\n",
    "f1 = f1_score(y_true, predictions, average='weighted')\n",
    "\n",
    "# print(\"Predicted classes:\", predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe4679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcaee945",
   "metadata": {},
   "source": [
    "# Installs & tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9d9b411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import mlflow\n",
    "except ImportError:\n",
    "    !pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c65f9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import dotenv\n",
    "except ImportError:\n",
    "    !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e77b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# # Log into huggingface via Kaggle Secrets\n",
    "\n",
    "# import os\n",
    "# import huggingface_hub\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# user_secrets = UserSecretsClient()\n",
    "# HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "# huggingface_hub.login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3f67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# # Log into huggingface via .env\n",
    "\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# import huggingface_hub\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "# huggingface_hub.login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a0467",
   "metadata": {},
   "source": [
    "# Choose notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "be91836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "## CHOOSE MODEL PARAMETERS #################################################\n",
    "\n",
    "HIDDEN_DIM = 3*768\n",
    "DATA_PATH = 'data/'\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NAME_MODEL_NAME = 'cointegrated/rubert-tiny' # 'DeepPavlov/distilrubert-tiny-cased-conversational-v1'\n",
    "DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny'\n",
    "\n",
    "# BATCH_SIZE=90\n",
    "# NUM_WORKERS=8\n",
    "# NUM_DEBUG_SAMPLES=None\n",
    "# EPOCHS=20\n",
    "\n",
    "BATCH_SIZE=1\n",
    "NUM_WORKERS=0\n",
    "NUM_DEBUG_SAMPLES=2\n",
    "EPOCHS=2\n",
    "\n",
    "EMB_SIZE=768\n",
    "VALIDATION_SPLIT=.25\n",
    "SHUFFLE_DATASET=True\n",
    "RANDOM_SEED=42\n",
    "LR=9e-5\n",
    "MOMENTUM=0.9\n",
    "WEIGHT_DECAY=1e-2\n",
    "CONTRASTIVE_MARGIN=1.5\n",
    "CONTRASTIVE_THRESHOLD=0.3\n",
    "SHEDULER_PATIENCE=3 # in epochs\n",
    "\n",
    "MODEL_NAME_POSTFIX='splitting-by-query'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f3d9e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHOOSE DATA #########################################################\n",
    "\n",
    "# # These table files need 'image_name_first', 'image_name_second' constructed from sku to be usable in current pipeline\n",
    "# TABLE_DATASET_FILE = 'tables_labeled/processed/labeled_1.3k_with-options.csv'\n",
    "# TABLE_DATASET_FILE = 'tables_labeled/processed/labeled_56k_with-options.csv'\n",
    "# IMG_DATASET_NAME = 'images_7k'\n",
    "# STRATIFY_COLS = None\n",
    "\n",
    "# TABLE_DATASET_FILE = 'tables_labeled/processed/labeled_5k_with-options.csv'\n",
    "# IMG_DATASET_NAME = 'images_7k' \n",
    "# STRATIFY_COLS = None\n",
    "\n",
    "# TABLE_DATASET_FILE = 'tables_WB_OZ_100/WB_OZ_100.csv'\n",
    "# TABLE_DATASET_FILE = 'tables_WB_OZ_100/WB_OZ_100_conjugated.csv'\n",
    "# TABLE_DATASET_FILE = 'tables_WB_OZ_100/WB_OZ_100_conjugated_shuffled_seed=42_fraction=1.csv'\n",
    "# TABLE_DATASET_FILE = 'tables_WB_OZ_100/WB_OZ_100_conjugated_shuffled_seed=42_fraction=0.5.csv'\n",
    "# IMG_DATASET_NAME = 'images_WB_OZ_100'\n",
    "# STRATIFY_COLS = None\n",
    "\n",
    "TABLE_DATASET_FILE = 'tables_OZ_geo_5500/processed/regex-pairwise-dataset_num-queries=20_num-pairs=6226_patterns-dict-hash=6dbf9b3ef9568e60cd959f87be7e3b26.csv'\n",
    "IMG_DATASET_NAME = 'images_OZ_geo_5500'\n",
    "STRATIFY_COLS = ['sku_first', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOGGING PARAMS ######################################################################\n",
    "\n",
    "# MLFLOW_URI = \"http://176.56.185.96:5000\"\n",
    "# MLFLOW_URI = \"http://localhost:5000\"\n",
    "MLFLOW_URI = None\n",
    "\n",
    "MLFLOW_EXPERIMENT = \"siamese/1fold\"\n",
    "\n",
    "TELEGRAM_TOKEN = None\n",
    "# TELEGRAM_TOKEN = '' # set token to get notifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b40bc83",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "00017085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "from timm import create_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "# import transformers\n",
    "# from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer,\\\n",
    "#         get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# import json\n",
    "# from itertools import product\n",
    "\n",
    "# import datasets\n",
    "# from datasets import Dataset, concatenate_datasets\n",
    "# import argparse\n",
    "import requests\n",
    "\n",
    "# from io import BytesIO\n",
    "# from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "# import more_itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3494173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tg_report(text, token=None) -> None:\n",
    "    method = 'sendMessage'\n",
    "    chat_id = 324956476\n",
    "    _ = requests.post(\n",
    "            url='https://api.telegram.org/bot{0}/{1}'.format(token, method),\n",
    "            data={'chat_id': chat_id, 'text': text} \n",
    "        ).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "84518fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuCLIPtiny(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.visual = create_model('convnext_tiny',\n",
    "                                   pretrained=False, # TODO: берём претрейн\n",
    "                                   num_classes=0,\n",
    "                                   in_chans=3)  # out 768\n",
    "\n",
    "        self.transformer = AutoModel.from_pretrained(NAME_MODEL_NAME)\n",
    "        name_model_output_shape = self.transformer.config.hidden_size  # dynamically get hidden size\n",
    "        self.final_ln = torch.nn.Linear(name_model_output_shape, 768)  # now uses the transformer hidden size\n",
    "        self.logit_scale = torch.nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.stem[0].weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        x = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = x.last_hidden_state[:, 0, :]\n",
    "        x = self.final_ln(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(input_ids, attention_mask)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text\n",
    "    \n",
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        _convert_image_to_rgb,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]), ])\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "class Tokenizers:\n",
    "    def __init__(self):\n",
    "        self.name_tokenizer = AutoTokenizer.from_pretrained(NAME_MODEL_NAME)\n",
    "        self.desc_tokenizer = AutoTokenizer.from_pretrained(DESCRIPTION_MODEL_NAME)\n",
    "\n",
    "    def tokenize_name(self, texts, max_len=77):\n",
    "        tokenized = self.name_tokenizer.batch_encode_plus(texts,\n",
    "                                                     truncation=True,\n",
    "                                                     add_special_tokens=True,\n",
    "                                                     max_length=max_len,\n",
    "                                                     padding='max_length',\n",
    "                                                     return_attention_mask=True,\n",
    "                                                     return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "    \n",
    "    def tokenize_description(self, texts, max_len=77):\n",
    "        tokenized = self.desc_tokenizer(texts,\n",
    "                                        truncation=True,\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=max_len,\n",
    "                                        padding='max_length',\n",
    "                                        return_attention_mask=True,\n",
    "                                        return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e2d263be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseRuCLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df=None, labels=None, df_path=None, images_dir=DATA_PATH+'images/'):\n",
    "        # loads data either from path using `df_path` or directly from `df` argument\n",
    "        self.df = pd.read_csv(df_path) if df_path is not None else df\n",
    "        self.labels = labels\n",
    "        self.images_dir = images_dir\n",
    "        self.tokenizers = Tokenizers()\n",
    "        self.transform = get_transform()\n",
    "        # \n",
    "        self.max_len = 77\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        name_tokens = self.tokenizers.tokenize_name([str(row.name_first), \n",
    "                                               str(row.name_second)], max_len=self.max_len)\n",
    "        name_first = name_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        name_second = name_tokens[:, 1, :]\n",
    "        desc_tokens = self.tokenizers.tokenize_description([str(row.description_first), \n",
    "                                               str(row.description_second)])\n",
    "        desc_first = desc_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        desc_second = desc_tokens[:, 1, :]\n",
    "        im_first = cv2.imread(os.path.join(self.images_dir, row.image_name_first))\n",
    "        im_first = cv2.cvtColor(im_first, cv2.COLOR_BGR2RGB)\n",
    "        im_first = Image.fromarray(im_first)\n",
    "        im_first = self.transform(im_first)\n",
    "        im_second = cv2.imread(os.path.join(self.images_dir, row.image_name_second))\n",
    "        im_second = cv2.cvtColor(im_second, cv2.COLOR_BGR2RGB)\n",
    "        im_second = Image.fromarray(im_second)\n",
    "        im_second = self.transform(im_second)\n",
    "        label = self.labels[idx]\n",
    "        return im_first, name_first, desc_first, im_second, name_second, desc_second, label\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.df)\n",
    "    \n",
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        last_hidden = last_hidden_states.masked_fill(\n",
    "            ~attention_mask[..., None].bool(), 0.0\n",
    "        )\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "class SiameseRuCLIP(nn.Module):\n",
    "    def __init__(self, preload_ruclip=True, device='cpu', hidden_dim=HIDDEN_DIM, models_dir=DATA_PATH + 'train_results/'):\n",
    "        super().__init__()\n",
    "        self.ruclip = RuCLIPtiny()\n",
    "        if preload_ruclip:\n",
    "            preload_model_name = 'cc12m_rubert_tiny_ep_1.pt' #'cc12m_ddp_4mill_ep_4.pt'\n",
    "            std = torch.load(models_dir + preload_model_name, weights_only=True, map_location=device)\n",
    "            self.ruclip.load_state_dict(std)\n",
    "            self.ruclip = self.ruclip.to(device)\n",
    "            self.ruclip.eval()\n",
    "        self.description_transformer = AutoModel.from_pretrained(DESCRIPTION_MODEL_NAME)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            # # nn.BatchNorm1d(hidden_dim),\n",
    "            # nn.Dropout(0.3), \n",
    "            # nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            # nn.ReLU(), \n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "        )\n",
    "        \n",
    "    def encode_description(self, desc):\n",
    "        # desc is [input_ids, attention_mask]\n",
    "        last_hidden_states = self.description_transformer(desc[:, 0, :], desc[:, 1, :]).last_hidden_state\n",
    "        attention_mask = desc[:, 1, :]\n",
    "        # TODO: нужно ли делать пулинг, посмотреть на результаты\n",
    "        return average_pool(last_hidden_states, attention_mask)\n",
    "    \n",
    "    def forward(self, im1, name1, desc1, im2, name2, desc2):\n",
    "        image_emb1 = self.ruclip.encode_image(im1)\n",
    "        image_emb2 = self.ruclip.encode_image(im2)\n",
    "        name_emb1 = self.ruclip.encode_text(name1[:, 0, :], name1[:, 1, :])\n",
    "        name_emb2 = self.ruclip.encode_text(name2[:, 0, :], name2[:, 1, :])\n",
    "        desc_emb1 = self.ruclip.encode_text(desc1[:, 0, :], desc1[:, 1, :])\n",
    "        desc_emb2 = self.ruclip.encode_text(desc2[:, 0, :], desc2[:, 1, :])\n",
    "        # desc_emb1 = self.encode_description(desc1) \n",
    "        # desc_emb2 = self.encode_description(desc2)\n",
    "        first_emb = torch.cat([image_emb1, name_emb1, desc_emb1], dim=1)\n",
    "        second_emb = torch.cat([image_emb2, name_emb2, desc_emb2], dim=1)\n",
    "        out1 = self.head(first_emb)\n",
    "        out2 = self.head(second_emb)\n",
    "        return out1, out2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "987d1ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def __name__(self,):\n",
    "        return 'ContrastiveLoss'\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        pos = (1-label) * torch.pow(euclidean_distance, 2)\n",
    "        neg = label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        loss_contrastive = torch.mean( pos + neg )\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a9c6281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pair(output1, output2, target, threshold):\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    # меньше границы, там где будет True — конкуренты\n",
    "    cond = euclidean_distance < threshold\n",
    "    pos_sum = 0\n",
    "    neg_sum = 0\n",
    "    pos_acc = 0\n",
    "    neg_acc = 0\n",
    "\n",
    "    for i in range(len(cond)):\n",
    "        # 1 значит не конкуренты\n",
    "        if target[i]:\n",
    "            neg_sum+=1\n",
    "            # 0 в cond значит дальше друг от друга чем threshold\n",
    "            if not cond[i]:\n",
    "                neg_acc+=1\n",
    "        elif not target[i]:\n",
    "            pos_sum+=1\n",
    "            if cond[i]:\n",
    "                pos_acc+=1\n",
    "\n",
    "    return pos_acc, pos_sum, neg_acc, neg_sum\n",
    "\n",
    "def predict(out1, out2, threshold=CONTRASTIVE_THRESHOLD):\n",
    "    # вернёт 1 если похожи\n",
    "    return F.pairwise_distance(out1, out2) < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6664f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, valid_loader, epoch, device='cpu', split_name='validation') -> float:\n",
    "    assert split_name in ['train', 'validation', 'test']\n",
    "\n",
    "    valid_loss = 0\n",
    "    val_pos_accuracy = 0\n",
    "    val_neg_accuracy = 0\n",
    "    num_pos = 0\n",
    "    num_neg = 0\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for data in tqdm(valid_loader, desc=split_name):\n",
    "            im1, name1, desc1, im2, name2, desc2, label = data \n",
    "            im1, name1, desc1, im2, name2, desc2, label = (\n",
    "                im1.to(device), name1.to(device), desc1.to(device),\n",
    "                im2.to(device), name2.to(device), desc2.to(device), label.to(device)\n",
    "            )\n",
    "            out1, out2 = model(im1, name1, desc1, im2, name2, desc2) \n",
    "            loss = criterion(out1, out2, label)\n",
    "            pos_acc, pos_sum, neg_acc, neg_sum = evaluate_pair(out1, out2, label, CONTRASTIVE_THRESHOLD)\n",
    "            val_pos_accuracy += pos_acc\n",
    "            val_neg_accuracy += neg_acc\n",
    "            num_pos += pos_sum\n",
    "            num_neg += neg_sum\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    val_pos_accuracy = val_pos_accuracy / num_pos if num_pos > 0 else 0.0\n",
    "    val_neg_accuracy = val_neg_accuracy / num_neg if num_neg > 0 else 0.0\n",
    "    valid_loss = valid_loss / len(valid_loader) if len(valid_loader) > 0 else 0.0\n",
    "\n",
    "    report = (\n",
    "        f\"[{split_name}] Epoch: {epoch}, loss: {valid_loss:.3f}, \"\n",
    "        f\"P Acc: {val_pos_accuracy:.3f}, N Acc: {val_neg_accuracy:.3f} \" + MODEL_NAME_POSTFIX + '\\n'\n",
    "    )\n",
    "    print(report)\n",
    "    make_tg_report(report, TELEGRAM_TOKEN)\n",
    "    \n",
    "    return val_pos_accuracy, val_neg_accuracy, (val_pos_accuracy + val_neg_accuracy) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e34b5c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def plot_epoch(loss_history, filename=\"data/runs_artifacts/epoch_loss.png\") -> None:\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Iteration number\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(loss_history, 'b')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)  # Save the plot to a file\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4952b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, \n",
    "          epochs_num, train_loader, valid_loader=None, \n",
    "          device='cpu', print_epoch=False) -> None:\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    counter = []\n",
    "    loss_history = [] \n",
    "    it_number = 0\n",
    "    best_valid_score = 0\n",
    "    best_weights = None\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"max\",\n",
    "                                            factor=0.1, patience=SHEDULER_PATIENCE,\n",
    "                                            threshold=0.0001,\n",
    "                                            threshold_mode='rel', cooldown=0,\n",
    "                                            min_lr=0, eps=1e-08)\n",
    "\n",
    "    for epoch in range(epochs_num):\n",
    "        print(\"Epoch：\", epoch)\n",
    "        for i, data in enumerate(tqdm(train_loader, desc='train')):\n",
    "            im1, name1, desc1, im2, name2, desc2, label = data \n",
    "            im1, name1, desc1, im2, name2, desc2, label = im1.to(device), name1.to(device), desc1.to(device), im2.to(device), name2.to(device), desc2.to(device), label.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            out1, out2 = model(im1, name1, desc1, im2, name2, desc2)\n",
    "            loss = criterion(out1, out2, label)\n",
    "            if MLFLOW_URI:\n",
    "                mlflow.log_metric(\"train_loss\", f\"{loss:2f}\", step=epoch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 1 == 0: # show changes of loss value after each 10 batches\n",
    "                # it_number += 5\n",
    "                counter.append(it_number)\n",
    "                loss_history.append(loss.item())\n",
    "\n",
    "        # validate after each epoch\n",
    "        if print_epoch:\n",
    "            val_pos_acc, val_neg_acc, val_acc = validation(\n",
    "                model, criterion, valid_loader, epoch, device, split_name='validation'\n",
    "            )\n",
    "\n",
    "            if MLFLOW_URI:\n",
    "                mlflow.log_metric(\"valid_pos_accuracy\", f\"{val_pos_acc:2f}\", step=epoch)\n",
    "                mlflow.log_metric(\"valid_neg_accuracy\", f\"{val_neg_acc:2f}\", step=epoch)\n",
    "                mlflow.log_metric(\"valid_accuracy\", f\"{val_acc:2f}\", step=epoch)\n",
    "\n",
    "            plot_epoch(loss_history)\n",
    "            print(f'Current train loss: {loss}')\n",
    "            # print(f'Current {score.__name__}: {valid_score}')\n",
    "            lr_to_log = optimizer.param_groups[0]['lr']\n",
    "            if MLFLOW_URI:\n",
    "                mlflow.log_metric(\"lr\", f\"{lr_to_log:2f}\", step=epoch)\n",
    "            scheduler.step(val_acc)\n",
    "            if val_acc > best_valid_score:\n",
    "                best_valid_score = val_acc\n",
    "                best_weights = model.state_dict()\n",
    "    return best_valid_score, best_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481940bc",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226754b",
   "metadata": {},
   "source": [
    "## Download data from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a7428c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tables_OZ_geo_5500/processed/regex-pairwise-dataset_num-queries=20_num-pairs=6226_patterns-dict-hash=6dbf9b3ef9568e60cd959f87be7e3b26.csv'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TABLE_DATASET_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8cde88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db8503e665b408eaac8e193bc4c4dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cfa322cc6a42a98d8e96bd199fdaf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "siamese_contrastive_1gpu.pt:   0%|          | 0.00/220M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Download models' weights & text/image datasets\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ID = \"INDEEPA/clip-siamese\"\n",
    "LOCAL_DIR = Path(\"data/train_results\")\n",
    "LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type='dataset',\n",
    "    local_dir='data',\n",
    "    allow_patterns=[\n",
    "        \"train_results/cc12m*.pt\",\n",
    "        TABLE_DATASET_FILE,\n",
    "        f\"{IMG_DATASET_NAME}.zip\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "!unzip -n -q data/{IMG_DATASET_NAME}.zip -d data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d35d53",
   "metadata": {},
   "source": [
    "## Split data by query sku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387fca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique query sku: 20\n"
     ]
    }
   ],
   "source": [
    "TABLE_DATASET_PATH = DATA_PATH + TABLE_DATASET_FILE\n",
    "\n",
    "labeled = pd.read_csv(TABLE_DATASET_PATH)\n",
    "print(f\"Unique query sku: {labeled.sku_query.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c414d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/val/test\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "def split_pairwise(\n",
    "    df: pd.DataFrame,\n",
    "    test_size: float = 0.20,\n",
    "    random_state: int | None = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Leakage-free DEV / TEST split for a pair-wise SKU dataset.\n",
    "    Input `df` must have columns ['sku_query','sku_candidate','label'].\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    test_rows, dev_rows   = [], []\n",
    "    test_entities: set[str] = set()\n",
    "\n",
    "    # ---- iterate over each original query SKU ------------------------------\n",
    "    for q_sku, grp in df.groupby(\"sku_query\"):\n",
    "        pos_idx = grp.index[grp.label == 1].tolist()\n",
    "        neg_idx = grp.index[grp.label == 0].tolist()\n",
    "\n",
    "        # --- (1) sample TEST rows -------------------------------------------\n",
    "        n_pos = int(np.ceil(test_size * len(pos_idx))) if pos_idx else 0\n",
    "        n_neg = int(np.ceil(test_size * len(neg_idx))) if neg_idx else 0\n",
    "\n",
    "        pos_test = rng.choice(pos_idx, size=n_pos, replace=False) if n_pos else []\n",
    "        neg_test = rng.choice(neg_idx, size=n_neg, replace=False) if n_neg else []\n",
    "\n",
    "        test_rows.extend(pos_test)\n",
    "        test_rows.extend(neg_test)\n",
    "\n",
    "        # register every entity that just entered TEST\n",
    "        test_entities.add(q_sku)\n",
    "        test_entities.update(df.loc[pos_test, \"sku_candidate\"])\n",
    "        test_entities.update(df.loc[neg_test, \"sku_candidate\"])\n",
    "\n",
    "        # --- (2) build DEV from remaining rows ------------------------------\n",
    "        remain_pos = list(set(pos_idx) - set(pos_test))\n",
    "        remain_neg = list(set(neg_idx) - set(neg_test))\n",
    "\n",
    "        if remain_pos:\n",
    "            # choose substitute query (one of the remaining positives)\n",
    "            sub_idx  = int(rng.choice(remain_pos))\n",
    "            sub_sku  = df.loc[sub_idx, \"sku_candidate\"]\n",
    "\n",
    "            for idx in remain_pos:\n",
    "                if idx == sub_idx:            # skip (sub,sub) self-pair\n",
    "                    continue\n",
    "                row = df.loc[idx].copy()\n",
    "                row[\"sku_query\"] = sub_sku\n",
    "                dev_rows.append(row)\n",
    "\n",
    "            for idx in remain_neg:\n",
    "                row = df.loc[idx].copy()\n",
    "                row[\"sku_query\"] = sub_sku\n",
    "                dev_rows.append(row)\n",
    "\n",
    "    # ---- materialise the splits -------------------------------------------\n",
    "    test_df = df.loc[test_rows].reset_index(drop=True)\n",
    "    dev_df  = pd.DataFrame(dev_rows).reset_index(drop=True)\n",
    "\n",
    "    # ---- (3) final purge: remove any row touching a TEST entity ------------\n",
    "    mask = ~(dev_df[\"sku_query\"].isin(test_entities) |\n",
    "             dev_df[\"sku_candidate\"].isin(test_entities))\n",
    "    dev_df = dev_df[mask].reset_index(drop=True)\n",
    "\n",
    "    return dev_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e00ad1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2548, 95), (409, 95), (332, 95))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into train/val/test\n",
    "\n",
    "dev_df,  test_df  = split_pairwise(labeled,  test_size=0.05, random_state=42)\n",
    "train_df, val_df  = split_pairwise(dev_df,   test_size=0.1, random_state=42)\n",
    "\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "efc1fa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       negative  positive  total\n",
      "split                           \n",
      "train      2210       338   2548\n",
      "val         348        61    409\n",
      "test        280        52    332\n"
     ]
    }
   ],
   "source": [
    "# Print positive/negative pairs count per each split \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# collect your splits in a dict\n",
    "splits = {\n",
    "    'train': train_df,\n",
    "    'val':   val_df,\n",
    "    'test':  test_df,\n",
    "}\n",
    "\n",
    "# build the summary_df records\n",
    "records = []\n",
    "for name, df in splits.items():\n",
    "    vc = df['label'].value_counts()\n",
    "    records.append({\n",
    "        'split':    name,\n",
    "        'negative': vc.get(0, 0),\n",
    "        'positive': vc.get(1, 0),\n",
    "        'total':    len(df),\n",
    "    })\n",
    "\n",
    "# create a DataFrame and set the split name as index\n",
    "summary_df = (\n",
    "    pd.DataFrame(records)\n",
    "      .set_index('split')\n",
    "      .astype(int)\n",
    ")\n",
    "\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1bed0574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All sanity checks passed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def sanity_checks(train: pd.DataFrame, val: pd.DataFrame, test: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Verify that:\n",
    "      1) Each query SKU appears in exactly one split\n",
    "      2) No SKU (query or candidate) overlaps across splits\n",
    "      3) No duplicate pairs across splits\n",
    "      4) Each split has at least one positive and one negative\n",
    "    \"\"\"\n",
    "    # 1) Query-level disjointness\n",
    "    q_train = set(train[\"sku_query\"])\n",
    "    q_val   = set(val  [\"sku_query\"])\n",
    "    q_test  = set(test [\"sku_query\"])\n",
    "    assert not (q_train & q_val),   f\"Query SKU overlap train↔val: {q_train & q_val}\"\n",
    "    assert not (q_train & q_test),  f\"Query SKU overlap train↔test: {q_train & q_test}\"\n",
    "    assert not (q_val   & q_test),  f\"Query SKU overlap val↔test:   {q_val   & q_test}\"\n",
    "    \n",
    "    # 2) Global SKU disjointness (query OR candidate)\n",
    "    def all_skus(df):\n",
    "        return set(df[\"sku_query\"]) | set(df[\"sku_candidate\"])\n",
    "    s_train, s_val, s_test = all_skus(train), all_skus(val), all_skus(test)\n",
    "    assert not (s_train & s_val),   f\"SKU overlap train↔val: {s_train & s_val}\"\n",
    "    assert not (s_train & s_test),  f\"SKU overlap train↔test: {s_train & s_test}\"\n",
    "    assert not (s_val   & s_test),  f\"SKU overlap val↔test:   {s_val   & s_test}\"\n",
    "    \n",
    "    # 3) Unique pairs\n",
    "    all_pairs = pd.concat([train, val, test], ignore_index=True)\n",
    "    dupes = all_pairs.duplicated(subset=[\"sku_query\",\"sku_candidate\",\"label\"])\n",
    "    assert not dupes.any(), f\"Found {dupes.sum()} duplicate pairs across splits\"\n",
    "    \n",
    "    # 4) Label coverage in each split\n",
    "    for name, df in [(\"train\", train), (\"val\", val), (\"test\", test)]:\n",
    "        labels = df[\"label\"].unique()\n",
    "        assert set(labels) == {0,1}, f\"{name} split has labels {labels}, expected {{0,1}}\"\n",
    "    \n",
    "    print(\"✅ All sanity checks passed!\")\n",
    "\n",
    "sanity_checks(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b4aa24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_split(df: pd.DataFrame, num_samples: int | None, random_state: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    If num_samples is set, take up to that many random rows;\n",
    "    otherwise just shuffle the entire DataFrame.\n",
    "    Always resets the index.\n",
    "    \"\"\"\n",
    "    if num_samples is not None:\n",
    "        n = min(num_samples, len(df))\n",
    "        out = df.sample(n=n, random_state=random_state)\n",
    "    else:\n",
    "        out = df.sample(frac=1, random_state=random_state)\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "# apply to each split\n",
    "train_df = sample_split(train_df, NUM_DEBUG_SAMPLES, RANDOM_SEED)\n",
    "val_df   = sample_split(val_df,   NUM_DEBUG_SAMPLES, RANDOM_SEED)\n",
    "test_df  = sample_split(test_df,  NUM_DEBUG_SAMPLES, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17b84c",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "afb31be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def _run():\n",
    "    images_dir = os.path.join(DATA_PATH, IMG_DATASET_NAME)\n",
    "\n",
    "    # 1) Prepare splits and rename columns\n",
    "    splits = {\n",
    "        'train':      train_df,\n",
    "        'validation': val_df,\n",
    "        'test':       test_df,\n",
    "    }\n",
    "    loaders = {}\n",
    "\n",
    "    for split_name, df in splits.items():\n",
    "        # rename for dataset compatibility\n",
    "        df = df.rename(columns={\n",
    "            col: col.replace(\"_query\", \"_first\").replace(\"_candidate\", \"_second\")\n",
    "            for col in df.columns\n",
    "            if \"_query\" in col or \"_candidate\" in col\n",
    "        })\n",
    "\n",
    "        # build dataset + loader\n",
    "        labels = df[\"label\"].values\n",
    "        ds     = SiameseRuCLIPDataset(df.drop(columns=\"label\"), labels, images_dir=images_dir)\n",
    "\n",
    "        shuffle = (split_name == 'train')\n",
    "        loaders[split_name] = DataLoader(\n",
    "            ds,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "    train_loader = loaders['train']\n",
    "    valid_loader = loaders['validation']\n",
    "    test_loader  = loaders['test']\n",
    "\n",
    "    # 2) Model, loss, optimizer\n",
    "    print('Loading model...')\n",
    "    model     = SiameseRuCLIP(device=DEVICE).to(DEVICE)\n",
    "    criterion = ContrastiveLoss(margin=CONTRASTIVE_MARGIN).to(DEVICE)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    print('Done loading model.')\n",
    "\n",
    "    # 3) (Optional) MLflow setup\n",
    "    if MLFLOW_URI:\n",
    "        mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "        mlflow.set_experiment(MLFLOW_EXPERIMENT)\n",
    "        mlflow.enable_system_metrics_logging()\n",
    "        mlflow.start_run()\n",
    "        mlflow.log_params({\n",
    "            \"epochs\":       EPOCHS,\n",
    "            \"lr\":           LR,\n",
    "            \"batch_size\":   BATCH_SIZE,\n",
    "            \"weight_decay\": WEIGHT_DECAY,\n",
    "            \"num_workers\":  NUM_WORKERS,\n",
    "        })\n",
    "\n",
    "    # 4) Training + validation\n",
    "    _, best_weights = train(\n",
    "        model, optimizer, criterion,\n",
    "        EPOCHS, train_loader, valid_loader,\n",
    "        print_epoch=True,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    # 5) FINAL TESTING\n",
    "    test_pos_acc, test_neg_acc, test_acc = validation(\n",
    "        model,\n",
    "        criterion,\n",
    "        test_loader,\n",
    "        epoch=EPOCHS,\n",
    "        device=DEVICE,\n",
    "        split_name='test'\n",
    "    )\n",
    "\n",
    "    # 6) Save best weights\n",
    "    print('Saving model...')\n",
    "    torch.save(best_weights, DATA_PATH + f\"train_results/siamese_contrastive_{MODEL_NAME_POSTFIX}.pt\")\n",
    "\n",
    "    # 7) Log model signature (no example) if using MLflow\n",
    "    if MLFLOW_URI:\n",
    "        # infer signature from a small batch\n",
    "        batch = next(iter(valid_loader))\n",
    "        # move to CPU/NumPy and back to torch to compute output shapes\n",
    "        inputs = [t.to(DEVICE).cpu().numpy() for t in batch[:-1]]\n",
    "        tensors = [torch.from_numpy(arr).to(DEVICE) for arr in inputs]\n",
    "        outputs = model(*tensors)\n",
    "        outputs = [o.cpu().detach().numpy() for o in outputs]\n",
    "\n",
    "        signature = infer_signature(\n",
    "            {k: v for k, v in zip(\n",
    "                [\"im1\",\"name1\",\"desc1\",\"im2\",\"name2\",\"desc2\"], inputs\n",
    "            )},\n",
    "            {\"out1\": outputs[0], \"out2\": outputs[1]}\n",
    "        )\n",
    "\n",
    "        mlflow.pytorch.log_model(\n",
    "            model,\n",
    "            artifact_path=\"model\",\n",
    "            signature=signature\n",
    "        )\n",
    "    print('Done saving model.')\n",
    "\n",
    "    if MLFLOW_URI:\n",
    "        mlflow.log_metric(\"test_pos_accuracy\", f\"{test_pos_acc:2f}\")\n",
    "        mlflow.log_metric(\"test_neg_accuracy\", f\"{test_neg_acc:2f}\")\n",
    "        mlflow.log_metric(\"test_accuracy\", f\"{test_acc:2f}\")\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f6511a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current train loss: 3.376269578933716\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b982f74ed46458a9577bc6d0b1a3a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] Epoch: 2, loss: 3.878, P Acc: 0.000, N Acc: 0.000 splitting-by-query\n",
      "\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/21 16:32:35 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2025/05/21 16:32:35 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving model.\n",
      "🏃 View run beautiful-seal-509 at: http://localhost:5000/#/experiments/103866307993097995/runs/cbed3e205f364fa1a87ac71a17851ae3\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/103866307993097995\n"
     ]
    }
   ],
   "source": [
    "_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

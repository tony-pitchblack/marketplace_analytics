{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcaee945",
   "metadata": {},
   "source": [
    "# Installs & tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d9b411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import mlflow\n",
    "except ImportError:\n",
    "    !pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c65f9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import dotenv\n",
    "except ImportError:\n",
    "    !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0c3f67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Kaggle environment. Skipping Kaggle secrets.\n",
      "Trying to load HF_TOKEN from .env.\n",
      "Success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Log into huggingface via Kaggle Secrets or .env\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import huggingface_hub\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "    user_secrets = UserSecretsClient()\n",
    "    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not Kaggle environment. Skipping Kaggle secrets.\")\n",
    "    print(\"Trying to load HF_TOKEN from .env.\")\n",
    "    load_dotenv()\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "    print(\"Success!\")\n",
    "\n",
    "huggingface_hub.login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a0467",
   "metadata": {},
   "source": [
    "# Choose notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be91836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "## CHOOSE MODEL PARAMETERS #################################################\n",
    "\n",
    "MODEL_NAME_POSTFIX='splitting-by-query'\n",
    "NAME_MODEL_NAME = 'cointegrated/rubert-tiny' # 'DeepPavlov/distilrubert-tiny-cased-conversational-v1'\n",
    "DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny'\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "RESULTS_DIR = 'train_results/'\n",
    "\n",
    "# BATCH_SIZE=60 # uses 14.5GiB of 1 GPU\n",
    "# NUM_WORKERS=2 # TODO: use multiple GPU, tune number of workers\n",
    "# NUM_DEBUG_SAMPLES=None\n",
    "# EPOCHS=10 # epochs > 8 => overfit; NOTE: can train for longer since we take best validation checkpoint anyway\n",
    "\n",
    "BATCH_SIZE=1\n",
    "NUM_WORKERS=0\n",
    "NUM_DEBUG_SAMPLES=2\n",
    "EPOCHS=2\n",
    "\n",
    "PRELOAD_MODEL_NAME = 'cc12m_rubert_tiny_ep_1.pt' # preload ruclip\n",
    "# PRELOAD_MODEL_NAME = None\n",
    "\n",
    "POS_WEIGHT = 4.0 # TODO: infer from data\n",
    "\n",
    "# USE_ALL_TRAIN_PAIRS = False\n",
    "# MAX_SAMPLES_PER_EPOCH = None\n",
    "\n",
    "USE_ALL_TRAIN_PAIRS = True\n",
    "MAX_SAMPLES_PER_EPOCH = 2_500\n",
    "# MAX_SAMPLES_PER_EPOCH = 2_500 * 12\n",
    "\n",
    "DROPOUT = 0.5\n",
    "# DROPOUT = None\n",
    "\n",
    "# BEST_CKPT_METRIC = 'f1'\n",
    "BEST_CKPT_METRIC = 'pos_acc'\n",
    "\n",
    "VALIDATION_SPLIT=.05\n",
    "TEST_SPLIT=.1\n",
    "RANDOM_SEED=42\n",
    "LR=9e-5\n",
    "MOMENTUM=0.9\n",
    "WEIGHT_DECAY=1e-2\n",
    "CONTRASTIVE_MARGIN=1.5\n",
    "CONTRASTIVE_THRESHOLD=0.3\n",
    "SHEDULER_PATIENCE=3 # in epochs\n",
    "\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3d9e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHOOSE DATA #########################################################\n",
    "\n",
    "DATA_PATH=  'data/'\n",
    "\n",
    "# --- Load source_df and pairwise_mapping_df ---\n",
    "SOURCE_TABLE_NAME = 'tables_OZ_geo_5500/processed/OZ_geo_5500.csv'\n",
    "PAIRWISE_TABLE_NAME = 'tables_OZ_geo_5500/processed/regex-pairwise-groups_num-queries=2_patterns-dict-hash=6dbf9b3ef9568e60cd959f87be7e3b26.csv'\n",
    "IMG_DATASET_NAME = 'images_OZ_geo_5500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14b5eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOGGING PARAMS ######################################################################\n",
    "\n",
    "# MLFLOW_URI = \"http://176.56.185.96:5000\"\n",
    "# MLFLOW_URI = \"http://localhost:5000\"\n",
    "MLFLOW_URI = None\n",
    "\n",
    "MLFLOW_EXPERIMENT = \"siamese/1fold\"\n",
    "\n",
    "TELEGRAM_TOKEN = None\n",
    "# TELEGRAM_TOKEN = '' # set token to get notifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b40bc83",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00017085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "from timm import create_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "# import transformers\n",
    "# from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer,\\\n",
    "#         get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# import json\n",
    "# from itertools import product\n",
    "\n",
    "# import datasets\n",
    "# from datasets import Dataset, concatenate_datasets\n",
    "# import argparse\n",
    "import requests\n",
    "\n",
    "# from io import BytesIO\n",
    "# from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "# import more_itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import mlflow\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3494173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tg_report(text, token=None) -> None:\n",
    "    method = 'sendMessage'\n",
    "    chat_id = 324956476\n",
    "    _ = requests.post(\n",
    "            url='https://api.telegram.org/bot{0}/{1}'.format(token, method),\n",
    "            data={'chat_id': chat_id, 'text': text} \n",
    "        ).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34706f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuCLIPtiny(nn.Module):\n",
    "    def __init__(self, name_model_name):\n",
    "        super().__init__()\n",
    "        self.visual = create_model('convnext_tiny',\n",
    "                                   pretrained=False, # TODO: берём претрейн\n",
    "                                   num_classes=0,\n",
    "                                   in_chans=3)  # out 768\n",
    "\n",
    "        self.transformer = AutoModel.from_pretrained(name_model_name)\n",
    "        name_model_output_shape = self.transformer.config.hidden_size  # dynamically get hidden size\n",
    "        self.final_ln = torch.nn.Linear(name_model_output_shape, 768)  # now uses the transformer hidden size\n",
    "        self.logit_scale = torch.nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.stem[0].weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        x = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = x.last_hidden_state[:, 0, :]\n",
    "        x = self.final_ln(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(input_ids, attention_mask)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84518fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        _convert_image_to_rgb,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]), ])\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "class Tokenizers:\n",
    "    def __init__(self):\n",
    "        self.name_tokenizer = AutoTokenizer.from_pretrained(NAME_MODEL_NAME)\n",
    "        self.desc_tokenizer = AutoTokenizer.from_pretrained(DESCRIPTION_MODEL_NAME)\n",
    "\n",
    "    def tokenize_name(self, texts, max_len=77):\n",
    "        tokenized = self.name_tokenizer.batch_encode_plus(texts,\n",
    "                                                     truncation=True,\n",
    "                                                     add_special_tokens=True,\n",
    "                                                     max_length=max_len,\n",
    "                                                     padding='max_length',\n",
    "                                                     return_attention_mask=True,\n",
    "                                                     return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "    \n",
    "    def tokenize_description(self, texts, max_len=77):\n",
    "        tokenized = self.desc_tokenizer(texts,\n",
    "                                        truncation=True,\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=max_len,\n",
    "                                        padding='max_length',\n",
    "                                        return_attention_mask=True,\n",
    "                                        return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "\n",
    "class SiameseRuCLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df=None, labels=None, df_path=None, images_dir=DATA_PATH+'images/'):\n",
    "        # loads data either from path using `df_path` or directly from `df` argument\n",
    "        self.df = pd.read_csv(df_path) if df_path is not None else df\n",
    "        self.labels = labels\n",
    "        self.images_dir = images_dir\n",
    "        self.tokenizers = Tokenizers()\n",
    "        self.transform = get_transform()\n",
    "        # \n",
    "        self.max_len = 77\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        name_tokens = self.tokenizers.tokenize_name([str(row.name_first), \n",
    "                                               str(row.name_second)], max_len=self.max_len)\n",
    "        name_first = name_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        name_second = name_tokens[:, 1, :]\n",
    "        desc_tokens = self.tokenizers.tokenize_description([str(row.description_first), \n",
    "                                               str(row.description_second)])\n",
    "        desc_first = desc_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        desc_second = desc_tokens[:, 1, :]\n",
    "        im_first = cv2.imread(os.path.join(self.images_dir, row.image_name_first))\n",
    "        im_first = cv2.cvtColor(im_first, cv2.COLOR_BGR2RGB)\n",
    "        im_first = Image.fromarray(im_first)\n",
    "        im_first = self.transform(im_first)\n",
    "        im_second = cv2.imread(os.path.join(self.images_dir, row.image_name_second))\n",
    "        im_second = cv2.cvtColor(im_second, cv2.COLOR_BGR2RGB)\n",
    "        im_second = Image.fromarray(im_second)\n",
    "        im_second = self.transform(im_second)\n",
    "        label = self.labels[idx]\n",
    "        return im_first, name_first, desc_first, im_second, name_second, desc_second, label\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2d263be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "class SiameseRuCLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 device: str,\n",
    "                 name_model_name: str,\n",
    "                 description_model_name: str,\n",
    "                 preload_model_name: str = None,\n",
    "                 models_dir: str = None,\n",
    "                 dropout: float = None):\n",
    "        \"\"\"\n",
    "        Initializes the SiameseRuCLIP model.\n",
    "        Required parameters:\n",
    "          - models_dir: directory containing saved checkpoints.\n",
    "          - name_model_name: model name for text (name) branch.\n",
    "          - description_model_name: model name for description branch.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        device = torch.device(device)\n",
    "\n",
    "        # Initialize RuCLIPtiny\n",
    "        self.ruclip = RuCLIPtiny(name_model_name)\n",
    "        if preload_model_name is not None:\n",
    "            std = torch.load(\n",
    "                os.path.join(models_dir, preload_model_name),\n",
    "                weights_only=True,\n",
    "                map_location=device\n",
    "            )\n",
    "            self.ruclip.load_state_dict(std)\n",
    "            self.ruclip.eval()\n",
    "        self.ruclip = self.ruclip.to(device)\n",
    "\n",
    "        # Initialize the description transformer\n",
    "        self.description_transformer = AutoModel.from_pretrained(description_model_name)\n",
    "        self.description_transformer = self.description_transformer.to(device)\n",
    "\n",
    "        # Determine dimensionality\n",
    "        vision_dim = self.ruclip.visual.num_features\n",
    "        name_dim = self.ruclip.final_ln.out_features\n",
    "        desc_dim = self.description_transformer.config.hidden_size\n",
    "        self.hidden_dim = vision_dim + name_dim + desc_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define MLP head with optional dropout\n",
    "        layers = [\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            *( [nn.Dropout(self.dropout)] if self.dropout is not None else [] ),\n",
    "            nn.Linear(self.hidden_dim // 2, self.hidden_dim // 4),\n",
    "        ]\n",
    "        self.head = nn.Sequential(*layers).to(device)\n",
    "\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.ruclip.encode_image(image)\n",
    "\n",
    "    def encode_name(self, name):\n",
    "        return self.ruclip.encode_text(name[:, 0, :], name[:, 1, :])\n",
    "\n",
    "    def encode_description(self, desc):\n",
    "        last_hidden_states = self.description_transformer(desc[:, 0, :], desc[:, 1, :]).last_hidden_state\n",
    "        attention_mask = desc[:, 1, :]\n",
    "        return average_pool(last_hidden_states, attention_mask)\n",
    "\n",
    "    def get_final_embedding(self, im, name, desc):\n",
    "        image_emb = self.encode_image(im)\n",
    "        name_emb = self.encode_name(name)\n",
    "        desc_emb = self.encode_description(desc)\n",
    "\n",
    "        # Concatenate the embeddings and forward through the head\n",
    "        combined_emb = torch.cat([image_emb, name_emb, desc_emb], dim=1)\n",
    "        final_embedding = self.head(combined_emb)\n",
    "        return final_embedding\n",
    "\n",
    "    def forward(self, im1, name1, desc1, im2, name2, desc2):\n",
    "        out1 = self.get_final_embedding(im1, name1, desc1)\n",
    "        out2 = self.get_final_embedding(im2, name2, desc2)\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "987d1ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # old\n",
    "# class ContrastiveLoss(torch.nn.Module):\n",
    "#     def __init__(self, margin=2.0):\n",
    "#         super(ContrastiveLoss, self).__init__()\n",
    "#         self.margin = margin\n",
    "        \n",
    "#     def __name__(self,):\n",
    "#         return 'ContrastiveLoss'\n",
    "\n",
    "#     def forward(self, output1, output2, label):\n",
    "#         euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "#         pos = (1-label) * torch.pow(euclidean_distance, 2)\n",
    "#         neg = label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "#         loss_contrastive = torch.mean( pos + neg )\n",
    "#         return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36cc0798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin: float = 1.5, pos_weight: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.margin      = margin\n",
    "        self.pos_weight  = pos_weight\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        d   = F.pairwise_distance(output1, output2)\n",
    "        pos = (1 - label) * d.pow(2)                            # duplicates (label==0)\n",
    "        neg = label * F.relu(self.margin - d).pow(2)            # different (label==1)\n",
    "        return (self.pos_weight * pos + neg).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfe81dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot epoch after each train epoch in `train()`\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_epoch(loss_history, filename=\"data/runs_artifacts/epoch_loss.png\") -> None:\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Iteration number\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(loss_history, 'b')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)  # Save the plot to a file\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9c6281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pair(output1, output2, target, threshold):\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    # меньше границы, там где будет True — конкуренты\n",
    "    cond = euclidean_distance < threshold\n",
    "    pos_sum = 0\n",
    "    neg_sum = 0\n",
    "    pos_acc = 0\n",
    "    neg_acc = 0\n",
    "\n",
    "    for i in range(len(cond)):\n",
    "        # 1 значит не конкуренты\n",
    "        if target[i]:\n",
    "            neg_sum+=1\n",
    "            # 0 в cond значит дальше друг от друга чем threshold\n",
    "            if not cond[i]:\n",
    "                neg_acc+=1\n",
    "        elif not target[i]:\n",
    "            pos_sum+=1\n",
    "            if cond[i]:\n",
    "                pos_acc+=1\n",
    "\n",
    "    return pos_acc, pos_sum, neg_acc, neg_sum\n",
    "\n",
    "def predict(out1, out2, threshold=CONTRASTIVE_THRESHOLD):\n",
    "    # вернёт 1 если похожи\n",
    "    return F.pairwise_distance(out1, out2) < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "319a25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "import mlflow\n",
    "from copy import deepcopy\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pathlib import Path\n",
    "\n",
    "def validation(model,\n",
    "               criterion,\n",
    "               data_loader,\n",
    "               epoch,\n",
    "               device='cpu',\n",
    "               split_name='validation',\n",
    "               threshold=None,\n",
    "               margin=1.5,\n",
    "               steps=200,\n",
    "               metric='f1'):\n",
    "    \"\"\"\n",
    "    Runs one pass over `data_loader`, returning:\n",
    "      pos_acc, neg_acc, avg_acc, f1, avg_loss, best_thr\n",
    "\n",
    "    threshold sweep: if threshold is None, tests `steps` values in [0, margin]\n",
    "    and picks the one that maximises either:\n",
    "      - F1              if metric=='f1'\n",
    "      - positive accuracy if metric=='pos_acc'\n",
    "    \"\"\"\n",
    "    assert metric in ('f1', 'pos_acc'), \"metric must be 'f1' or 'pos_acc'\"\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_d, all_lbl = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            im1, n1, d1, im2, n2, d2, lbl = [t.to(device) for t in batch]\n",
    "            out1, out2 = model(im1, n1, d1, im2, n2, d2)\n",
    "            total_loss += criterion(out1, out2, lbl).item()\n",
    "            all_d.append(F.pairwise_distance(out1, out2).cpu())\n",
    "            all_lbl.append(lbl.cpu())\n",
    "\n",
    "    distances = torch.cat(all_d)\n",
    "    labels    = torch.cat(all_lbl)               # 0 = duplicate (positive), 1 = different (negative)\n",
    "    avg_loss  = total_loss / len(data_loader)\n",
    "\n",
    "    # === threshold sweep ===\n",
    "    if threshold is None:\n",
    "        grid = np.linspace(0.0, margin, steps)\n",
    "        best_val, best_thr = -1.0, 0.0\n",
    "        y_true = (labels.numpy() == 0).astype(int)   # 1 = positive\n",
    "        for t in grid:\n",
    "            y_pred = (distances.numpy() < t).astype(int)\n",
    "            if metric == 'f1':\n",
    "                val = f1_score(y_true, y_pred, zero_division=0)\n",
    "            else:  # metric == 'pos_acc'\n",
    "                # positive accuracy = TP / P\n",
    "                pos_mask = (y_true == 1)\n",
    "                val = (y_pred[pos_mask] == 1).mean() if pos_mask.sum() > 0 else 0.0\n",
    "            if val > best_val:\n",
    "                best_val, best_thr = val, t\n",
    "        threshold = best_thr\n",
    "    else:\n",
    "        best_thr = threshold\n",
    "\n",
    "    # === final metrics at chosen threshold ===\n",
    "    preds    = (distances < threshold).long()\n",
    "    pos_mask = (labels == 0)\n",
    "    neg_mask = (labels == 1)\n",
    "\n",
    "    pos_acc = (preds[pos_mask] == 1).float().mean().item() if pos_mask.any() else 0.0\n",
    "    neg_acc = (preds[neg_mask] == 0).float().mean().item() if neg_mask.any() else 0.0\n",
    "    avg_acc = (pos_acc + neg_acc) / 2.0\n",
    "    f1      = f1_score((labels.numpy() == 0).astype(int),\n",
    "                       preds.numpy(), zero_division=0)\n",
    "\n",
    "    # log to Telegram / console\n",
    "    report = (f\"[{split_name}] Epoch {epoch} – \"\n",
    "              f\"loss: {avg_loss:.4f}, \"\n",
    "              f\"P Acc: {pos_acc:.3f}, \"\n",
    "              f\"N Acc: {neg_acc:.3f}, \"\n",
    "              f\"Avg Acc: {avg_acc:.3f}, \"\n",
    "              f\"F1: {f1:.3f}, \"\n",
    "              f\"thr*: {threshold:.3f} \"\n",
    "              f\"(optimised: {metric})\")\n",
    "    print(report)\n",
    "    make_tg_report(report, TELEGRAM_TOKEN)\n",
    "\n",
    "    # log to MLflow under the chosen metric\n",
    "    if MLFLOW_URI and split_name == 'validation':\n",
    "        if metric == 'f1':\n",
    "            mlflow.log_metric(\"valid_f1_score\", f1, step=epoch)\n",
    "        else:\n",
    "            mlflow.log_metric(\"valid_pos_accuracy\", pos_acc, step=epoch)\n",
    "\n",
    "    return pos_acc, neg_acc, avg_acc, f1, avg_loss, threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff968d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from datetime import timedelta\n",
    "\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          criterion,\n",
    "          epochs_num,\n",
    "          train_loader,\n",
    "          valid_loader=None,\n",
    "          device='cpu',\n",
    "          print_epoch=False,\n",
    "          models_dir=None,\n",
    "          metric='f1'):\n",
    "    \"\"\"\n",
    "    Trains for `epochs_num` epochs, using `validation(..., metric=metric)` each epoch.\n",
    "    Uses the same `metric` to step the LR scheduler and to pick the best checkpoint.\n",
    "\n",
    "    Returns:\n",
    "      train_losses, val_losses, best_valid_metric, best_weights, thr_history\n",
    "    \"\"\"\n",
    "    assert metric in ('f1', 'pos_acc'), \"metric must be 'f1' or 'pos_acc'\"\n",
    "\n",
    "    model.to(device)\n",
    "    train_losses, val_losses, thr_history = [], [], []\n",
    "    best_valid_metric, best_threshold = float('-inf'), None\n",
    "    best_weights = None\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"max\",\n",
    "        factor=0.1,\n",
    "        patience=SHEDULER_PATIENCE,\n",
    "        threshold=1e-4,\n",
    "        threshold_mode='rel'\n",
    "    )\n",
    "\n",
    "    if models_dir:\n",
    "        Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, epochs_num + 1):\n",
    "        # ---- training ----\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            im1, n1, d1, im2, n2, d2, lbl = [t.to(device) for t in batch]\n",
    "            optimizer.zero_grad()\n",
    "            out1, out2 = model(im1, n1, d1, im2, n2, d2)\n",
    "            loss = criterion(out1, out2, lbl)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        train_losses.append(total_train_loss / len(train_loader))\n",
    "\n",
    "        # ---- validation & checkpointing ----\n",
    "        if print_epoch and valid_loader is not None:\n",
    "            pos_acc, neg_acc, avg_acc, f1_val, val_loss, val_thr = validation(\n",
    "                model,\n",
    "                criterion,\n",
    "                valid_loader,\n",
    "                epoch,\n",
    "                device=device,\n",
    "                split_name='validation',\n",
    "                threshold=None,\n",
    "                margin=CONTRASTIVE_MARGIN,\n",
    "                steps=200,\n",
    "                metric=metric\n",
    "            )\n",
    "            val_losses.append(val_loss)\n",
    "            thr_history.append(val_thr)\n",
    "\n",
    "            # pick the metric value to step & compare\n",
    "            cur_metric = pos_acc if metric == 'pos_acc' else f1_val\n",
    "            scheduler.step(cur_metric)\n",
    "\n",
    "            # save checkpoint every epoch if requested\n",
    "            if models_dir:\n",
    "                torch.save(model.state_dict(),\n",
    "                           Path(models_dir) / f\"checkpoint_epoch_{epoch}.pt\")\n",
    "\n",
    "            # update best if improved\n",
    "            if cur_metric > best_valid_metric:\n",
    "                best_valid_metric = cur_metric\n",
    "                best_threshold     = val_thr\n",
    "                best_weights       = deepcopy(model.state_dict())\n",
    "\n",
    "        print(f'Epoch {epoch} done.')\n",
    "\n",
    "    print(f\"Best validation {metric}: {best_valid_metric:.3f}  (thr={best_threshold:.3f})\")\n",
    "    return train_losses, val_losses, best_valid_metric, best_weights, thr_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481940bc",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226754b",
   "metadata": {},
   "source": [
    "## Download data from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b8cde88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c42431a930e46dea49c72f7e5f9d476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/anton/marketplace/clip-siamese/data'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download models' weights & text/image datasets\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ID = \"INDEEPA/clip-siamese\"\n",
    "LOCAL_DIR = Path(\"data/train_results\")\n",
    "LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type='dataset',\n",
    "    local_dir='data',\n",
    "    allow_patterns=[\n",
    "        \"train_results/cc12m*.pt\",\n",
    "        SOURCE_TABLE_NAME, PAIRWISE_TABLE_NAME,\n",
    "        f\"{IMG_DATASET_NAME}.zip\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# The following shell command was removed for script compatibility:\n",
    "# !unzip -n -q data/{IMG_DATASET_NAME}.zip -d data/\n",
    "\n",
    "# If you need to unzip in Python, use:\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile(f\"data/{IMG_DATASET_NAME}.zip\", 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be5962fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = pd.read_csv(DATA_PATH + SOURCE_TABLE_NAME)\n",
    "pairwise_mapping_df = pd.read_csv(DATA_PATH + PAIRWISE_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24386f95",
   "metadata": {},
   "source": [
    "# Compute embeddings for soft negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75bea699",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_COMPUTE_SOFT_NEG_EMB = True\n",
    "# DO_COMPUTE_SOFT_NEG_EMB = False\n",
    "\n",
    "EMBEDDING_MODEL_NAME = 'sergeyzh/LaBSE-ru-turbo'\n",
    "EMB_BATCH_SIZE = 512 if torch.cuda.is_available() else 8\n",
    "\n",
    "NUM_EMBS = 2\n",
    "# NUM_EMBS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc928414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>name_and_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1871769771</td>\n",
       "      <td>Карты МИРА и РОССИИ настенные политические,160...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1679550303</td>\n",
       "      <td>Схема линий скоростного транспорта Москвы (Мет...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1200553001</td>\n",
       "      <td>Политическая карта МИРА 160х109 см, Карта мира...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>922231521</td>\n",
       "      <td>Политическая карта МИРА настенная, 100х70см, ш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>922230517</td>\n",
       "      <td>Политическая карта МИРА настенная, 160х102см, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sku                               name_and_description\n",
       "0  1871769771  Карты МИРА и РОССИИ настенные политические,160...\n",
       "1  1679550303  Схема линий скоростного транспорта Москвы (Мет...\n",
       "2  1200553001  Политическая карта МИРА 160х109 см, Карта мира...\n",
       "3   922231521  Политическая карта МИРА настенная, 100х70см, ш...\n",
       "4   922230517  Политическая карта МИРА настенная, 160х102см, ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Construct 'name_and_description' column for all SKUs in source_df ---\n",
    "source_df['name_and_description'] = source_df['name'].fillna('') + '.\\n' + source_df['description'].fillna('')\n",
    "display.display(source_df[['sku', 'name_and_description']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4c7e8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073476f0d8f945afa22438384ddb3dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>name_desc_emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1871769771</td>\n",
       "      <td>[-0.020089328289031982, -0.05487040802836418, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1679550303</td>\n",
       "      <td>[-0.004182410426437855, -0.040884267538785934,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sku                                      name_desc_emb\n",
       "0  1871769771  [-0.020089328289031982, -0.05487040802836418, ...\n",
       "1  1679550303  [-0.004182410426437855, -0.040884267538785934,..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to:\n",
      "embeddings/OZ_geo_5500/OZ_geo_5500_name-and-description_embeddings_num-rows=2.parquet\n"
     ]
    }
   ],
   "source": [
    "# --- Compute and save embeddings for all SKUs in source_df ---\n",
    "if DO_COMPUTE_SOFT_NEG_EMB:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from tqdm import tqdm\n",
    "    from pathlib import Path\n",
    "    import torch\n",
    "\n",
    "    all_skus_df = source_df.copy()\n",
    "    if NUM_EMBS is not None:\n",
    "        all_skus_df = all_skus_df.head(NUM_EMBS)\n",
    "\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)\n",
    "\n",
    "    emb_table = all_skus_df[['sku', 'name_and_description']].copy().reset_index(drop=True)\n",
    "    candidate_texts = emb_table['name_and_description'].astype(str).tolist()\n",
    "\n",
    "    embeddings = model.encode(\n",
    "        candidate_texts,\n",
    "        batch_size=EMB_BATCH_SIZE,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    emb_table['name_desc_emb'] = [emb.tolist() if hasattr(emb, 'tolist') else emb for emb in embeddings]\n",
    "    emb_table = emb_table[['sku', 'name_desc_emb']]\n",
    "    display.display(emb_table.head())\n",
    "\n",
    "    # Save to parquet\n",
    "    file_dir = Path('embeddings/OZ_geo_5500')\n",
    "    file_name = f\"{Path(SOURCE_TABLE_NAME).stem}_name-and-description_embeddings_num-rows={len(emb_table)}.parquet\"\n",
    "    full_file_path = Path(DATA_PATH) / file_dir / file_name\n",
    "    full_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    emb_table.to_parquet(full_file_path, index=False)\n",
    "    print(f\"Saved embeddings to:\\n{file_dir / file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9f69cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca3b891305044a7afb0a054abcc96b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "OZ_geo_5500_name-and-description_embeddings_num-rows=2.parquet:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if DO_COMPUTE_SOFT_NEG_EMB:\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    from huggingface_hub import HfApi, login\n",
    "\n",
    "    # Load HF_TOKEN from .env\n",
    "    load_dotenv()\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    if not hf_token:\n",
    "        raise ValueError(\"HF_TOKEN not found in .env file\")\n",
    "\n",
    "    # Log into HuggingFace\n",
    "    login(token=hf_token)\n",
    "\n",
    "    # Upload the folder\n",
    "    api = HfApi()\n",
    "    api.upload_folder(\n",
    "        folder_path=DATA_PATH / file_dir,  # Path to the local directory\n",
    "        path_in_repo=str(file_dir),\n",
    "        repo_id=\"INDEEPA/clip-siamese\",\n",
    "        repo_type=\"dataset\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70179151",
   "metadata": {},
   "source": [
    "# Cluster soft negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c67ed0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggest the correct path to the embedding file based on the context and previous file saving logic\n",
    "CHOSEN_EMBEDDING_FILE = 'embeddings/OZ_geo_5500/OZ_geo_5500_name-and-description_embeddings_num-rows=2.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3586cf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded embedding file to:\n",
      "data/embeddings/OZ_geo_5500/OZ_geo_5500_name-and-description_embeddings_num-rows=2.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>name_desc_emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1871769771</td>\n",
       "      <td>[-0.020089328289031982, -0.05487040802836418, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1679550303</td>\n",
       "      <td>[-0.004182410426437855, -0.040884267538785934,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sku                                      name_desc_emb\n",
       "0  1871769771  [-0.020089328289031982, -0.05487040802836418, ...\n",
       "1  1679550303  [-0.004182410426437855, -0.040884267538785934,..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "\n",
    "# Download the chosen embedding file from HuggingFace Hub to DATA_PATH\n",
    "from pathlib import Path\n",
    "\n",
    "downloaded_emb_file = hf_hub_download(\n",
    "    repo_id=\"INDEEPA/clip-siamese\",\n",
    "    repo_type=\"dataset\",\n",
    "    filename=CHOSEN_EMBEDDING_FILE,\n",
    "    local_dir=DATA_PATH,\n",
    ")\n",
    "\n",
    "print(f\"Downloaded embedding file to:\\n{downloaded_emb_file}\")\n",
    "emb_table = pd.read_parquet(downloaded_emb_file)\n",
    "emb_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6d4a152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster label counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count\n",
       "cluster       \n",
       "-1           2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Prepare the embeddings as a numpy array\n",
    "embeddings = np.stack(emb_table['name_desc_emb'].values)\n",
    "\n",
    "# Run HDBSCAN clustering using sklearn's implementation\n",
    "clusterer = HDBSCAN(min_cluster_size=2, metric='cosine')\n",
    "cluster_labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "# Add cluster labels to the emb_table and assign to cluster_emb_table\n",
    "cluster_emb_table = emb_table.copy()\n",
    "cluster_emb_table['cluster'] = cluster_labels\n",
    "\n",
    "# Print cluster label counts\n",
    "print(\"Cluster label counts:\")\n",
    "display.display(cluster_emb_table['cluster'].value_counts().to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5dda22",
   "metadata": {},
   "source": [
    "# Make pairwise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c03bced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique query sku: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_116370/1367893158.py:3: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  regex_pairwise_df = pd.read_csv(TABLE_DATASET_PATH)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Parse stringified lists into actual lists\n",
    "for col in ['sku_pos', 'sku_hard_neg', 'sku_soft_neg']:\n",
    "    pairwise_mapping_df[col] = pairwise_mapping_df[col].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else [])\n",
    "\n",
    "# --- Explode to long format: each row is a (sku_query, sku_pos/hard_neg/soft_neg) pair ---\n",
    "def explode_pairs(df, col, label):\n",
    "    return pd.DataFrame({\n",
    "        'sku_query': df['sku_query'].repeat(df[col].str.len()),\n",
    "        'sku_candidate': [sku for sublist in df[col] for sku in sublist],\n",
    "        'label': label\n",
    "    })\n",
    "\n",
    "pairs_pos = explode_pairs(pairwise_mapping_df, 'sku_pos', 1)\n",
    "pairs_hard_neg = explode_pairs(pairwise_mapping_df, 'sku_hard_neg', 0)\n",
    "pairs_soft_neg = explode_pairs(pairwise_mapping_df, 'sku_soft_neg', 0.5)\n",
    "\n",
    "# Combine all pairs\n",
    "regex_pairwise_df = pd.concat([pairs_pos, pairs_hard_neg, pairs_soft_neg], ignore_index=True)\n",
    "print(f\"Pairs: pos={len(pairs_pos)}, hard_neg={len(pairs_hard_neg)}, soft_neg={len(pairs_soft_neg)}, total={len(regex_pairwise_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def split_pairwise(\n",
    "    df: pd.DataFrame,\n",
    "    test_size: float = 0.20,\n",
    "    random_state: int | None = None,\n",
    "    use_all_train_pairs: bool = False,        # ← NEW\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Leakage-free DEV / TEST split for a pair-wise SKU dataset.\n",
    "\n",
    "    When `use_all_train_pairs=False` (default) the behaviour is identical to the\n",
    "    previous implementation: a single `sku_substitute` is chosen and paired with\n",
    "    every remaining positive / negative of the current group.\n",
    "\n",
    "    When `use_all_train_pairs=True` **every** remaining positive becomes an\n",
    "    anchor.  For each anchor we create:\n",
    "        • (anchor, other_positive)   for all other positives in the group\n",
    "        • (anchor, negative)         for every remaining negative in the group\n",
    "    Self-pairs are skipped.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain the columns ['sku_query', 'sku_candidate', 'label'].\n",
    "    test_size : float, optional\n",
    "        Fraction of positives *and* negatives to move to TEST, by default 0.20.\n",
    "    random_state : int | None, optional\n",
    "        Seed for the random number generator.\n",
    "    use_all_train_pairs : bool, optional\n",
    "        False → original «single substitute» logic.\n",
    "        True  → build full Cartesian train pairs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dev_df, test_df : Tuple[pd.DataFrame, pd.DataFrame]\n",
    "        The DEV (train/validation) and TEST splits.\n",
    "    \"\"\"\n",
    "    print(\"Building pairwise dataset...\")\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    test_rows, dev_rows = [], []\n",
    "    test_entities: set[str] = set()\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────────────\n",
    "    for q_sku, grp in df.groupby(\"sku_query\"):\n",
    "        pos_idx = grp.index[grp.label == 1].tolist()\n",
    "        neg_idx = grp.index[grp.label == 0].tolist()\n",
    "\n",
    "        # ---- 1) sample TEST rows --------------------------------------------\n",
    "        n_pos = int(np.ceil(test_size * len(pos_idx))) if pos_idx else 0\n",
    "        n_neg = int(np.ceil(test_size * len(neg_idx))) if neg_idx else 0\n",
    "\n",
    "        pos_test = rng.choice(pos_idx, size=n_pos, replace=False) if n_pos else []\n",
    "        neg_test = rng.choice(neg_idx, size=n_neg, replace=False) if n_neg else []\n",
    "\n",
    "        test_rows.extend(pos_test)\n",
    "        test_rows.extend(neg_test)\n",
    "\n",
    "        # track every entity that landed in TEST\n",
    "        test_entities.add(q_sku)\n",
    "        test_entities.update(df.loc[pos_test, \"sku_candidate\"])\n",
    "        test_entities.update(df.loc[neg_test, \"sku_candidate\"])\n",
    "\n",
    "        # ---- 2) build DEV rows ----------------------------------------------\n",
    "        remain_pos = list(set(pos_idx) - set(pos_test))\n",
    "        remain_neg = list(set(neg_idx) - set(neg_test))\n",
    "\n",
    "        if not remain_pos:                     # nothing left to anchor on\n",
    "            continue\n",
    "\n",
    "        # (a) ORIGINAL behaviour: one substitute ------------------------------\n",
    "        if not use_all_train_pairs:\n",
    "            sub_idx  = int(rng.choice(remain_pos))\n",
    "            sub_sku  = df.loc[sub_idx, \"sku_candidate\"]\n",
    "\n",
    "            # pair substitute with (other positives + negatives)\n",
    "            for idx in remain_pos:\n",
    "                if idx == sub_idx:             # skip self-pair\n",
    "                    continue\n",
    "                row = df.loc[idx].copy()\n",
    "                row[\"sku_query\"] = sub_sku\n",
    "                dev_rows.append(row)\n",
    "\n",
    "            for idx in remain_neg:\n",
    "                row = df.loc[idx].copy()\n",
    "                row[\"sku_query\"] = sub_sku\n",
    "                dev_rows.append(row)\n",
    "\n",
    "        # (b) NEW behaviour: full Cartesian ----------------------------------\n",
    "        else:\n",
    "            for anchor_idx in remain_pos:                      # every pos is anchor\n",
    "                anchor_sku = df.loc[anchor_idx, \"sku_candidate\"]\n",
    "\n",
    "                # anchor × (other positives)\n",
    "                for idx in remain_pos:\n",
    "                    if idx == anchor_idx:\n",
    "                        continue\n",
    "                    row = df.loc[idx].copy()\n",
    "                    row[\"sku_query\"] = anchor_sku\n",
    "                    dev_rows.append(row)\n",
    "\n",
    "                # anchor × (negatives)\n",
    "                for idx in remain_neg:\n",
    "                    row = df.loc[idx].copy()\n",
    "                    row[\"sku_query\"] = anchor_sku\n",
    "                    dev_rows.append(row)\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────────────\n",
    "    test_df = df.loc[test_rows].reset_index(drop=True)\n",
    "    dev_df  = pd.DataFrame(dev_rows).reset_index(drop=True)\n",
    "\n",
    "    test_df = (\n",
    "        df.loc[test_rows]\n",
    "          .drop_duplicates(subset=[\"sku_query\", \"sku_candidate\", \"label\"])\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    dev_df  = (\n",
    "        pd.DataFrame(dev_rows)\n",
    "          .drop_duplicates(subset=[\"sku_query\", \"sku_candidate\", \"label\"])\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # final purge: any pair touching a TEST entity is removed from DEV\n",
    "    mask = ~(dev_df[\"sku_query\"].isin(test_entities) |\n",
    "             dev_df[\"sku_candidate\"].isin(test_entities))\n",
    "    dev_df = dev_df[mask].reset_index(drop=True)\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return dev_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e12227f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building pairwise dataset...\n",
      "Done.\n",
      "Building pairwise dataset...\n",
      "Done.\n",
      "Split sizes: 80601 175 644\n",
      "Unique sku per split: 237 15 20\n"
     ]
    }
   ],
   "source": [
    "# split into train/val/test\n",
    "\n",
    "dev_df,  test_df  = split_pairwise(regex_pairwise_df,  test_size=TEST_SPLIT, random_state=42)\n",
    "train_df, val_df  = split_pairwise(dev_df,   test_size=VALIDATION_SPLIT, random_state=42, use_all_train_pairs=USE_ALL_TRAIN_PAIRS)\n",
    "\n",
    "print('Split sizes:', len(train_df), len(val_df), len(test_df))\n",
    "print('Unique sku per split:', train_df.sku_query.nunique(), val_df.sku_query.nunique(), test_df.sku_query.nunique(),)\n",
    "\n",
    "# Optionally: check label distribution\n",
    "print('Label counts (train):', train_df.label.value_counts().to_dict())\n",
    "print('Label counts (val):', val_df.label.value_counts().to_dict())\n",
    "print('Label counts (test):', test_df.label.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "efc1fa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       hard_negative  positive  total\n",
      "split                                \n",
      "train          69559     11042  80601\n",
      "val              146        29    175\n",
      "test             551        93    644\n"
     ]
    }
   ],
   "source": [
    "# Print positive/hard_negative pairs count per each split \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# collect your splits in a dict\n",
    "splits = {\n",
    "    'train': train_df,\n",
    "    'val':   val_df,\n",
    "    'test':  test_df,\n",
    "}\n",
    "\n",
    "# build the summary_df records\n",
    "records = []\n",
    "for name, df in splits.items():\n",
    "    vc = df['label'].value_counts()\n",
    "    records.append({\n",
    "        'split':    name,\n",
    "        'hard_negative': vc.get(0, 0),\n",
    "        'positive': vc.get(1, 0),\n",
    "        'total':    len(df),\n",
    "    })\n",
    "\n",
    "# create a DataFrame and set the split name as index\n",
    "summary_df = (\n",
    "    pd.DataFrame(records)\n",
    "      .set_index('split')\n",
    "      .astype(int)\n",
    ")\n",
    "\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1bed0574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All sanity checks passed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def sanity_checks(train: pd.DataFrame, val: pd.DataFrame, test: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Verify that:\n",
    "      1) Each query SKU appears in exactly one split\n",
    "      2) No SKU (query or candidate) overlaps across splits\n",
    "      3) No duplicate pairs across splits\n",
    "      4) Each split has at least one positive and one hard_negative\n",
    "    \"\"\"\n",
    "    # 1) Query-level disjointness\n",
    "    q_train = set(train[\"sku_query\"])\n",
    "    q_val   = set(val  [\"sku_query\"])\n",
    "    q_test  = set(test [\"sku_query\"])\n",
    "    assert not (q_train & q_val),   f\"Query SKU overlap train↔val: {q_train & q_val}\"\n",
    "    assert not (q_train & q_test),  f\"Query SKU overlap train↔test: {q_train & q_test}\"\n",
    "    assert not (q_val   & q_test),  f\"Query SKU overlap val↔test:   {q_val   & q_test}\"\n",
    "    \n",
    "    # 2) Global SKU disjointness (query OR candidate)\n",
    "    def all_skus(df):\n",
    "        return set(df[\"sku_query\"]) | set(df[\"sku_candidate\"])\n",
    "    s_train, s_val, s_test = all_skus(train), all_skus(val), all_skus(test)\n",
    "    assert not (s_train & s_val),   f\"SKU overlap train↔val: {s_train & s_val}\"\n",
    "    assert not (s_train & s_test),  f\"SKU overlap train↔test: {s_train & s_test}\"\n",
    "    assert not (s_val   & s_test),  f\"SKU overlap val↔test:   {s_val   & s_test}\"\n",
    "    \n",
    "    # 3) Unique pairs\n",
    "    all_pairs = pd.concat([train, val, test], ignore_index=True)\n",
    "    dupes = all_pairs.duplicated(subset=[\"sku_query\",\"sku_candidate\",\"label\"])\n",
    "    assert not dupes.any(), f\"Found {dupes.sum()} duplicate pairs across splits\"\n",
    "    \n",
    "    # 4) Label coverage in each split\n",
    "    for name, df in [(\"train\", train), (\"val\", val), (\"test\", test)]:\n",
    "        labels = df[\"label\"].unique()\n",
    "        assert set(labels) == {0,1}, f\"{name} split has labels {labels}, expected {{0,1}}\"\n",
    "    \n",
    "    print(\"✅ All sanity checks passed!\")\n",
    "\n",
    "sanity_checks(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b096d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename for compatibility with torch.Dataset\n",
    "\n",
    "def rename_cols(df):\n",
    "    df = df.rename(columns={\n",
    "        col: col.replace(\"_query\", \"_first\").replace(\"_candidate\", \"_second\")\n",
    "        for col in df.columns\n",
    "        if \"_query\" in col or \"_candidate\" in col\n",
    "    })\n",
    "    return df\n",
    "\n",
    "train_df = rename_cols(train_df)\n",
    "val_df = rename_cols(val_df)\n",
    "test_df = rename_cols(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0626ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for later usage\n",
    "\n",
    "folder_name = f'test={TEST_SPLIT}_val={VALIDATION_SPLIT}'\n",
    "\n",
    "# # TODO: different naming if need to evaluate on train split later\n",
    "# if USE_ALL_TRAIN_PAIRS:\n",
    "#     folder_name += '_use-all-train-pairs'\n",
    "#     folder_name += f'_max-samples-per-epoch={MAX_SAMPLES_PER_EPOCH}'\n",
    "\n",
    "dataset_name = Path(TABLE_DATASET_PATH).parts[1]\n",
    "\n",
    "common_file_prefix = (\n",
    "    Path(DATA_PATH) / dataset_name / 'processed' /\n",
    "    'pairwise-splits' / folder_name\n",
    ")\n",
    "common_file_prefix.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_df.to_csv(common_file_prefix / 'train.csv', index=False)\n",
    "val_df.to_csv(common_file_prefix / 'val.csv', index=False)\n",
    "test_df.to_csv(common_file_prefix / 'test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b4aa24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take samples for each split to debug on smaller subset if necessary\n",
    "\n",
    "def sample_split(df: pd.DataFrame, num_samples: int | None, random_state: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    If num_samples is set, take up to that many random rows;\n",
    "    otherwise just shuffle the entire DataFrame.\n",
    "    Always resets the index.\n",
    "    \"\"\"\n",
    "    if num_samples is not None:\n",
    "        n = min(num_samples, len(df))\n",
    "        out = df.sample(n=n, random_state=random_state)\n",
    "    else:\n",
    "        out = df.sample(frac=1, random_state=random_state)\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "actual_train_df = sample_split(train_df, NUM_DEBUG_SAMPLES, RANDOM_SEED)\n",
    "actual_val_df   = sample_split(val_df,   NUM_DEBUG_SAMPLES, RANDOM_SEED)\n",
    "actual_test_df  = sample_split(test_df,  NUM_DEBUG_SAMPLES, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d1d87f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: very important; set inverse target for siamese net:\n",
    "# label=0 (distance is minimal) -> new_label=1 (pair is similar)\n",
    "# label=1 (distance is maximal) -> new_label=0 (pair is dissimilar)\n",
    "\n",
    "actual_train_df[\"label\"] = 1 - actual_train_df[\"label\"]\n",
    "actual_val_df[\"label\"]   = 1 - actual_val_df[\"label\"]\n",
    "actual_test_df[\"label\"]  = 1 - actual_test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17b84c",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "96a54d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score                 # ← new\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def best_threshold(distances: torch.Tensor,\n",
    "                   labels:    torch.Tensor,\n",
    "                   steps:     int = 200,\n",
    "                   margin:    float = 1.5):\n",
    "    \"\"\"\n",
    "    Sweep `steps` evenly-spaced thresholds between 0 and `margin`\n",
    "    and return the one that maximises duplicate-class F1.\n",
    "    Labels: 0 = duplicate (positive), 1 = different (negative).\n",
    "    \"\"\"\n",
    "    d   = distances.detach().cpu().numpy()\n",
    "    y   = labels.detach().cpu().numpy()\n",
    "    thr = np.linspace(0.0, margin, steps)\n",
    "\n",
    "    best_f1, best_thr = -1.0, 0.0\n",
    "    for t in thr:\n",
    "        y_pred = (d < t).astype(int)          # 1 = duplicate prediction\n",
    "        f1     = f1_score(1 - y, y_pred)      # make 1 = positive for sklearn\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, t\n",
    "    return best_thr, best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6cb34367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3777/1435658876.py:20: RuntimeWarning: divide by zero encountered in divide\n",
      "  cls_weights    = 1.0 / cls_cnt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and optimizer…\n",
      "Done.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[175]\u001b[39m\u001b[32m, line 142\u001b[39m\n\u001b[32m    139\u001b[39m         mlflow.log_metric(\u001b[33m\"\u001b[39m\u001b[33mtest_f1_score\u001b[39m\u001b[33m\"\u001b[39m,     test_f1)\n\u001b[32m    140\u001b[39m         mlflow.end_run()\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[175]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36m_run\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# ---------- 3) training ----------\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tempfile.TemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m tmp_ckpt_dir:\n\u001b[32m     71\u001b[39m     (train_losses, val_losses,\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m      best_metric_val, best_weights, thr_history) = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprint_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodels_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtmp_ckpt_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBEST_CKPT_METRIC\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m→ Best validation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBEST_CKPT_METRIC\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_metric_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# ---------- 4) loss curves ----------\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[160]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, optimizer, criterion, epochs_num, train_loader, valid_loader, device, print_epoch, models_dir, metric)\u001b[39m\n\u001b[32m     47\u001b[39m out1, out2 = model(im1, n1, d1, im2, n2, d2)\n\u001b[32m     48\u001b[39m loss = criterion(out1, out2, lbl)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m optimizer.step()\n\u001b[32m     51\u001b[39m total_train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/clip-siamese/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/clip-siamese/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/clip-siamese/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "def _run():\n",
    "    images_dir = os.path.join(DATA_PATH, IMG_DATASET_NAME)\n",
    "\n",
    "    # ---------- 1) build DataLoaders ----------\n",
    "    splits  = {'train': actual_train_df,\n",
    "               'validation': actual_val_df,\n",
    "               'test': actual_test_df}\n",
    "    loaders = {}\n",
    "\n",
    "    for split_name, df in splits.items():\n",
    "        labels = df[\"label\"].values\n",
    "        ds     = SiameseRuCLIPDataset(df.drop(columns=\"label\"),\n",
    "                                      labels,\n",
    "                                      images_dir=images_dir)\n",
    "\n",
    "        if split_name == \"train\":\n",
    "            cls_cnt        = np.bincount(labels, minlength=2)\n",
    "            cls_weights    = 1.0 / cls_cnt\n",
    "            sample_weights = cls_weights[labels]\n",
    "            total = len(sample_weights)\n",
    "            max_n = MAX_SAMPLES_PER_EPOCH or total\n",
    "            n_samples = min(total, max_n)\n",
    "\n",
    "            sampler = WeightedRandomSampler(\n",
    "                sample_weights,\n",
    "                num_samples=n_samples,\n",
    "                replacement=True\n",
    "            )\n",
    "            loaders[split_name] = DataLoader(\n",
    "                ds,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                sampler=sampler,\n",
    "                num_workers=NUM_WORKERS\n",
    "            )\n",
    "        else:\n",
    "            loaders[split_name] = DataLoader(\n",
    "                ds,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=False,\n",
    "                num_workers=NUM_WORKERS\n",
    "            )\n",
    "\n",
    "    train_loader = loaders['train']\n",
    "    valid_loader = loaders['validation']\n",
    "    test_loader  = loaders['test']\n",
    "\n",
    "    # ---------- 2) model / optimiser ----------\n",
    "    print(\"Loading model and optimizer…\")\n",
    "    model = SiameseRuCLIP(\n",
    "        DEVICE, NAME_MODEL_NAME,\n",
    "        DESCRIPTION_MODEL_NAME,\n",
    "        PRELOAD_MODEL_NAME,\n",
    "        DATA_PATH + RESULTS_DIR,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    criterion = ContrastiveLoss(\n",
    "        margin=CONTRASTIVE_MARGIN,\n",
    "        pos_weight=POS_WEIGHT\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # ---------- 3) training ----------\n",
    "    with tempfile.TemporaryDirectory() as tmp_ckpt_dir:\n",
    "        (train_losses, val_losses,\n",
    "         best_metric_val, best_weights, thr_history) = train(\n",
    "            model, optimizer, criterion,\n",
    "            EPOCHS, train_loader, valid_loader,\n",
    "            print_epoch=True, device=DEVICE,\n",
    "            models_dir=tmp_ckpt_dir,\n",
    "            metric=BEST_CKPT_METRIC\n",
    "        )\n",
    "\n",
    "    print(f\"→ Best validation {BEST_CKPT_METRIC}: {best_metric_val:.3f}\")\n",
    "\n",
    "    # ---------- 4) loss curves ----------\n",
    "    epochs_ax = list(range(1, len(train_losses) + 1))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs_ax, train_losses, label='Train Loss')\n",
    "    ax.plot(epochs_ax, val_losses,   label='Val   Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training & Validation Loss by Epoch')\n",
    "    ax.legend()\n",
    "    if MLFLOW_URI:\n",
    "        mlflow.log_figure(fig, 'loss_by_epoch.png')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- 5) pick threshold for the *best* model ----------\n",
    "    model.load_state_dict(best_weights)\n",
    "    (_, _, _, _, _, best_thr) = validation(\n",
    "        model, criterion, valid_loader,\n",
    "        epoch='best', device=DEVICE,\n",
    "        split_name='validation',\n",
    "        threshold=None,\n",
    "        metric=BEST_CKPT_METRIC\n",
    "    )\n",
    "    print(f\"Chosen threshold from validation: {best_thr:.3f}\")\n",
    "\n",
    "    # ---------- 6) final TEST ----------\n",
    "    (test_pos_acc, test_neg_acc,\n",
    "     test_acc, test_f1,\n",
    "     test_loss, _) = validation(\n",
    "        model, criterion, test_loader,\n",
    "        epoch='test', device=DEVICE,\n",
    "        split_name='test',\n",
    "        threshold=best_thr,\n",
    "        metric=BEST_CKPT_METRIC\n",
    "    )\n",
    "\n",
    "    # pick out the right test-metric value\n",
    "    test_metric = test_pos_acc if BEST_CKPT_METRIC == 'pos_acc' else test_f1\n",
    "    print(f\"Test {BEST_CKPT_METRIC}: {test_metric:.3f}\")\n",
    "\n",
    "    # ---------- 7) save checkpoint ----------\n",
    "    filename = (\n",
    "        f\"siamese_contrastive_test-{BEST_CKPT_METRIC}={test_metric:.3f}\"\n",
    "        f\"{'_' + MODEL_NAME_POSTFIX if MODEL_NAME_POSTFIX else ''}\"\n",
    "        f\"{'_' + PRELOAD_MODEL_NAME  if PRELOAD_MODEL_NAME else ''}\"\n",
    "        f\"_best-thr={best_thr:.3f}.pt\"\n",
    "    )\n",
    "    final_path = Path(DATA_PATH + RESULTS_DIR) / filename\n",
    "    final_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(best_weights, final_path)\n",
    "    print(f\"Saved best‐{BEST_CKPT_METRIC} checkpoint to {final_path}\")\n",
    "\n",
    "    if MLFLOW_URI:\n",
    "        mlflow.log_metric(\"test_pos_accuracy\", test_pos_acc)\n",
    "        mlflow.log_metric(\"test_neg_accuracy\", test_neg_acc)\n",
    "        mlflow.log_metric(\"test_accuracy\",     test_acc)\n",
    "        mlflow.log_metric(\"test_f1_score\",     test_f1)\n",
    "        mlflow.end_run()\n",
    "\n",
    "_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

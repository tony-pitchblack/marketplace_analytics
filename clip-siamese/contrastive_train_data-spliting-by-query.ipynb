{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcaee945",
   "metadata": {},
   "source": [
    "# Installs & tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deaea7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    from hdbscan import HDBSCAN\n",
    "except ImportError:\n",
    "    !pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "befae79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import mlflow\n",
    "except ImportError:\n",
    "    !pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65f9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import dotenv\n",
    "except ImportError:\n",
    "    !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c3f67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Kaggle environment. Skipping Kaggle secrets.\n",
      "Trying to load HF_TOKEN from .env.\n",
      "Success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Log into huggingface via Kaggle Secrets or .env\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import huggingface_hub\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "    user_secrets = UserSecretsClient()\n",
    "    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not Kaggle environment. Skipping Kaggle secrets.\")\n",
    "    print(\"Trying to load HF_TOKEN from .env.\")\n",
    "    load_dotenv()\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "    print(\"Success!\")\n",
    "\n",
    "huggingface_hub.login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a0467",
   "metadata": {},
   "source": [
    "# Choose notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5360940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging / file management parameters\n",
    "\n",
    "MODEL_NAME_POSTFIX = 'splitting-by-query'\n",
    "DATA_PATH = 'data/'\n",
    "RESULTS_DIR = 'train_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be91836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "## CHOOSE MODEL PARAMETERS #################################################\n",
    "\n",
    "# NAME_MODEL_NAME = 'cointegrated/rubert-tiny' # 'DeepPavlov/distilrubert-tiny-cased-conversational-v1'\n",
    "# DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny'\n",
    "# PRELOAD_MODEL_NAME = None\n",
    "\n",
    "NAME_MODEL_NAME = None\n",
    "DESCRIPTION_MODEL_NAME = None\n",
    "PRELOAD_MODEL_NAME = 'cc12m_rubert_tiny_ep_1.pt' # preload ruclip\n",
    "\n",
    "DROPOUT = 0.5\n",
    "# DROPOUT = None\n",
    "\n",
    "BEST_CKPT_METRIC = 'f1'\n",
    "# BEST_CKPT_METRIC = 'pos_acc'\n",
    "\n",
    "MOMENTUM=0.9\n",
    "WEIGHT_DECAY=1e-2\n",
    "CONTRASTIVE_THRESHOLD=0.3\n",
    "\n",
    "TEST_RATIO = 0.1\n",
    "VAL_RATIO = 0.1\n",
    "\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "RANDOM_SEED=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a30e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose parameters for training\n",
    "\n",
    "# # GPU config (large)\n",
    "# TODO: introduce gradient accumulation\n",
    "# BATCH_SIZE_PER_DEVICE=60\n",
    "# EPOCHS=10\n",
    "# POS_NEG_RATIO=1.0\n",
    "# HARD_SOFT_RATIO=0.7 # TODO: 0.85\n",
    "# LIMIT_TRAIN_POS_PAIRS_PER_QUERY=5000\n",
    "# LIMIT_VAL_POS_PAIRS_PER_QUERY=None\n",
    "# LIMIT_TEST_POS_PAIRS_PER_QUERY=None\n",
    "# LIMIT_QUERIES = None\n",
    "# SHEDULER_PATIENCE=1 # in epochs\n",
    "# LR=3e-5\n",
    "# CONTRASTIVE_MARGIN=1.0\n",
    "\n",
    "# # GPU config (small-medium)\n",
    "# BATCH_SIZE_PER_DEVICE=60\n",
    "# EPOCHS=20\n",
    "# POS_NEG_RATIO=1.0\n",
    "# HARD_SOFT_RATIO=0.7\n",
    "# LIMIT_TRAIN_POS_PAIRS_PER_QUERY=50 # 50 for small; 500 for medium\n",
    "# LIMIT_VAL_POS_PAIRS_PER_QUERY=None\n",
    "# LIMIT_TEST_POS_PAIRS_PER_QUERY=None\n",
    "# LIMIT_QUERIES = None\n",
    "# SHEDULER_PATIENCE=3 # in epochs\n",
    "# LR=9e-5\n",
    "# CONTRASTIVE_MARGIN=1.5\n",
    "\n",
    "# CPU smoke test config\n",
    "BATCH_SIZE_PER_DEVICE=1\n",
    "EPOCHS=1\n",
    "POS_NEG_RATIO=1.0\n",
    "HARD_SOFT_RATIO=0.5\n",
    "LIMIT_TRAIN_POS_PAIRS_PER_QUERY=2\n",
    "LIMIT_VAL_POS_PAIRS_PER_QUERY=2\n",
    "LIMIT_TEST_POS_PAIRS_PER_QUERY=2\n",
    "LIMIT_QUERIES = 2\n",
    "SHEDULER_PATIENCE=3 # in epochs\n",
    "LR=9e-5\n",
    "CONTRASTIVE_MARGIN=1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "880a333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHOOSE DATA #########################################################\n",
    "\n",
    "DATA_PATH=  'data/'\n",
    "SOURCE_TABLE_NAME = 'tables_OZ_geo_5500/processed/OZ_geo_5500.csv'\n",
    "\n",
    "# --- Load source_df and pairwise_mapping_df from Parquet ---\n",
    "SOURCE_TABLE_NAME = 'tables_OZ_geo_5500/processed/OZ_geo_5500.csv' # TODO: OZ_geo_5500_manual-edit.csv\n",
    "PAIRWISE_TABLE_NAME = 'tables_OZ_geo_5500/processed/regex-pairwise-groups/regex-pairwise-groups_num-queries=20_patterns-dict-hash=a6223255f273e52a893ba7235e3c19b3/mapping.parquet'\n",
    "IMG_DATASET_NAME = 'images_OZ_geo_5500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b5eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOGGING PARAMS ######################################################################\n",
    "\n",
    "# MLFLOW_URI = \"http://176.56.185.96:5000\"\n",
    "# MLFLOW_URI = \"http://localhost:5000\"\n",
    "MLFLOW_URI = None\n",
    "\n",
    "MLFLOW_EXPERIMENT = \"siamese/1fold\"\n",
    "\n",
    "TELEGRAM_TOKEN = None\n",
    "# TELEGRAM_TOKEN = '' # set token to get notifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b40bc83",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00017085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "from timm import create_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "# import transformers\n",
    "# from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer,\\\n",
    "#         get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# import json\n",
    "# from itertools import product\n",
    "\n",
    "# import datasets\n",
    "# from datasets import Dataset, concatenate_datasets\n",
    "# import argparse\n",
    "import requests\n",
    "\n",
    "# from io import BytesIO\n",
    "# from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "# import more_itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import mlflow\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3494173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tg_report(text, token=None) -> None:\n",
    "    method = 'sendMessage'\n",
    "    chat_id = 324956476\n",
    "    _ = requests.post(\n",
    "            url='https://api.telegram.org/bot{0}/{1}'.format(token, method),\n",
    "            data={'chat_id': chat_id, 'text': text} \n",
    "        ).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34706f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuCLIPtiny(nn.Module):\n",
    "    def __init__(self, name_model_name):\n",
    "        super().__init__()\n",
    "        self.visual = create_model('convnext_tiny',\n",
    "                                   pretrained=False, # TODO: берём претрейн\n",
    "                                   num_classes=0,\n",
    "                                   in_chans=3)  # out 768\n",
    "\n",
    "        self.transformer = AutoModel.from_pretrained(name_model_name)\n",
    "        name_model_output_shape = self.transformer.config.hidden_size  # dynamically get hidden size\n",
    "        self.final_ln = torch.nn.Linear(name_model_output_shape, 768)  # now uses the transformer hidden size\n",
    "        self.logit_scale = torch.nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.stem[0].weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        x = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = x.last_hidden_state[:, 0, :]\n",
    "        x = self.final_ln(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(input_ids, attention_mask)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84518fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        _convert_image_to_rgb,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]), ])\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "class Tokenizers:\n",
    "    def __init__(self):\n",
    "        self.name_tokenizer = AutoTokenizer.from_pretrained(NAME_MODEL_NAME)\n",
    "        self.desc_tokenizer = AutoTokenizer.from_pretrained(DESCRIPTION_MODEL_NAME)\n",
    "\n",
    "    def tokenize_name(self, texts, max_len=77):\n",
    "        tokenized = self.name_tokenizer.batch_encode_plus(texts,\n",
    "                                                     truncation=True,\n",
    "                                                     add_special_tokens=True,\n",
    "                                                     max_length=max_len,\n",
    "                                                     padding='max_length',\n",
    "                                                     return_attention_mask=True,\n",
    "                                                     return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]], dim=1)\n",
    "    \n",
    "    def tokenize_description(self, texts, max_len=77):\n",
    "        tokenized = self.desc_tokenizer(texts,\n",
    "                                        truncation=True,\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=max_len,\n",
    "                                        padding='max_length',\n",
    "                                        return_attention_mask=True,\n",
    "                                        return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]], dim=1)\n",
    "\n",
    "class SiameseRuCLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df=None, labels=None, df_path=None, images_dir=DATA_PATH+'images/'):\n",
    "        # loads data either from path using `df_path` or directly from `df` argument\n",
    "        self.df = pd.read_csv(df_path) if df_path is not None else df\n",
    "        self.labels = labels\n",
    "        self.images_dir = images_dir\n",
    "        self.tokenizers = Tokenizers()\n",
    "        self.transform = get_transform()\n",
    "        # \n",
    "        self.max_len = 77\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        name_tokens = self.tokenizers.tokenize_name([str(row.name_first), \n",
    "                                               str(row.name_second)], max_len=self.max_len)\n",
    "        name_first = name_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        name_second = name_tokens[:, 1, :]\n",
    "        desc_tokens = self.tokenizers.tokenize_description([str(row.description_first), \n",
    "                                               str(row.description_second)])\n",
    "        desc_first = desc_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        desc_second = desc_tokens[:, 1, :]\n",
    "        im_first = cv2.imread(os.path.join(self.images_dir, row.image_name_first))\n",
    "        im_first = cv2.cvtColor(im_first, cv2.COLOR_BGR2RGB)\n",
    "        im_first = Image.fromarray(im_first)\n",
    "        im_first = self.transform(im_first)\n",
    "        im_second = cv2.imread(os.path.join(self.images_dir, row.image_name_second))\n",
    "        im_second = cv2.cvtColor(im_second, cv2.COLOR_BGR2RGB)\n",
    "        im_second = Image.fromarray(im_second)\n",
    "        im_second = self.transform(im_second)\n",
    "        label = self.labels[idx]\n",
    "        return im_first, name_first, desc_first, im_second, name_second, desc_second, label\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2d263be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "class SiameseRuCLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 device: str,\n",
    "                 name_model_name: str,\n",
    "                 description_model_name: str,\n",
    "                 preload_model_name: str = None,\n",
    "                 models_dir: str = None,\n",
    "                 dropout: float = None):\n",
    "        \"\"\"\n",
    "        Initializes the SiameseRuCLIP model.\n",
    "        Required parameters:\n",
    "          - models_dir: directory containing saved checkpoints.\n",
    "          - name_model_name: model name for text (name) branch.\n",
    "          - description_model_name: model name for description branch.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        device = torch.device(device)\n",
    "\n",
    "        # Initialize RuCLIPtiny\n",
    "        self.ruclip = RuCLIPtiny(name_model_name)\n",
    "        if preload_model_name is not None:\n",
    "            std = torch.load(\n",
    "                os.path.join(models_dir, preload_model_name),\n",
    "                weights_only=True,\n",
    "                map_location=device\n",
    "            )\n",
    "            self.ruclip.load_state_dict(std)\n",
    "            self.ruclip.eval()\n",
    "        self.ruclip = self.ruclip.to(device)\n",
    "\n",
    "        # Initialize the description transformer\n",
    "        self.description_transformer = AutoModel.from_pretrained(description_model_name)\n",
    "        self.description_transformer = self.description_transformer.to(device)\n",
    "\n",
    "        # Determine dimensionality\n",
    "        vision_dim = self.ruclip.visual.num_features\n",
    "        name_dim = self.ruclip.final_ln.out_features\n",
    "        desc_dim = self.description_transformer.config.hidden_size\n",
    "        self.hidden_dim = vision_dim + name_dim + desc_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define MLP head with optional dropout\n",
    "        layers = [\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            *( [nn.Dropout(self.dropout)] if self.dropout is not None else [] ),\n",
    "            nn.Linear(self.hidden_dim // 2, self.hidden_dim // 4),\n",
    "        ]\n",
    "        self.head = nn.Sequential(*layers).to(device)\n",
    "\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.ruclip.encode_image(image)\n",
    "\n",
    "    def encode_name(self, name):\n",
    "        return self.ruclip.encode_text(name[:, 0, :], name[:, 1, :])\n",
    "\n",
    "    def encode_description(self, desc):\n",
    "        last_hidden_states = self.description_transformer(desc[:, 0, :], desc[:, 1, :]).last_hidden_state\n",
    "        attention_mask = desc[:, 1, :]\n",
    "        return average_pool(last_hidden_states, attention_mask)\n",
    "\n",
    "    def get_final_embedding(self, im, name, desc):\n",
    "        image_emb = self.encode_image(im)\n",
    "        name_emb = self.encode_name(name)\n",
    "        desc_emb = self.encode_description(desc)\n",
    "\n",
    "        # Concatenate the embeddings and forward through the head\n",
    "        combined_emb = torch.cat([image_emb, name_emb, desc_emb], dim=1)\n",
    "        final_embedding = self.head(combined_emb)\n",
    "        return final_embedding\n",
    "\n",
    "    def forward(self, im1, name1, desc1, im2, name2, desc2):\n",
    "        out1 = self.get_final_embedding(im1, name1, desc1)\n",
    "        out2 = self.get_final_embedding(im2, name2, desc2)\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "987d1ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def __name__(self,):\n",
    "        return 'ContrastiveLoss'\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        pos = (1-label) * torch.pow(euclidean_distance, 2)\n",
    "        neg = label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        loss_contrastive = torch.mean( pos + neg )\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfe81dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot epoch after each train epoch in `train()`\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_epoch(loss_history, filename=\"data/runs_artifacts/epoch_loss.png\") -> None:\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Iteration number\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(loss_history, 'b')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)  # Save the plot to a file\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9c6281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pair(output1, output2, target, threshold):\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    # меньше границы, там где будет True — конкуренты\n",
    "    cond = euclidean_distance < threshold\n",
    "    pos_sum = 0\n",
    "    neg_sum = 0\n",
    "    pos_acc = 0\n",
    "    neg_acc = 0\n",
    "\n",
    "    for i in range(len(cond)):\n",
    "        # 1 значит не конкуренты\n",
    "        if target[i]:\n",
    "            neg_sum+=1\n",
    "            # 0 в cond значит дальше друг от друга чем threshold\n",
    "            if not cond[i]:\n",
    "                neg_acc+=1\n",
    "        elif not target[i]:\n",
    "            pos_sum+=1\n",
    "            if cond[i]:\n",
    "                pos_acc+=1\n",
    "\n",
    "    return pos_acc, pos_sum, neg_acc, neg_sum\n",
    "\n",
    "def predict(out1, out2, threshold=CONTRASTIVE_THRESHOLD):\n",
    "    # вернёт 1 если похожи\n",
    "    return F.pairwise_distance(out1, out2) < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae17e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sku_to_model_inputs(sku_list, source_df, images_dir, tokenizers, transform):\n",
    "    \"\"\"\n",
    "    Convert list of SKUs to model inputs (images, names, descriptions).\n",
    "    This is the key function that bridges SKU IDs to actual model data.\n",
    "    \"\"\"\n",
    "    # Get data from source_df\n",
    "    sku_data = source_df.loc[source_df['sku'].isin(sku_list)].set_index('sku')\n",
    "    \n",
    "    images = []\n",
    "    names = []\n",
    "    descriptions = []\n",
    "    \n",
    "    for sku in sku_list:\n",
    "        if sku in sku_data.index:\n",
    "            row = sku_data.loc[sku]\n",
    "            \n",
    "            # Load and transform image\n",
    "            import cv2\n",
    "            from PIL import Image\n",
    "            img_path = os.path.join(images_dir, row['image_name'])\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = Image.fromarray(img)\n",
    "            img = transform(img)\n",
    "            images.append(img)\n",
    "            \n",
    "            # Get text data\n",
    "            names.append(str(row['name']))\n",
    "            descriptions.append(str(row['description']))\n",
    "        else:\n",
    "            # Handle missing SKU - create dummy data\n",
    "            images.append(torch.zeros(3, 224, 224))  # Dummy image\n",
    "            names.append(\"Unknown Product\")\n",
    "            descriptions.append(\"No description available\")\n",
    "    \n",
    "    # Stack images and tokenize text\n",
    "    images = torch.stack(images)\n",
    "    name_tokens = tokenizers.tokenize_name(names)\n",
    "    desc_tokens = tokenizers.tokenize_description(descriptions)\n",
    "    \n",
    "    return images, name_tokens, desc_tokens\n",
    "\n",
    "\n",
    "def train_with_new_dataset(model, optimizer, criterion, epochs_num, train_loader, \n",
    "                          valid_loader=None, device='cpu', print_epoch=False, \n",
    "                          models_dir=None, metric='f1', source_df=None, images_dir=None):\n",
    "    \"\"\"\n",
    "    Updated training function that handles PairGenerationDataset batch format.\n",
    "    \"\"\"\n",
    "    from copy import deepcopy\n",
    "    from timm import create_model\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    \n",
    "    assert metric in ('f1', 'pos_acc'), \"metric must be 'f1' or 'pos_acc'\"\n",
    "\n",
    "    model.to(device)\n",
    "    train_losses, val_losses, thr_history = [], [], []\n",
    "    best_valid_metric, best_threshold = float('-inf'), None\n",
    "    best_weights = None\n",
    "\n",
    "    # Initialize tokenizers and transform for SKU-to-input conversion\n",
    "    tokenizers = Tokenizers()  # Your existing tokenizer class\n",
    "    transform = get_transform()   # Your existing transform function\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"max\",\n",
    "        factor=0.1,\n",
    "        patience=SHEDULER_PATIENCE,\n",
    "        threshold=1e-4,\n",
    "        threshold_mode='rel'\n",
    "    )\n",
    "\n",
    "    if models_dir:\n",
    "        Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, epochs_num + 1):\n",
    "        # ---- training ----\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "            # Handle PairGenerationDataset format (dict) vs old format (tuple)\n",
    "            if isinstance(batch, dict):\n",
    "                # New format: batch is a dict with keys like 'sku_first', 'sku_second', 'label'\n",
    "                sku_first = batch['sku_first']\n",
    "                sku_second = batch['sku_second'] \n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # Convert SKUs to model inputs\n",
    "                im1, n1, d1 = sku_to_model_inputs(sku_first.tolist(), source_df, images_dir, tokenizers, transform)\n",
    "                im2, n2, d2 = sku_to_model_inputs(sku_second.tolist(), source_df, images_dir, tokenizers, transform)\n",
    "                \n",
    "                # Move to device\n",
    "                im1, n1, d1 = im1.to(device), n1.to(device), d1.to(device)\n",
    "                im2, n2, d2 = im2.to(device), n2.to(device), d2.to(device)\n",
    "                \n",
    "            else:\n",
    "                # Old format: tuple of tensors\n",
    "                im1, n1, d1, im2, n2, d2, labels = [t.to(device) for t in batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out1, out2 = model(im1, n1, d1, im2, n2, d2)\n",
    "            loss = criterion(out1, out2, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "        train_losses.append(total_train_loss / len(train_loader))\n",
    "\n",
    "        # ---- evaluation & checkpointing ----\n",
    "        if print_epoch and valid_loader is not None:\n",
    "            pos_acc, neg_acc, avg_acc, f1_val, val_loss, val_thr = evaluation(\n",
    "                model, criterion, valid_loader, epoch, device=device,\n",
    "                split_name='val', threshold=None, margin=CONTRASTIVE_MARGIN,\n",
    "                steps=200, metric=metric, source_df=source_df, images_dir=images_dir\n",
    "            )\n",
    "            val_losses.append(val_loss)\n",
    "            thr_history.append(val_thr)\n",
    "\n",
    "            # pick the metric value to step & compare\n",
    "            cur_metric = pos_acc if metric == 'pos_acc' else f1_val\n",
    "            scheduler.step(cur_metric)\n",
    "\n",
    "            # save checkpoint every epoch if requested\n",
    "            if models_dir:\n",
    "                torch.save(model.state_dict(),\n",
    "                           Path(models_dir) / f\"checkpoint_epoch_{epoch}.pt\")\n",
    "\n",
    "            # update best if improved\n",
    "            if cur_metric > best_valid_metric:\n",
    "                best_valid_metric = cur_metric\n",
    "                best_threshold     = val_thr\n",
    "                best_weights       = deepcopy(model.state_dict())\n",
    "\n",
    "        print(f'Epoch {epoch} done.')\n",
    "\n",
    "    print(f\"Best evaluation {metric}: {best_valid_metric:.3f}  (thr={best_threshold:.3f})\")\n",
    "    return train_losses, val_losses, best_valid_metric, best_weights, thr_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "319a25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "import mlflow\n",
    "from copy import deepcopy\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pathlib import Path\n",
    "\n",
    "def evaluation(model, criterion, data_loader, epoch, device='cpu',\n",
    "              split_name='val', threshold=None, margin=1.5,\n",
    "              steps=200, metric='f1', source_df=None, images_dir=None,\n",
    "              precompute_pairs=False):\n",
    "    \"\"\"\n",
    "    Evaluation function that handles both precomputed and on-the-fly data formats.\n",
    "    \n",
    "    Args:\n",
    "        precompute_pairs: If True, expects PrecomputedPairDataset format with pre-loaded tensors.\n",
    "                         If False, expects PairGenerationDataset format and converts SKUs to tensors.\n",
    "    \"\"\"\n",
    "    assert metric in ('f1', 'pos_acc'), \"metric must be 'f1' or 'pos_acc'\"\n",
    "    assert split_name in ('val', 'test'), \"split_name must be 'val' or 'test'\"\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_d, all_lbl = [], []\n",
    "\n",
    "    # Only initialize tokenizers if not using precomputed pairs\n",
    "    if not precompute_pairs:\n",
    "        tokenizers = Tokenizers()\n",
    "        transform = get_transform()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=f\"Evaluation on {split_name}\"):\n",
    "            if precompute_pairs:\n",
    "                # PrecomputedPairDataset format - data is already tensors\n",
    "                im1 = batch['image_first'].to(device)\n",
    "                n1 = batch['name_first'].to(device)\n",
    "                d1 = batch['desc_first'].to(device)\n",
    "                im2 = batch['image_second'].to(device)\n",
    "                n2 = batch['name_second'].to(device)\n",
    "                d2 = batch['desc_second'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "            else:\n",
    "                # PairGenerationDataset format - convert SKUs to tensors\n",
    "                sku_first = batch['sku_first']\n",
    "                sku_second = batch['sku_second']\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # Convert SKUs to model inputs\n",
    "                im1, n1, d1 = sku_to_model_inputs(sku_first.tolist(), source_df, images_dir, tokenizers, transform)\n",
    "                im2, n2, d2 = sku_to_model_inputs(sku_second.tolist(), source_df, images_dir, tokenizers, transform)\n",
    "                \n",
    "                # Move to device\n",
    "                im1, n1, d1 = im1.to(device), n1.to(device), d1.to(device)\n",
    "                im2, n2, d2 = im2.to(device), n2.to(device), d2.to(device)\n",
    "            \n",
    "            out1, out2 = model(im1, n1, d1, im2, n2, d2)\n",
    "            total_loss += criterion(out1, out2, labels).item()\n",
    "            all_d.append(F.pairwise_distance(out1, out2).cpu())\n",
    "            all_lbl.append(labels.cpu())\n",
    "\n",
    "    distances = torch.cat(all_d)\n",
    "    labels = torch.cat(all_lbl)\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # === threshold sweep ===\n",
    "    if threshold is None:\n",
    "        grid = np.linspace(0.0, margin, steps)\n",
    "        best_val, best_thr = -1.0, 0.0\n",
    "        y_true = (labels.numpy() == 0).astype(int)\n",
    "        for t in grid:\n",
    "            y_pred = (distances.numpy() < t).astype(int)\n",
    "            if metric == 'f1':\n",
    "                val = f1_score(y_true, y_pred, zero_division=0)\n",
    "            else:  # metric == 'pos_acc'\n",
    "                pos_mask = (y_true == 1)\n",
    "                val = (y_pred[pos_mask] == 1).mean() if pos_mask.sum() > 0 else 0.0\n",
    "            if val > best_val:\n",
    "                best_val, best_thr = val, t\n",
    "        threshold = best_thr\n",
    "    else:\n",
    "        best_thr = threshold\n",
    "\n",
    "    # === final metrics at chosen threshold ===\n",
    "    preds = (distances < threshold).long()\n",
    "    pos_mask = (labels == 0)\n",
    "    neg_mask = (labels == 1)\n",
    "\n",
    "    pos_acc = (preds[pos_mask] == 1).float().mean().item() if pos_mask.any() else 0.0\n",
    "    neg_acc = (preds[neg_mask] == 0).float().mean().item() if neg_mask.any() else 0.0\n",
    "    avg_acc = (pos_acc + neg_acc) / 2.0\n",
    "    f1 = f1_score((labels.numpy() == 0).astype(int),\n",
    "                  preds.numpy(), zero_division=0)\n",
    "\n",
    "    # log to console\n",
    "    report = (f\"[{split_name}] Epoch {epoch} – \"\n",
    "              f\"loss: {avg_loss:.4f}, \"\n",
    "              f\"P Acc: {pos_acc:.3f}, \"\n",
    "              f\"N Acc: {neg_acc:.3f}, \"\n",
    "              f\"Avg Acc: {avg_acc:.3f}, \"\n",
    "              f\"F1: {f1:.3f}, \"\n",
    "              f\"thr*: {threshold:.3f} \"\n",
    "              f\"(optimised: {metric})\")\n",
    "    print(report)\n",
    "    make_tg_report(report, TELEGRAM_TOKEN)\n",
    "\n",
    "    # log to MLflow\n",
    "    if MLFLOW_URI and split_name == 'val':\n",
    "        if metric == 'f1':\n",
    "            mlflow.log_metric(\"valid_f1_score\", f1, step=epoch)\n",
    "        else:\n",
    "            mlflow.log_metric(\"valid_pos_accuracy\", pos_acc, step=epoch)\n",
    "\n",
    "    return pos_acc, neg_acc, avg_acc, f1, avg_loss, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff968d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from datetime import timedelta\n",
    "\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          criterion,\n",
    "          epochs_num,\n",
    "          train_loader,\n",
    "          valid_loader=None,\n",
    "          device='cpu',\n",
    "          print_epoch=False,\n",
    "          models_dir=None,\n",
    "          metric='f1'):\n",
    "    \"\"\"\n",
    "    Trains for `epochs_num` epochs, using `evaluation(..., metric=metric)` each epoch.\n",
    "    Uses the same `metric` to step the LR scheduler and to pick the best checkpoint.\n",
    "\n",
    "    Returns:\n",
    "      train_losses, val_losses, best_valid_metric, best_weights, thr_history\n",
    "    \"\"\"\n",
    "    assert metric in ('f1', 'pos_acc'), \"metric must be 'f1' or 'pos_acc'\"\n",
    "\n",
    "    model.to(device)\n",
    "    train_losses, val_losses, thr_history = [], [], []\n",
    "    best_valid_metric, best_threshold = float('-inf'), None\n",
    "    best_weights = None\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"max\",\n",
    "        factor=0.1,\n",
    "        patience=SHEDULER_PATIENCE,\n",
    "        threshold=1e-4,\n",
    "        threshold_mode='rel'\n",
    "    )\n",
    "\n",
    "    if models_dir:\n",
    "        Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, epochs_num + 1):\n",
    "        # ---- training ----\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            im1, n1, d1, im2, n2, d2, lbl = [t.to(device) for t in batch]\n",
    "            optimizer.zero_grad()\n",
    "            out1, out2 = model(im1, n1, d1, im2, n2, d2)\n",
    "            loss = criterion(out1, out2, lbl)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        train_losses.append(total_train_loss / len(train_loader))\n",
    "\n",
    "        # ---- evaluation & checkpointing ----\n",
    "        if print_epoch and valid_loader is not None:\n",
    "            pos_acc, neg_acc, avg_acc, f1_val, val_loss, val_thr = evaluation(\n",
    "                model,\n",
    "                criterion,\n",
    "                valid_loader,\n",
    "                epoch,\n",
    "                device=device,\n",
    "                split_name='val',\n",
    "                threshold=None,\n",
    "                margin=CONTRASTIVE_MARGIN,\n",
    "                steps=200,\n",
    "                metric=metric\n",
    "            )\n",
    "            val_losses.append(val_loss)\n",
    "            thr_history.append(val_thr)\n",
    "\n",
    "            # pick the metric value to step & compare\n",
    "            cur_metric = pos_acc if metric == 'pos_acc' else f1_val\n",
    "            scheduler.step(cur_metric)\n",
    "\n",
    "            # save checkpoint every epoch if requested\n",
    "            if models_dir:\n",
    "                torch.save(model.state_dict(),\n",
    "                           Path(models_dir) / f\"checkpoint_epoch_{epoch}.pt\")\n",
    "\n",
    "            # update best if improved\n",
    "            if cur_metric > best_valid_metric:\n",
    "                best_valid_metric = cur_metric\n",
    "                best_threshold     = val_thr\n",
    "                best_weights       = deepcopy(model.state_dict())\n",
    "\n",
    "        print(f'Epoch {epoch} done.')\n",
    "\n",
    "    print(f\"Best evaluation {metric}: {best_valid_metric:.3f}  (thr={best_threshold:.3f})\")\n",
    "    return train_losses, val_losses, best_valid_metric, best_weights, thr_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481940bc",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226754b",
   "metadata": {},
   "source": [
    "## Download data from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b8cde88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6536560244014f9f901fbb112ec879ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download models' weights & text/image datasets\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ID = \"INDEEPA/clip-siamese\"\n",
    "LOCAL_DIR = Path(\"data/train_results\")\n",
    "LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type='dataset',\n",
    "    local_dir='data',\n",
    "    allow_patterns=[\n",
    "        f\"train_results/{PRELOAD_MODEL_NAME}\",\n",
    "        SOURCE_TABLE_NAME,\n",
    "        PAIRWISE_TABLE_NAME,\n",
    "        f\"{IMG_DATASET_NAME}.zip\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Unzip the image dataset\n",
    "import zipfile\n",
    "with zipfile.ZipFile(f\"data/{IMG_DATASET_NAME}.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "be5962fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sku',\n",
       " 'description',\n",
       " 'image_url',\n",
       " 'name',\n",
       " 'category',\n",
       " 'схема',\n",
       " 'brand',\n",
       " 'niche',\n",
       " 'seller',\n",
       " 'balance_fbo',\n",
       " 'balance_fbs',\n",
       " 'warehouses_count',\n",
       " 'comments',\n",
       " 'final_price',\n",
       " 'max_price',\n",
       " 'min_price',\n",
       " 'average_price',\n",
       " 'median_price',\n",
       " 'membership_card_price',\n",
       " 'sales',\n",
       " 'revenue',\n",
       " 'revenue_potential',\n",
       " 'revenue_average',\n",
       " 'lost_profit',\n",
       " 'lost_profit_percent',\n",
       " 'url',\n",
       " 'thumb',\n",
       " 'pics_count',\n",
       " 'has_video',\n",
       " 'first_date',\n",
       " 'days_in_website',\n",
       " 'days_in_stock',\n",
       " 'days_with_sales',\n",
       " 'average_if_in_stock',\n",
       " 'rating',\n",
       " 'fbs',\n",
       " 'base_price',\n",
       " 'category_position',\n",
       " 'categories_last_count',\n",
       " 'sales_per_day_average',\n",
       " 'sales.1',\n",
       " 'frozen_stocks',\n",
       " 'frozen_stocks_cost',\n",
       " 'frozen_stocks_percent',\n",
       " 'balance',\n",
       " 'image_name']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_df = pd.read_csv(DATA_PATH + SOURCE_TABLE_NAME)\n",
    "source_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4c50327e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sku_query', 'sku_pos', 'sku_hard_neg', 'sku_soft_neg']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_mapping_df = pd.read_parquet(DATA_PATH + PAIRWISE_TABLE_NAME)\n",
    "pairwise_mapping_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70179151",
   "metadata": {},
   "source": [
    "# Cluster soft negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "32453eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embeddings/OZ_geo_5500/OZ_geo_5500_name-and-description_embeddings_num-rows=2.parquet'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'embeddings/OZ_geo_5500/OZ_geo_5500_name-and-description_embeddings_num-rows=5562.parquet'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List embeddings files in repo\n",
    "FILTER_STRING = 'name-and-description_embeddings'\n",
    "\n",
    "from huggingface_hub import list_repo_files\n",
    "\n",
    "emb_files = [name for name in list_repo_files(\"INDEEPA/clip-siamese\", repo_type=\"dataset\") if FILTER_STRING in name and \"OZ_geo_5500\" in name]\n",
    "for file in emb_files:\n",
    "    display(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c67ed0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggest the correct path to the embedding file based on the context and previous file saving logic\n",
    "CHOSEN_EMBEDDING_FILE = 'OZ_geo_5500_name-and-description_embeddings_num-rows=5562.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3586cf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded embedding file to:\n",
      "data/embeddings/OZ_geo_5500/OZ_geo_5500_name-and-description_embeddings_num-rows=5562.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>name_desc_emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1871769771</td>\n",
       "      <td>[-0.020089346915483475, -0.05487045273184776, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1679550303</td>\n",
       "      <td>[-0.00418242160230875, -0.04088427498936653, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1200553001</td>\n",
       "      <td>[-0.023978281766176224, -0.05447990819811821, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>922231521</td>\n",
       "      <td>[-0.024106157943606377, -0.053567297756671906,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>922230517</td>\n",
       "      <td>[-0.02229023538529873, -0.05309479311108589, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sku                                      name_desc_emb\n",
       "0  1871769771  [-0.020089346915483475, -0.05487045273184776, ...\n",
       "1  1679550303  [-0.00418242160230875, -0.04088427498936653, 0...\n",
       "2  1200553001  [-0.023978281766176224, -0.05447990819811821, ...\n",
       "3   922231521  [-0.024106157943606377, -0.053567297756671906,...\n",
       "4   922230517  [-0.02229023538529873, -0.05309479311108589, -..."
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "\n",
    "# Download the chosen embedding file from HuggingFace Hub to DATA_PATH\n",
    "from pathlib import Path\n",
    "\n",
    "downloaded_emb_file = hf_hub_download(\n",
    "    repo_id=\"INDEEPA/clip-siamese\",\n",
    "    repo_type=\"dataset\",\n",
    "    filename=f'embeddings/OZ_geo_5500/{CHOSEN_EMBEDDING_FILE}',\n",
    "    local_dir=DATA_PATH,\n",
    ")\n",
    "\n",
    "print(f\"Downloaded embedding file to:\\n{downloaded_emb_file}\")\n",
    "emb_table = pd.read_parquet(downloaded_emb_file)\n",
    "emb_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2de7bc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/micromamba/envs/clip-siamese/lib/python3.13/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/anton/micromamba/envs/clip-siamese/lib/python3.13/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster label counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>cluster_id</th>\n",
       "      <th>-1</th>\n",
       "      <th>357</th>\n",
       "      <th>1</th>\n",
       "      <th>301</th>\n",
       "      <th>92</th>\n",
       "      <th>235</th>\n",
       "      <th>355</th>\n",
       "      <th>345</th>\n",
       "      <th>163</th>\n",
       "      <th>193</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>287</th>\n",
       "      <th>282</th>\n",
       "      <th>295</th>\n",
       "      <th>33</th>\n",
       "      <th>272</th>\n",
       "      <th>336</th>\n",
       "      <th>381</th>\n",
       "      <th>192</th>\n",
       "      <th>435</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1285</td>\n",
       "      <td>62</td>\n",
       "      <td>47</td>\n",
       "      <td>45</td>\n",
       "      <td>42</td>\n",
       "      <td>37</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 448 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "cluster_id  -1     357   1     301   92    235   355   345   163   193  ...  \\\n",
       "count       1285    62    47    45    42    37    31    30    30    30  ...   \n",
       "\n",
       "cluster_id   35    287   282   295   33    272   336   381   192   435  \n",
       "count          5     5     5     5     5     5     5     5     5     5  \n",
       "\n",
       "[1 rows x 448 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Prepare the embeddings as a numpy array\n",
    "embeddings = np.stack(emb_table['name_desc_emb'].values)\n",
    "\n",
    "# Run HDBSCAN clustering using sklearn's implementation\n",
    "clusterer = HDBSCAN(\n",
    "    min_samples=2,\n",
    "    # metric='cosine',\n",
    ")\n",
    "\n",
    "cluster_labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "# Add cluster labels to the emb_table and assign to cluster_emb_table\n",
    "cluster_emb_table = emb_table.copy()\n",
    "cluster_emb_table['cluster_id'] = cluster_labels\n",
    "\n",
    "# Print cluster label counts\n",
    "print(\"Cluster label counts:\")\n",
    "display(cluster_emb_table['cluster_id'].value_counts().to_frame().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9a02aeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster IDs with size > 4:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>cluster_id</th>\n",
       "      <th>-1</th>\n",
       "      <th>357</th>\n",
       "      <th>1</th>\n",
       "      <th>301</th>\n",
       "      <th>92</th>\n",
       "      <th>235</th>\n",
       "      <th>355</th>\n",
       "      <th>345</th>\n",
       "      <th>163</th>\n",
       "      <th>193</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>287</th>\n",
       "      <th>282</th>\n",
       "      <th>295</th>\n",
       "      <th>33</th>\n",
       "      <th>272</th>\n",
       "      <th>336</th>\n",
       "      <th>381</th>\n",
       "      <th>192</th>\n",
       "      <th>435</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1285</td>\n",
       "      <td>62</td>\n",
       "      <td>47</td>\n",
       "      <td>45</td>\n",
       "      <td>42</td>\n",
       "      <td>37</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 448 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "cluster_id  -1     357   1     301   92    235   355   345   163   193  ...  \\\n",
       "count       1285    62    47    45    42    37    31    30    30    30  ...   \n",
       "\n",
       "cluster_id   35    287   282   295   33    272   336   381   192   435  \n",
       "count          5     5     5     5     5     5     5     5     5     5  \n",
       "\n",
       "[1 rows x 448 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print cluster ids with size > N\n",
    "N = 4  # You can change N to any desired threshold\n",
    "cluster_counts = cluster_emb_table['cluster_id'].value_counts()\n",
    "large_clusters = cluster_counts[cluster_counts > N].to_frame()\n",
    "print(f\"Cluster IDs with size > {N}:\")\n",
    "display(large_clusters.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "74754845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKUs in cluster 272:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1913636945, 1893883403, 1857098848, 1853440559, 1783442338]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print SKUs for a given CLUSTER_ID\n",
    "CLUSTER_ID = 272  # Change this to the desired cluster id\n",
    "\n",
    "skus_in_cluster = cluster_emb_table.loc[cluster_emb_table['cluster_id'] == CLUSTER_ID, 'sku']\n",
    "print(f\"SKUs in cluster {CLUSTER_ID}:\")\n",
    "display(skus_in_cluster.tolist()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5dda22",
   "metadata": {},
   "source": [
    "# Make train/val/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "51377883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_query_groups(\n",
    "    mapping_df: pd.DataFrame,\n",
    "    test_size: float = 0.2,\n",
    "    val_size: float = 0.05,\n",
    "    random_state: int = 42,\n",
    "    min_positives_for_3way: int = 6  # minimum positives (including query) for 3-way split\n",
    "):\n",
    "    \"\"\"\n",
    "    Adaptive splitting based on number of positives (not including query):\n",
    "    - 6+ positives: normal train/val/test split with query in test\n",
    "    - 5 positives: pos1,pos2 to train; pos3,pos4 to val; q,pos5 to test\n",
    "    - 4 positives: pos1,pos2 to train; pos3,pos4 to val; q,pos3,pos4 to test\n",
    "    - 3 positives: pos1,pos2 to train; pos2,pos3 to val; q,pos3 to test\n",
    "    - 2 positives: pos1,pos2 to train; q,pos1 to val; q,pos2 to test\n",
    "    - 1 positive: copy q,pos to all splits\n",
    "    - 0 positives: skip\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    split_rows = []\n",
    "\n",
    "    for _, row in mapping_df.iterrows():\n",
    "        q = row['sku_query']\n",
    "        pos_without_query = list(set(row['sku_pos']) - {q})  # Convert to list for indexing\n",
    "        hard_neg = set(row['sku_hard_neg']) - {q}\n",
    "        soft_neg = set(row['sku_soft_neg']) - {q}\n",
    "        \n",
    "        total_positives = len(pos_without_query)  # Not including query\n",
    "\n",
    "        def split_list(lst, test_frac, val_frac=None):\n",
    "            lst = np.array(list(lst))\n",
    "            n = len(lst)\n",
    "            if val_frac is None:  # 2-way split\n",
    "                n_test = int(np.ceil(test_frac * n))\n",
    "                idx = rng.permutation(n)\n",
    "                test_idx = idx[:n_test]\n",
    "                train_idx = idx[n_test:]\n",
    "                return lst[train_idx].tolist(), [], lst[test_idx].tolist()\n",
    "            else:  # 3-way split\n",
    "                n_test = int(np.ceil(test_frac * n))\n",
    "                n_val = int(np.ceil(val_frac * n))\n",
    "                idx = rng.permutation(n)\n",
    "                test_idx = idx[:n_test]\n",
    "                val_idx = idx[n_test:n_test+n_val]\n",
    "                train_idx = idx[n_test+n_val:]\n",
    "                return lst[train_idx].tolist(), lst[val_idx].tolist(), lst[test_idx].tolist()\n",
    "\n",
    "        if total_positives >= 6:\n",
    "            # 6+ positives: normal train/val/test split\n",
    "            pos_train, pos_val, pos_test = split_list(pos_without_query, test_size, val_size)\n",
    "            hard_train, hard_val, hard_test = split_list(hard_neg, test_size, val_size)\n",
    "            soft_train, soft_val, soft_test = split_list(soft_neg, test_size, val_size)\n",
    "            pos_test.append(q)  # query goes to test\n",
    "            \n",
    "            splits_to_create = ['train', 'val', 'test']\n",
    "            pos_lists = [pos_train, pos_val, pos_test]\n",
    "            hard_lists = [hard_train, hard_val, hard_test]\n",
    "            soft_lists = [soft_train, soft_val, soft_test]\n",
    "            \n",
    "        elif total_positives == 5:\n",
    "            # 5 pos: pos1,pos2 to train; pos3,pos4 to val; q,pos5 to test\n",
    "            rng.shuffle(pos_without_query)  # randomize order\n",
    "            pos1, pos2, pos3, pos4, pos5 = pos_without_query[:5]\n",
    "            \n",
    "            # Split negatives proportionally\n",
    "            hard_train, hard_val, hard_test = split_list(hard_neg, test_size, val_size)\n",
    "            soft_train, soft_val, soft_test = split_list(soft_neg, test_size, val_size)\n",
    "            \n",
    "            splits_to_create = ['train', 'val', 'test']\n",
    "            pos_lists = [\n",
    "                [pos1, pos2],      # train\n",
    "                [pos3, pos4],      # val\n",
    "                [q, pos5]          # test\n",
    "            ]\n",
    "            hard_lists = [hard_train, hard_val, hard_test]\n",
    "            soft_lists = [soft_train, soft_val, soft_test]\n",
    "            \n",
    "        elif total_positives == 4:\n",
    "            # 4 pos: pos1,pos2 to train; pos3,pos4 to val; q,pos3,pos4 to test\n",
    "            rng.shuffle(pos_without_query)  # randomize order\n",
    "            pos1, pos2, pos3, pos4 = pos_without_query[:4]\n",
    "            \n",
    "            # Split negatives proportionally\n",
    "            hard_train, hard_val, hard_test = split_list(hard_neg, test_size, val_size)\n",
    "            soft_train, soft_val, soft_test = split_list(soft_neg, test_size, val_size)\n",
    "            \n",
    "            splits_to_create = ['train', 'val', 'test']\n",
    "            pos_lists = [\n",
    "                [pos1, pos2],          # train\n",
    "                [pos3, pos4],          # val\n",
    "                [q, pos3, pos4]        # test (overlaps with val)\n",
    "            ]\n",
    "            hard_lists = [hard_train, hard_val, hard_test]\n",
    "            soft_lists = [soft_train, soft_val, soft_test]\n",
    "            \n",
    "        elif total_positives == 3:\n",
    "            # 3 pos: pos1,pos2 to train; pos2,pos3 to val; q,pos3 to test\n",
    "            rng.shuffle(pos_without_query)  # randomize order\n",
    "            pos1, pos2, pos3 = pos_without_query[:3]\n",
    "            \n",
    "            # Split negatives proportionally\n",
    "            hard_train, hard_val, hard_test = split_list(hard_neg, test_size, val_size)\n",
    "            soft_train, soft_val, soft_test = split_list(soft_neg, test_size, val_size)\n",
    "            \n",
    "            splits_to_create = ['train', 'val', 'test']\n",
    "            pos_lists = [\n",
    "                [pos1, pos2],      # train\n",
    "                [pos2, pos3],      # val (overlaps with train)\n",
    "                [q, pos3]          # test (overlaps with val)\n",
    "            ]\n",
    "            hard_lists = [hard_train, hard_val, hard_test]\n",
    "            soft_lists = [soft_train, soft_val, soft_test]\n",
    "            \n",
    "        elif total_positives == 2:\n",
    "            # 2 pos: pos1,pos2 to train; q,pos1 to val; q,pos2 to test\n",
    "            rng.shuffle(pos_without_query)  # randomize order\n",
    "            pos1, pos2 = pos_without_query[:2]\n",
    "            \n",
    "            # Split negatives proportionally\n",
    "            hard_train, hard_val, hard_test = split_list(hard_neg, test_size, val_size)\n",
    "            soft_train, soft_val, soft_test = split_list(soft_neg, test_size, val_size)\n",
    "            \n",
    "            splits_to_create = ['train', 'val', 'test']\n",
    "            pos_lists = [\n",
    "                [pos1, pos2],      # train: both positives\n",
    "                [q, pos1],         # val: query + first positive\n",
    "                [q, pos2]          # test: query + second positive\n",
    "            ]\n",
    "            hard_lists = [hard_train, hard_val, hard_test]\n",
    "            soft_lists = [soft_train, soft_val, soft_test]\n",
    "            \n",
    "        elif total_positives == 1:\n",
    "            # 1 pos: copy q,pos to all splits\n",
    "            pos = pos_without_query[0]\n",
    "            \n",
    "            # Split negatives proportionally\n",
    "            hard_train, hard_val, hard_test = split_list(hard_neg, test_size, val_size)\n",
    "            soft_train, soft_val, soft_test = split_list(soft_neg, test_size, val_size)\n",
    "            \n",
    "            splits_to_create = ['train', 'val', 'test']\n",
    "            pos_lists = [\n",
    "                [q, pos],  # train: query + positive\n",
    "                [q, pos],  # val: same pair\n",
    "                [q, pos]   # test: same pair\n",
    "            ]\n",
    "            hard_lists = [hard_train, hard_val, hard_test]\n",
    "            soft_lists = [soft_train, soft_val, soft_test]\n",
    "            \n",
    "        else:\n",
    "            # Skip queries with no positives\n",
    "            continue\n",
    "\n",
    "        # Create the split rows\n",
    "        for split_name, pos_list, hard_list, soft_list in zip(\n",
    "            splits_to_create, pos_lists, hard_lists, soft_lists\n",
    "        ):\n",
    "            split_rows.append({\n",
    "                'sku_query': q,\n",
    "                'split': split_name,\n",
    "                'sku_pos': pos_list,\n",
    "                'sku_hard_neg': hard_list,\n",
    "                'sku_soft_neg': soft_list\n",
    "            })\n",
    "\n",
    "    split_df = pd.DataFrame(split_rows)\n",
    "    split_dict = {\n",
    "        split: split_df[split_df['split'] == split].reset_index(drop=True)\n",
    "        for split in ['train', 'val', 'test'] if split in split_df['split'].values\n",
    "    }\n",
    "    return split_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "763373ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_query</th>\n",
       "      <th>split</th>\n",
       "      <th>sku_pos</th>\n",
       "      <th>sku_hard_neg</th>\n",
       "      <th>sku_soft_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1871769771</td>\n",
       "      <td>test</td>\n",
       "      <td>[467396304, 1871769771]</td>\n",
       "      <td>[1873027006, 1166886051, 601557370]</td>\n",
       "      <td>[1899881468, 1290396077, 1597431764, 165269677...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1200553001</td>\n",
       "      <td>test</td>\n",
       "      <td>[945075396, 1436509994, 1436449707, 1438364324...</td>\n",
       "      <td>[1499532091, 963112482, 1422204647, 1122827873...</td>\n",
       "      <td>[1878150702, 1901123430, 1595672507, 679265327...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>922231521</td>\n",
       "      <td>test</td>\n",
       "      <td>[1436509994, 1158222448, 1081199697, 490461399...</td>\n",
       "      <td>[1001260979, 1802254834, 1252814277, 805782980...</td>\n",
       "      <td>[1032263980, 879403681, 1816716304, 1630407222...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>922230517</td>\n",
       "      <td>test</td>\n",
       "      <td>[600803111, 1125093440, 1726148392, 974286048,...</td>\n",
       "      <td>[564434635, 1449544071, 1333611366, 1294181877...</td>\n",
       "      <td>[1807617650, 1113350792, 1245721824, 620961901...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>922230183</td>\n",
       "      <td>test</td>\n",
       "      <td>[1819952117, 1679157969, 914654189, 922230183]</td>\n",
       "      <td>[959054273, 601557360, 1705669581, 950215375, ...</td>\n",
       "      <td>[1634447035, 1706808534, 1438798026, 181995203...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sku_query split                                            sku_pos  \\\n",
       "0  1871769771  test                            [467396304, 1871769771]   \n",
       "1  1200553001  test  [945075396, 1436509994, 1436449707, 1438364324...   \n",
       "2   922231521  test  [1436509994, 1158222448, 1081199697, 490461399...   \n",
       "3   922230517  test  [600803111, 1125093440, 1726148392, 974286048,...   \n",
       "4   922230183  test     [1819952117, 1679157969, 914654189, 922230183]   \n",
       "\n",
       "                                        sku_hard_neg  \\\n",
       "0                [1873027006, 1166886051, 601557370]   \n",
       "1  [1499532091, 963112482, 1422204647, 1122827873...   \n",
       "2  [1001260979, 1802254834, 1252814277, 805782980...   \n",
       "3  [564434635, 1449544071, 1333611366, 1294181877...   \n",
       "4  [959054273, 601557360, 1705669581, 950215375, ...   \n",
       "\n",
       "                                        sku_soft_neg  \n",
       "0  [1899881468, 1290396077, 1597431764, 165269677...  \n",
       "1  [1878150702, 1901123430, 1595672507, 679265327...  \n",
       "2  [1032263980, 879403681, 1816716304, 1630407222...  \n",
       "3  [1807617650, 1113350792, 1245721824, 620961901...  \n",
       "4  [1634447035, 1706808534, 1438798026, 181995203...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_dataset = split_query_groups(\n",
    "    pairwise_mapping_df,\n",
    "    test_size=0.1,\n",
    "    val_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pd.reset_option('display.max_colwidth')\n",
    "splits_dataset['test'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ac3ab06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   split  avg_soft_neg  min_soft_neg  max_soft_neg\n",
      "0  train       4198.80          3583          4446\n",
      "1    val        525.45           449           556\n",
      "2   test        525.45           449           556\n"
     ]
    }
   ],
   "source": [
    "# Compute avg, min, max count of soft negatives per query (per split) and output as table\n",
    "import pandas as pd\n",
    "\n",
    "soft_neg_stats = []\n",
    "for split_name, df in splits_dataset.items():\n",
    "    soft_neg_counts = df['sku_soft_neg'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    avg_soft_neg = soft_neg_counts.mean() if not soft_neg_counts.empty else 0\n",
    "    min_soft_neg = soft_neg_counts.min() if not soft_neg_counts.empty else 0\n",
    "    max_soft_neg = soft_neg_counts.max() if not soft_neg_counts.empty else 0\n",
    "    soft_neg_stats.append({\n",
    "        'split': split_name,\n",
    "        'avg_soft_neg': avg_soft_neg,\n",
    "        'min_soft_neg': min_soft_neg,\n",
    "        'max_soft_neg': max_soft_neg\n",
    "    })\n",
    "\n",
    "soft_neg_stats_df = pd.DataFrame(soft_neg_stats)\n",
    "print(soft_neg_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a90de9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>#queries</th>\n",
       "      <th>#pos</th>\n",
       "      <th>#hard_neg</th>\n",
       "      <th>#soft_neg</th>\n",
       "      <th>#total_sku</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>20</td>\n",
       "      <td>939</td>\n",
       "      <td>4004</td>\n",
       "      <td>83976</td>\n",
       "      <td>5562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val</td>\n",
       "      <td>20</td>\n",
       "      <td>134</td>\n",
       "      <td>513</td>\n",
       "      <td>10509</td>\n",
       "      <td>4899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>20</td>\n",
       "      <td>150</td>\n",
       "      <td>514</td>\n",
       "      <td>10509</td>\n",
       "      <td>4918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split  #queries  #pos  #hard_neg  #soft_neg  #total_sku\n",
       "0  train        20   939       4004      83976        5562\n",
       "1    val        20   134        513      10509        4899\n",
       "2   test        20   150        514      10509        4918"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare a summary table for each split\n",
    "import pandas as pd\n",
    "\n",
    "summary_rows = []\n",
    "per_query_stats = {}\n",
    "\n",
    "for split_name, df in splits_dataset.items():\n",
    "    num_rows = len(df)\n",
    "    num_pos = df['sku_pos'].apply(lambda x: len(x) if isinstance(x, list) else 0).sum()\n",
    "    num_hard = df['sku_hard_neg'].apply(lambda x: len(x) if isinstance(x, list) else 0).sum()\n",
    "    num_soft = df['sku_soft_neg'].apply(lambda x: len(x) if isinstance(x, list) else 0).sum()\n",
    "    unique_skus = set(df['sku_query'])\n",
    "    for col in ['sku_pos', 'sku_hard_neg', 'sku_soft_neg']:\n",
    "        unique_skus.update([sku for sublist in df[col] for sku in (sublist if isinstance(sublist, list) else [])])\n",
    "    summary_rows.append({\n",
    "        'split': split_name,\n",
    "        '#queries': num_rows,\n",
    "        '#pos': num_pos,\n",
    "        '#hard_neg': num_hard,\n",
    "        '#soft_neg': num_soft,\n",
    "        '#total_sku': len(unique_skus)\n",
    "    })\n",
    "\n",
    "    # Per-query stats for each type\n",
    "    per_query = pd.DataFrame({\n",
    "        'pos': df['sku_pos'].apply(lambda x: len(x) if isinstance(x, list) else 0),\n",
    "        'hard_neg': df['sku_hard_neg'].apply(lambda x: len(x) if isinstance(x, list) else 0),\n",
    "        'soft_neg': df['sku_soft_neg'].apply(lambda x: len(x) if isinstance(x, list) else 0),\n",
    "    })\n",
    "    agg = per_query.agg(['mean', 'std', 'min', 'max']).T\n",
    "    agg.index.name = 'type'\n",
    "    agg.columns.name = 'agg'\n",
    "    per_query_stats[split_name] = agg\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "89bf141d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th colspan=\"4\" halign=\"left\">pos</th>\n",
       "      <th colspan=\"4\" halign=\"left\">hard_neg</th>\n",
       "      <th colspan=\"4\" halign=\"left\">soft_neg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agg</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>46</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>216</td>\n",
       "      <td>200</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>755</td>\n",
       "      <td>4198</td>\n",
       "      <td>306</td>\n",
       "      <td>3583</td>\n",
       "      <td>4446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>525</td>\n",
       "      <td>38</td>\n",
       "      <td>449</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>525</td>\n",
       "      <td>38</td>\n",
       "      <td>449</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "type   pos              hard_neg               soft_neg                 \n",
       "agg   mean std min  max     mean  std min  max     mean  std   min   max\n",
       "split                                                                   \n",
       "train   46  64   2  216      200  249   0  755     4198  306  3583  4446\n",
       "val      6   8   1   28       25   31   0   95      525   38   449   556\n",
       "test     7   8   2   29       25   31   1   95      525   38   449   556"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display per-query stats for each split\n",
    "multiindex_tuples = []\n",
    "values = []\n",
    "for split_name, agg in per_query_stats.items():\n",
    "    for t in agg.index:\n",
    "        for a in agg.columns:\n",
    "            multiindex_tuples.append((split_name, t, a))\n",
    "            values.append(agg.loc[t, a])\n",
    "multiindex = pd.MultiIndex.from_tuples(multiindex_tuples, names=['split', 'type', 'agg'])\n",
    "per_query_multi_df = pd.Series(values, index=multiindex).unstack(['type', 'agg']).swaplevel(axis=1)\n",
    "# The above gives columns as (agg, type), swap to (type, agg)\n",
    "per_query_multi_df.columns = per_query_multi_df.columns.swaplevel(0,1)\n",
    "per_query_multi_df = per_query_multi_df.sort_index(axis=1, level=0)\n",
    "\n",
    "# Cast all columns which are numerical to int (if possible)\n",
    "for col in per_query_multi_df.columns:\n",
    "    # Only cast if dtype is numeric and all values are close to integer (to avoid ValueError)\n",
    "    if pd.api.types.is_numeric_dtype(per_query_multi_df[col]):\n",
    "        if np.allclose(per_query_multi_df[col].dropna() % 1, 0):\n",
    "            per_query_multi_df[col] = per_query_multi_df[col].astype(int)\n",
    "\n",
    "# Reorder columns so that for each type (pos, hard_neg, soft_neg), columns are in order: mean, std, min, max\n",
    "ordered_types = ['pos', 'hard_neg', 'soft_neg']\n",
    "ordered_aggs = ['mean', 'std', 'min', 'max']\n",
    "per_query_multi_df = per_query_multi_df.loc[:, [(t, a) for t in ordered_types for a in ordered_aggs]]\n",
    "# Sort per_query_multi_df by split: train, val, test\n",
    "split_order = ['train', 'val', 'test']\n",
    "per_query_multi_df = per_query_multi_df.reindex(split_order)\n",
    "\n",
    "display(per_query_multi_df.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e60491be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train split to\n",
      "data/tables_OZ_geo_5500/processed/pairwise-mapping-splits/test=0.1_val=0.1/train.parquet\n",
      "\n",
      "Saved val split to\n",
      "data/tables_OZ_geo_5500/processed/pairwise-mapping-splits/test=0.1_val=0.1/val.parquet\n",
      "\n",
      "Saved test split to\n",
      "data/tables_OZ_geo_5500/processed/pairwise-mapping-splits/test=0.1_val=0.1/test.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save splits\n",
    "\n",
    "import os\n",
    "\n",
    "split_info = f\"test={TEST_RATIO}_val={VAL_RATIO}\"\n",
    "save_dir = os.path.join(\n",
    "    \"data/tables_OZ_geo_5500/processed/pairwise-mapping-splits\", split_info\n",
    ")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save each split as a parquet file\n",
    "for split_name, df in splits_dataset.items():\n",
    "    out_path = os.path.join(save_dir, f\"{split_name}.parquet\")\n",
    "    df.to_parquet(out_path, index=False)\n",
    "    print(f\"Saved {split_name} split to\\n{out_path}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f252c53",
   "metadata": {},
   "source": [
    "### Data leak sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9198d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DATA LEAK #1 (major). Common positives\n",
    "# # Some queries have common positives (see positive intersection matrix below)\n",
    "# # Potential solution (2 steps): \n",
    "# #   1. merge sku w/ many intersections, \n",
    "# #   2. disentangle common positives w/ few intersections\n",
    "# # NOTE: probably common hard negatives as well; certainly a lot of common soft negatives\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # Build a list of queries and a mapping from query to set of positives\n",
    "# queries = pairwise_mapping_df['sku_query'].tolist()\n",
    "# sku_to_pos = {\n",
    "#     row['sku_query']: set(row['sku_pos'])\n",
    "#     for _, row in pairwise_mapping_df.iterrows()\n",
    "# }\n",
    "\n",
    "# num_queries = len(queries)\n",
    "# intersection_matrix = np.zeros((num_queries, num_queries), dtype=int)\n",
    "\n",
    "# for i, q1 in enumerate(queries):\n",
    "#     pos1 = sku_to_pos[q1]\n",
    "#     for j, q2 in enumerate(queries):\n",
    "#         pos2 = sku_to_pos[q2]\n",
    "#         intersection_matrix[i, j] = len(pos1 & pos2)\n",
    "\n",
    "# # Mask diagonal and above with -1\n",
    "# mask = np.triu(np.ones_like(intersection_matrix, dtype=bool))\n",
    "# intersection_matrix[mask] = -1\n",
    "\n",
    "# # Optionally, wrap as DataFrame for readability\n",
    "# intersection_df = pd.DataFrame(\n",
    "#     intersection_matrix, \n",
    "#     index=queries, \n",
    "#     columns=queries\n",
    "# )\n",
    "# intersection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dc10fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DATA LEAK #2 (major). Lack of global disjointness w.r.t. sku over splits\n",
    "# # Many skus are present in one split as positives and in another split as negatives\n",
    "\n",
    "# def analyze_global_disjointness(splits_dataset):\n",
    "#     \"\"\"\n",
    "#     Analyze global SKU disjointness across train/val/test splits.\n",
    "#     Count positives in train that appear as negatives in val/test.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Collect all SKUs by split and role\n",
    "#     train_positives = set()\n",
    "#     val_negatives = set()\n",
    "#     test_negatives = set()\n",
    "    \n",
    "#     # Extract all positive SKUs from train split\n",
    "#     for _, row in splits_dataset['train'].iterrows():\n",
    "#         train_positives.update(row['sku_pos'])\n",
    "    \n",
    "#     # Extract all negative SKUs from val split\n",
    "#     for _, row in splits_dataset['val'].iterrows():\n",
    "#         val_negatives.update(row['sku_hard_neg'])\n",
    "#         val_negatives.update(row['sku_soft_neg'])\n",
    "    \n",
    "#     # Extract all negative SKUs from test split  \n",
    "#     for _, row in splits_dataset['test'].iterrows():\n",
    "#         test_negatives.update(row['sku_hard_neg'])\n",
    "#         test_negatives.update(row['sku_soft_neg'])\n",
    "    \n",
    "#     # Find overlaps (data leaks)\n",
    "#     train_pos_in_val_neg = train_positives & val_negatives\n",
    "#     train_pos_in_test_neg = train_positives & test_negatives\n",
    "    \n",
    "#     # Summary statistics\n",
    "#     print(\"=== GLOBAL DISJOINTNESS ANALYSIS ===\")\n",
    "#     print(f\"Train positives: {len(train_positives):,}\")\n",
    "#     print(f\"Val negatives: {len(val_negatives):,}\")\n",
    "#     print(f\"Test negatives: {len(test_negatives):,}\")\n",
    "#     print()\n",
    "#     print(\"=== DATA LEAKS DETECTED ===\")\n",
    "#     print(f\"Train positives appearing as Val negatives: {len(train_pos_in_val_neg):,}\")\n",
    "#     print(f\"Train positives appearing as Test negatives: {len(train_pos_in_test_neg):,}\")\n",
    "#     print()\n",
    "#     print(\"=== LEAK PERCENTAGES ===\")\n",
    "#     print(f\"% of train positives leaked to val: {len(train_pos_in_val_neg)/len(train_positives)*100:.1f}%\")\n",
    "#     print(f\"% of train positives leaked to test: {len(train_pos_in_test_neg)/len(train_positives)*100:.1f}%\")\n",
    "    \n",
    "#     return {\n",
    "#         'train_positives': train_positives,\n",
    "#         'val_negatives': val_negatives, \n",
    "#         'test_negatives': test_negatives,\n",
    "#         'train_pos_in_val_neg': train_pos_in_val_neg,\n",
    "#         'train_pos_in_test_neg': train_pos_in_test_neg\n",
    "#     }\n",
    "\n",
    "# # Run the analysis\n",
    "# leak_analysis = analyze_global_disjointness(splits_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e6186520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DATA LEAK #3 (minor). Reused train sku in test for low-resource query sku\n",
    "# # Some queries do not have enough positives for train/test or train/val/test splits\n",
    "# # train/test split requires at least 3 positives per query (4 including query) => 2 train / 2 test\n",
    "# # train/val/test split requires at least 5 positives per query (6 including query) => 2 train / 2 val / 2 test\n",
    "\n",
    "# pairwise_mapping_df.set_index('sku_query')['sku_pos'].map(lambda s: len(s)).sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a36ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA ISSUE (tiny): some query sku have bad descriptions (related to SEO-optimization or crawling issues)\n",
    "# OZ_geo_5500_manual-edit.csv contains manual edits to fix the descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07621661",
   "metadata": {},
   "source": [
    "## Make pairwise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6652bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Optional, List, Dict, Any\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class PairGenerationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for constructing training pairs using a hybrid sampling strategy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 split_df: pd.DataFrame,\n",
    "                 cluster_emb_table: pd.DataFrame,\n",
    "                 source_df: pd.DataFrame = None,\n",
    "                 images_dir: str = None,\n",
    "                 max_pos_pairs_per_query: int = 3,\n",
    "                 pos_neg_ratio: float = 2.0,\n",
    "                 hard_soft_ratio: float = 0.7,\n",
    "                 random_seed: int = 42):\n",
    "        \n",
    "        self.split_df = split_df.reset_index(drop=True)\n",
    "        self.cluster_emb_table = cluster_emb_table\n",
    "        self.source_df = source_df.set_index('sku') if source_df is not None else None\n",
    "        self.images_dir = images_dir\n",
    "        self.max_pos_pairs_per_query = max_pos_pairs_per_query\n",
    "        self.pos_neg_ratio = pos_neg_ratio\n",
    "        self.hard_soft_ratio = hard_soft_ratio\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        # Set random seeds for reproducibility\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        # Build cluster mappings for soft negatives\n",
    "        self._build_cluster_mappings()\n",
    "        \n",
    "        # Generate all pairs\n",
    "        self.pairs = self._generate_pairs()\n",
    "        \n",
    "    def _build_cluster_mappings(self):\n",
    "        \"\"\"Build mappings from cluster to SKUs for efficient sampling.\"\"\"\n",
    "        if self.cluster_emb_table is not None:\n",
    "            self.sku_to_cluster = dict(zip(self.cluster_emb_table['sku'], self.cluster_emb_table['cluster_id']))\n",
    "            self.cluster_to_skus = {}\n",
    "            for _, row in self.cluster_emb_table.iterrows():\n",
    "                cluster = row['cluster_id']\n",
    "                sku = row['sku']\n",
    "                if cluster not in self.cluster_to_skus:\n",
    "                    self.cluster_to_skus[cluster] = []\n",
    "                self.cluster_to_skus[cluster].append(sku)\n",
    "        else:\n",
    "            self.sku_to_cluster = {}\n",
    "            self.cluster_to_skus = {}\n",
    "    \n",
    "    def _generate_pairs(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate all training pairs using hybrid approach.\"\"\"\n",
    "        all_pairs = []\n",
    "        \n",
    "        for _, row in self.split_df.iterrows():\n",
    "            query_sku = row['sku_query']\n",
    "            pos_skus = row['sku_pos'] if isinstance(row['sku_pos'], list) else []\n",
    "            hard_neg_skus = row['sku_hard_neg'] if isinstance(row['sku_hard_neg'], list) else []\n",
    "            soft_neg_skus = row['sku_soft_neg'] if isinstance(row['sku_soft_neg'], list) else []\n",
    "            \n",
    "            # Generate positive pairs\n",
    "            positive_pairs = self._generate_positive_pairs_fixed(query_sku, pos_skus)\n",
    "            \n",
    "            # Calculate negatives based on actual positive count and ratio\n",
    "            actual_pos_count = len(positive_pairs)\n",
    "            total_negatives_needed = int(actual_pos_count * self.pos_neg_ratio)\n",
    "            \n",
    "            # Split negatives into hard and soft based on hard_soft_ratio\n",
    "            num_hard_neg = int(total_negatives_needed * self.hard_soft_ratio)\n",
    "            num_soft_neg = total_negatives_needed - num_hard_neg\n",
    "            \n",
    "            # Generate negative pairs\n",
    "            hard_negative_pairs = self._generate_hard_negative_pairs(\n",
    "                query_sku, pos_skus, hard_neg_skus, num_hard_neg\n",
    "            )\n",
    "            soft_negative_pairs = self._generate_soft_negative_pairs(\n",
    "                query_sku, pos_skus, soft_neg_skus, num_soft_neg\n",
    "            )\n",
    "            \n",
    "            # Combine all pairs for this query\n",
    "            all_pairs.extend(positive_pairs)\n",
    "            all_pairs.extend(hard_negative_pairs)\n",
    "            all_pairs.extend(soft_negative_pairs)\n",
    "            \n",
    "        return all_pairs\n",
    "    \n",
    "    def _generate_positive_pairs_fixed(self, query_sku: int, pos_skus: List[int]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate exactly max_pos_pairs_per_query positive pairs.\"\"\"\n",
    "        # Fix: Handle None case\n",
    "        if not pos_skus or (self.max_pos_pairs_per_query is not None and self.max_pos_pairs_per_query <= 0):\n",
    "            return []\n",
    "        \n",
    "        # Get all possible positive pairs (Cartesian product excluding self-pairs)\n",
    "        all_possible_pairs = []\n",
    "        for pos1 in pos_skus:\n",
    "            for pos2 in pos_skus:\n",
    "                if pos1 != pos2:\n",
    "                    all_possible_pairs.append((pos1, pos2))\n",
    "        \n",
    "        # Handle case where no pairs can be formed\n",
    "        if not all_possible_pairs:\n",
    "            if len(set(pos_skus)) == 1:\n",
    "                single_sku = pos_skus[0]\n",
    "                all_possible_pairs = [(single_sku, single_sku)]\n",
    "            else:\n",
    "                return []\n",
    "        \n",
    "        # Handle None case for max_pos_pairs_per_query\n",
    "        if self.max_pos_pairs_per_query is None:\n",
    "            # Use all available pairs if no limit set\n",
    "            selected_pairs = all_possible_pairs\n",
    "        elif len(all_possible_pairs) >= self.max_pos_pairs_per_query:\n",
    "            selected_pairs = random.sample(all_possible_pairs, self.max_pos_pairs_per_query)\n",
    "        else:\n",
    "            # If not enough unique pairs, sample with replacement\n",
    "            selected_pairs = random.choices(all_possible_pairs, k=self.max_pos_pairs_per_query)\n",
    "        \n",
    "        pairs = []\n",
    "        for pos1, pos2 in selected_pairs:\n",
    "            pairs.append({\n",
    "                'sku_first': pos1,\n",
    "                'sku_second': pos2,\n",
    "                'label': 0,  # 0 = positive (similar)\n",
    "                'query_sku': query_sku,\n",
    "                'pair_type': 'positive'\n",
    "            })\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def _generate_hard_negative_pairs(self, query_sku: int, pos_skus: List[int], \n",
    "                                    hard_neg_skus: List[int], num_hard_neg: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate hard negative pairs by pairing positives with hard negatives.\"\"\"\n",
    "        if num_hard_neg <= 0 or not pos_skus or not hard_neg_skus:\n",
    "            return []\n",
    "        \n",
    "        pairs = []\n",
    "        for _ in range(num_hard_neg):\n",
    "            # Randomly select a positive and a hard negative\n",
    "            pos_sku = random.choice(pos_skus)\n",
    "            hard_sku = random.choice(hard_neg_skus)\n",
    "            \n",
    "            pairs.append({\n",
    "                'sku_first': pos_sku,\n",
    "                'sku_second': hard_sku,\n",
    "                'label': 1,  # 1 = negative (different)\n",
    "                'query_sku': query_sku,\n",
    "                'pair_type': 'hard_negative'\n",
    "            })\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def _generate_soft_negative_pairs(self, query_sku: int, pos_skus: List[int], \n",
    "                                    soft_neg_skus: List[int], num_soft_neg: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate soft negative pairs by pairing positives with soft negatives.\"\"\"\n",
    "        if num_soft_neg <= 0 or not pos_skus or not soft_neg_skus:\n",
    "            return []\n",
    "        \n",
    "        pairs = []\n",
    "        for _ in range(num_soft_neg):\n",
    "            # Randomly select a positive and a soft negative\n",
    "            pos_sku = random.choice(pos_skus)\n",
    "            soft_sku = random.choice(soft_neg_skus)\n",
    "            \n",
    "            pairs.append({\n",
    "                'sku_first': pos_sku,\n",
    "                'sku_second': soft_sku,\n",
    "                'label': 1,  # 1 = negative (different)\n",
    "                'query_sku': query_sku,\n",
    "                'pair_type': 'soft_negative'\n",
    "            })\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def get_batch_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get detailed statistics about the generated pairs.\"\"\"\n",
    "        stats = {\n",
    "            'total_pairs': len(self.pairs),\n",
    "            'positives': sum(1 for p in self.pairs if p['pair_type'] == 'positive'),\n",
    "            'hard_negatives': sum(1 for p in self.pairs if p['pair_type'] == 'hard_negative'),\n",
    "            'soft_negatives': sum(1 for p in self.pairs if p['pair_type'] == 'soft_negative'),\n",
    "            'queries': len(self.split_df),\n",
    "        }\n",
    "        \n",
    "        stats['total_negatives'] = stats['hard_negatives'] + stats['soft_negatives']\n",
    "        \n",
    "        # Calculate actual ratios achieved\n",
    "        if stats['positives'] > 0:\n",
    "            stats['actual_neg_pos_ratio'] = stats['total_negatives'] / stats['positives']\n",
    "        else:\n",
    "            stats['actual_neg_pos_ratio'] = 0.0\n",
    "            \n",
    "        if stats['total_negatives'] > 0:\n",
    "            stats['actual_hard_soft_ratio'] = stats['hard_negatives'] / stats['total_negatives']\n",
    "        else:\n",
    "            stats['actual_hard_soft_ratio'] = 0.0\n",
    "        \n",
    "        # Calculate per-query averages\n",
    "        stats['avg_pos_per_query'] = stats['positives'] / max(stats['queries'], 1)\n",
    "        stats['avg_neg_per_query'] = stats['total_negatives'] / max(stats['queries'], 1)\n",
    "        stats['avg_pairs_per_query'] = stats['total_pairs'] / max(stats['queries'], 1)\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        return {\n",
    "            'sku_first': torch.tensor(pair['sku_first']),\n",
    "            'sku_second': torch.tensor(pair['sku_second']),\n",
    "            'label': torch.tensor(pair['label'], dtype=torch.float32),\n",
    "            'query_sku': pair['query_sku'],\n",
    "            'pair_type': pair['pair_type']\n",
    "        }\n",
    "    \n",
    "    def get_batch_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get detailed statistics about the generated pairs.\"\"\"\n",
    "        stats = {\n",
    "            'total_pairs': len(self.pairs),\n",
    "            'positives': sum(1 for p in self.pairs if p['pair_type'] == 'positive'),\n",
    "            'hard_negatives': sum(1 for p in self.pairs if p['pair_type'] == 'hard_negative'),\n",
    "            'soft_negatives': sum(1 for p in self.pairs if p['pair_type'] == 'soft_negative'),\n",
    "            'queries': len(self.split_df),\n",
    "        }\n",
    "        \n",
    "        stats['total_negatives'] = stats['hard_negatives'] + stats['soft_negatives']\n",
    "        \n",
    "        # Calculate actual ratios achieved\n",
    "        if stats['positives'] > 0:\n",
    "            stats['actual_neg_pos_ratio'] = stats['total_negatives'] / stats['positives']\n",
    "        else:\n",
    "            stats['actual_neg_pos_ratio'] = 0.0\n",
    "            \n",
    "        if stats['total_negatives'] > 0:\n",
    "            stats['actual_hard_soft_ratio'] = stats['hard_negatives'] / stats['total_negatives']\n",
    "        else:\n",
    "            stats['actual_hard_soft_ratio'] = 0.0\n",
    "        \n",
    "        # Calculate per-query averages\n",
    "        stats['avg_pos_per_query'] = stats['positives'] / max(stats['queries'], 1)\n",
    "        stats['avg_neg_per_query'] = stats['total_negatives'] / max(stats['queries'], 1)\n",
    "        stats['avg_pairs_per_query'] = stats['total_pairs'] / max(stats['queries'], 1)\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def print_detailed_stats(self):\n",
    "        \"\"\"Print comprehensive dataset statistics.\"\"\"\n",
    "        stats = self.get_batch_stats()\n",
    "        \n",
    "        print(\"=== Hybrid PairGenerationDataset Statistics ===\")\n",
    "        print(f\"📊 Dataset Overview:\")\n",
    "        print(f\"  Queries: {stats['queries']}\")\n",
    "        print(f\"  Total pairs: {stats['total_pairs']}\")\n",
    "        print(f\"  Avg pairs per query: {stats['avg_pairs_per_query']:.1f}\")\n",
    "        \n",
    "        print(f\"\\n🎯 Pair Breakdown:\")\n",
    "        print(f\"  Positives: {stats['positives']} ({stats['positives']/max(stats['total_pairs'],1)*100:.1f}%)\")\n",
    "        print(f\"  Hard negatives: {stats['hard_negatives']} ({stats['hard_negatives']/max(stats['total_pairs'],1)*100:.1f}%)\")\n",
    "        print(f\"  Soft negatives: {stats['soft_negatives']} ({stats['soft_negatives']/max(stats['total_pairs'],1)*100:.1f}%)\")\n",
    "        print(f\"  Total negatives: {stats['total_negatives']}\")\n",
    "        \n",
    "        print(f\"\\n⚖️ Ratios Achieved:\")\n",
    "        print(f\"  Target neg:pos ratio: {self.pos_neg_ratio:.2f}\")\n",
    "        print(f\"  Actual neg:pos ratio: {stats['actual_neg_pos_ratio']:.2f}\")\n",
    "        print(f\"  Target hard:soft ratio: {self.hard_soft_ratio:.2f}\")\n",
    "        print(f\"  Actual hard:soft ratio: {stats['actual_hard_soft_ratio']:.2f}\")\n",
    "        \n",
    "        print(f\"\\n📈 Per-Query Averages:\")\n",
    "        print(f\"  Avg positives per query: {stats['avg_pos_per_query']:.1f}\")\n",
    "        print(f\"  Avg negatives per query: {stats['avg_neg_per_query']:.1f}\")\n",
    "        print(f\"  Max pos pairs setting: {self.max_pos_pairs_per_query}\")\n",
    "\n",
    "    def to_pairwise_dataframe(self):\n",
    "        \"\"\"\n",
    "        Generate a full pairwise dataframe containing all pairs from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Pairwise dataframe with columns from source_df duplicated \n",
    "                        with '_first' and '_second' suffixes, plus 'pair_type', 'sku_query',\n",
    "                        'sku_first', 'sku_second', and 'label'\n",
    "        \"\"\"\n",
    "        if self.source_df is None:\n",
    "            raise ValueError(\"source_df is required to generate pairwise dataframe\")\n",
    "\n",
    "        pairwise_rows = []\n",
    "\n",
    "        # Map pair_type to desired format and label\n",
    "        pair_type_mapping = {\n",
    "            'positive': 'pos',\n",
    "            'hard_negative': 'hard_neg', \n",
    "            'soft_negative': 'soft_neg'\n",
    "        }\n",
    "        \n",
    "        # Map pair_type to binary labels for training\n",
    "        pair_type_to_label = {\n",
    "            'positive': 0,      # Similar pairs get label 0 (for contrastive loss)\n",
    "            'hard_negative': 1, # Dissimilar pairs get label 1 (for contrastive loss)\n",
    "            'soft_negative': 1  # Dissimilar pairs get label 1 (for contrastive loss)\n",
    "        }\n",
    "\n",
    "        # Add progress bar for pair processing\n",
    "        for pair in tqdm(self.pairs, desc=\"Converting to pairwise dataframe\"):\n",
    "            sku_first = pair['sku_first']\n",
    "            sku_second = pair['sku_second']\n",
    "            query_sku = pair['query_sku']\n",
    "            pair_type = pair['pair_type']\n",
    "\n",
    "            mapped_pair_type = pair_type_mapping.get(pair_type, pair_type)\n",
    "            label = pair_type_to_label.get(pair_type, 1)  # Default to 1 (dissimilar) if unknown\n",
    "\n",
    "            # Check if both SKUs exist in source_df\n",
    "            if sku_first in self.source_df.index and sku_second in self.source_df.index:\n",
    "                row_first = self.source_df.loc[sku_first]\n",
    "                row_second = self.source_df.loc[sku_second]\n",
    "\n",
    "                # Create pairwise row\n",
    "                pairwise_row = {}\n",
    "\n",
    "                # Add columns from source_df with _first suffix\n",
    "                for col in self.source_df.columns:\n",
    "                    pairwise_row[f\"{col}_first\"] = row_first[col]\n",
    "\n",
    "                # Add columns from source_df with _second suffix  \n",
    "                for col in self.source_df.columns:\n",
    "                    pairwise_row[f\"{col}_second\"] = row_second[col]\n",
    "\n",
    "                # Add pair metadata\n",
    "                pairwise_row['pair_type'] = mapped_pair_type\n",
    "                pairwise_row['sku_query'] = query_sku\n",
    "                pairwise_row['label'] = 1 - label  # Add the inverted label column (NOT for contrastive loss)\n",
    "\n",
    "                # Add sku_first and sku_second columns explicitly\n",
    "                pairwise_row['sku_first'] = sku_first\n",
    "                pairwise_row['sku_second'] = sku_second\n",
    "\n",
    "                pairwise_rows.append(pairwise_row)\n",
    "            else:\n",
    "                # Log warning for missing SKUs but continue\n",
    "                missing_skus = []\n",
    "                if sku_first not in self.source_df.index:\n",
    "                    missing_skus.append(sku_first)\n",
    "                if sku_second not in self.source_df.index:\n",
    "                    missing_skus.append(sku_second)\n",
    "                print(f\"Warning: SKU(s) {missing_skus} not found in source_df, skipping pair\")\n",
    "\n",
    "        return pd.DataFrame(pairwise_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c6b979f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hybrid PairGenerationDataset Statistics ===\n",
      "📊 Dataset Overview:\n",
      "  Queries: 20\n",
      "  Total pairs: 3977\n",
      "  Avg pairs per query: 198.8\n",
      "\n",
      "🎯 Pair Breakdown:\n",
      "  Positives: 1989 (50.0%)\n",
      "  Hard negatives: 992 (24.9%)\n",
      "  Soft negatives: 996 (25.0%)\n",
      "  Total negatives: 1988\n",
      "\n",
      "⚖️ Ratios Achieved:\n",
      "  Target neg:pos ratio: 1.00\n",
      "  Actual neg:pos ratio: 1.00\n",
      "  Target hard:soft ratio: 0.50\n",
      "  Actual hard:soft ratio: 0.50\n",
      "\n",
      "📈 Per-Query Averages:\n",
      "  Avg positives per query: 99.5\n",
      "  Avg negatives per query: 99.4\n",
      "  Max pos pairs setting: None\n"
     ]
    }
   ],
   "source": [
    "LIMIT_VAL_POS_FOR_SAVING = None\n",
    "\n",
    "val_dataset = PairGenerationDataset(\n",
    "    split_df=splits_dataset['val'],\n",
    "    cluster_emb_table=cluster_emb_table,\n",
    "    source_df=source_df,\n",
    "    images_dir = os.path.join(DATA_PATH, IMG_DATASET_NAME),\n",
    "    max_pos_pairs_per_query=LIMIT_VAL_POS_FOR_SAVING,\n",
    "    pos_neg_ratio=POS_NEG_RATIO,\n",
    "    hard_soft_ratio=HARD_SOFT_RATIO,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "val_dataset.print_detailed_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d3f5abf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to pairwise dataframe: 100%|██████████| 3977/3977 [00:01<00:00, 2926.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3977, 95)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pairwise_df = val_dataset.to_pairwise_dataframe()\n",
    "val_pairwise_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0895366d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hybrid PairGenerationDataset Statistics ===\n",
      "📊 Dataset Overview:\n",
      "  Queries: 20\n",
      "  Total pairs: 4476\n",
      "  Avg pairs per query: 223.8\n",
      "\n",
      "🎯 Pair Breakdown:\n",
      "  Positives: 2238 (50.0%)\n",
      "  Hard negatives: 1119 (25.0%)\n",
      "  Soft negatives: 1119 (25.0%)\n",
      "  Total negatives: 2238\n",
      "\n",
      "⚖️ Ratios Achieved:\n",
      "  Target neg:pos ratio: 1.00\n",
      "  Actual neg:pos ratio: 1.00\n",
      "  Target hard:soft ratio: 0.50\n",
      "  Actual hard:soft ratio: 0.50\n",
      "\n",
      "📈 Per-Query Averages:\n",
      "  Avg positives per query: 111.9\n",
      "  Avg negatives per query: 111.9\n",
      "  Max pos pairs setting: None\n"
     ]
    }
   ],
   "source": [
    "LIMIT_TEST_POS_FOR_SAVING = None\n",
    "\n",
    "test_dataset = PairGenerationDataset(\n",
    "    split_df=splits_dataset['test'],\n",
    "    cluster_emb_table=cluster_emb_table,\n",
    "    source_df=source_df,\n",
    "    images_dir = os.path.join(DATA_PATH, IMG_DATASET_NAME),\n",
    "    max_pos_pairs_per_query=LIMIT_TEST_POS_FOR_SAVING,\n",
    "    pos_neg_ratio=POS_NEG_RATIO,\n",
    "    hard_soft_ratio=HARD_SOFT_RATIO,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "test_dataset.print_detailed_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9e831b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to pairwise dataframe: 100%|██████████| 4476/4476 [00:01<00:00, 2746.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4476, 95)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pairwise_df = test_dataset.to_pairwise_dataframe()\n",
    "test_pairwise_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "eadeca0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved val pairwise_df to\n",
      "tables_OZ_geo_5500/processed/pairwise-rendered/val/num-rows=3977_limit-pos=None_pos-neg=1.0_hard-soft=0.5_seed=42/pairs.parquet\n",
      "\n",
      "Saved test pairwise_df to\n",
      "tables_OZ_geo_5500/processed/pairwise-rendered/test/num-rows=4476_limit-pos=None_pos-neg=1.0_hard-soft=0.5_seed=42/pairs.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save pairwise_df with a name including all sampling parameters\n",
    "\n",
    "# Prepare parameter names with dashes instead of underscores\n",
    "def param_name(param):\n",
    "    return param.replace('_', '-')\n",
    "\n",
    "for split, pairwise_df, pos_limit in [\n",
    "    ('val',  val_pairwise_df,  LIMIT_VAL_POS_FOR_SAVING),\n",
    "    ('test', test_pairwise_df, LIMIT_TEST_POS_FOR_SAVING), \n",
    "]:\n",
    "    # Collect parameters and their values\n",
    "    params = [\n",
    "        ('num-rows', len(pairwise_df)),\n",
    "        ('limit-pos', pos_limit),\n",
    "        ('pos-neg', POS_NEG_RATIO),\n",
    "        ('hard-soft', HARD_SOFT_RATIO),\n",
    "        ('seed', RANDOM_SEED)\n",
    "    ]\n",
    "\n",
    "    # Build folder_name\n",
    "    param_str = '_'.join([f\"{param}={value}\" for param, value in params])\n",
    "    # Use DATA_PATH as the root directory to ensure the full path exists\n",
    "    filepath = (\n",
    "        Path(DATA_PATH) / 'tables_OZ_geo_5500' \n",
    "        / 'processed' / 'pairwise-rendered' / split / param_str / 'pairs.parquet'\n",
    "    )\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save DataFrame\n",
    "    pairwise_df.to_parquet(filepath, index=False)\n",
    "    # Print path without DATA_PATH at the start\n",
    "    rel_path = filepath.relative_to(DATA_PATH)\n",
    "    print(f\"Saved {split} pairwise_df to\\n{rel_path}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca662349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class PrecomputedPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pre-compute all pairs and cache tokenized data for faster loading.\n",
    "    \"\"\"\n",
    "    def __init__(self, split_df, source_df, images_dir, **kwargs):\n",
    "        print(\"\\n=== Initializing PrecomputedPairDataset ===\")\n",
    "        self.images_dir = images_dir\n",
    "        \n",
    "        # Store parameters for pair generation\n",
    "        self.max_pos_pairs_per_query = kwargs.get('max_pos_pairs_per_query')\n",
    "        self.pos_neg_ratio = kwargs.get('pos_neg_ratio', 2.0)\n",
    "        self.hard_soft_ratio = kwargs.get('hard_soft_ratio', 0.5)\n",
    "        \n",
    "        # Pre-compute all pairs (without actual data loading)\n",
    "        print(\"\\n1. Generating pairs metadata...\")\n",
    "        self.pairs = self._generate_pairs_metadata(split_df, **kwargs)\n",
    "        print(f\"Generated {len(self.pairs)} pairs\")\n",
    "        \n",
    "        # Pre-load and cache all unique SKUs data\n",
    "        print(\"\\n2. Starting data precomputation...\")\n",
    "        self.sku_cache = self._preload_sku_data(source_df)\n",
    "        \n",
    "        print(f\"\\n=== Dataset Ready ===\")\n",
    "        print(f\"Total pairs: {len(self.pairs)}\")\n",
    "        print(f\"Unique SKUs cached: {len(self.sku_cache)}\")\n",
    "    \n",
    "    def get_batch_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get detailed statistics about the generated pairs.\"\"\"\n",
    "        stats = {\n",
    "            'total_pairs': len(self.pairs),\n",
    "            'positives': sum(1 for p in self.pairs if p['pair_type'] == 'positive'),\n",
    "            'hard_negatives': sum(1 for p in self.pairs if p['pair_type'] == 'hard_negative'),\n",
    "            'soft_negatives': sum(1 for p in self.pairs if p['pair_type'] == 'soft_negative'),\n",
    "            'queries': len(set(p['query_sku'] for p in self.pairs)),  # Count unique queries\n",
    "        }\n",
    "        \n",
    "        stats['total_negatives'] = stats['hard_negatives'] + stats['soft_negatives']\n",
    "        \n",
    "        # Calculate actual ratios achieved\n",
    "        if stats['positives'] > 0:\n",
    "            stats['actual_neg_pos_ratio'] = stats['total_negatives'] / stats['positives']\n",
    "        else:\n",
    "            stats['actual_neg_pos_ratio'] = 0.0\n",
    "            \n",
    "        if stats['total_negatives'] > 0:\n",
    "            stats['actual_hard_soft_ratio'] = stats['hard_negatives'] / stats['total_negatives']\n",
    "        else:\n",
    "            stats['actual_hard_soft_ratio'] = 0.0\n",
    "        \n",
    "        # Calculate per-query averages\n",
    "        stats['avg_pos_per_query'] = stats['positives'] / max(stats['queries'], 1)\n",
    "        stats['avg_neg_per_query'] = stats['total_negatives'] / max(stats['queries'], 1)\n",
    "        stats['avg_pairs_per_query'] = stats['total_pairs'] / max(stats['queries'], 1)\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def _generate_positive_pairs_fixed(self, query_sku: int, pos_skus: List[int], max_pairs: int = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate positive pairs with progress tracking.\"\"\"\n",
    "        if max_pairs is None:\n",
    "            max_pairs = self.max_pos_pairs_per_query\n",
    "            \n",
    "        # Fix: Handle None case\n",
    "        if not pos_skus or (max_pairs is not None and max_pairs <= 0):\n",
    "            return []\n",
    "        \n",
    "        # Get all possible positive pairs (Cartesian product excluding self-pairs)\n",
    "        all_possible_pairs = []\n",
    "        for pos1 in pos_skus:\n",
    "            for pos2 in pos_skus:\n",
    "                if pos1 != pos2:\n",
    "                    all_possible_pairs.append((pos1, pos2))\n",
    "        \n",
    "        # Handle case where no pairs can be formed\n",
    "        if not all_possible_pairs:\n",
    "            if len(set(pos_skus)) == 1:\n",
    "                single_sku = pos_skus[0]\n",
    "                all_possible_pairs = [(single_sku, single_sku)]\n",
    "            else:\n",
    "                return []\n",
    "        \n",
    "        # Handle None case for max_pos_pairs_per_query\n",
    "        if max_pairs is None:\n",
    "            # Use all available pairs if no limit set\n",
    "            selected_pairs = all_possible_pairs\n",
    "        elif len(all_possible_pairs) >= max_pairs:\n",
    "            selected_pairs = random.sample(all_possible_pairs, max_pairs)\n",
    "        else:\n",
    "            # If not enough unique pairs, sample with replacement\n",
    "            selected_pairs = random.choices(all_possible_pairs, k=max_pairs)\n",
    "        \n",
    "        pairs = []\n",
    "        for pos1, pos2 in selected_pairs:\n",
    "            pairs.append({\n",
    "                'sku_first': pos1,\n",
    "                'sku_second': pos2,\n",
    "                'label': 0,  # 0 = positive (similar)\n",
    "                'query_sku': query_sku,\n",
    "                'pair_type': 'positive'\n",
    "            })\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def _generate_hard_negative_pairs(self, query_sku: int, pos_skus: List[int], \n",
    "                                    hard_neg_skus: List[int], num_hard_neg: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate hard negative pairs by pairing positives with hard negatives.\"\"\"\n",
    "        if num_hard_neg <= 0 or not pos_skus or not hard_neg_skus:\n",
    "            return []\n",
    "        \n",
    "        pairs = []\n",
    "        for _ in range(num_hard_neg):\n",
    "            # Randomly select a positive and a hard negative\n",
    "            pos_sku = random.choice(pos_skus)\n",
    "            hard_sku = random.choice(hard_neg_skus)\n",
    "            \n",
    "            pairs.append({\n",
    "                'sku_first': pos_sku,\n",
    "                'sku_second': hard_sku,\n",
    "                'label': 1,  # 1 = negative (different)\n",
    "                'query_sku': query_sku,\n",
    "                'pair_type': 'hard_negative'\n",
    "            })\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def _generate_soft_negative_pairs(self, query_sku: int, pos_skus: List[int], \n",
    "                                    soft_neg_skus: List[int], num_soft_neg: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate soft negative pairs by pairing positives with soft negatives.\"\"\"\n",
    "        if num_soft_neg <= 0 or not pos_skus or not soft_neg_skus:\n",
    "            return []\n",
    "        \n",
    "        pairs = []\n",
    "        for _ in range(num_soft_neg):\n",
    "            # Randomly select a positive and a soft negative\n",
    "            pos_sku = random.choice(pos_skus)\n",
    "            soft_sku = random.choice(soft_neg_skus)\n",
    "            \n",
    "            pairs.append({\n",
    "                'sku_first': pos_sku,\n",
    "                'sku_second': soft_sku,\n",
    "                'label': 1,  # 1 = negative (different)\n",
    "                'query_sku': query_sku,\n",
    "                'pair_type': 'soft_negative'\n",
    "            })\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def _preload_sku_data(self, source_df):\n",
    "        \"\"\"Pre-load and cache all SKU data including tokenized text.\"\"\"\n",
    "        tokenizers = Tokenizers()\n",
    "        transform = get_transform()\n",
    "        \n",
    "        sku_cache = {}\n",
    "        unique_skus = set()\n",
    "        \n",
    "        # Collect all unique SKUs from pairs\n",
    "        for pair in self.pairs:\n",
    "            unique_skus.add(pair['sku_first'])\n",
    "            unique_skus.add(pair['sku_second'])\n",
    "        \n",
    "        print(f\"Found {len(unique_skus)} unique SKUs\")\n",
    "        source_indexed = source_df.set_index('sku')\n",
    "        \n",
    "        # Single progress bar for all operations\n",
    "        pbar = tqdm(total=len(unique_skus), desc=\"Overall progress\")\n",
    "        \n",
    "        for sku in unique_skus:\n",
    "            if sku in source_indexed.index:\n",
    "                row = source_indexed.loc[sku]\n",
    "                \n",
    "                # Pre-tokenize text\n",
    "                name_tokens = tokenizers.tokenize_name([str(row['name'])])\n",
    "                desc_tokens = tokenizers.tokenize_description([str(row['description'])])\n",
    "                \n",
    "                # Pre-load and transform image\n",
    "                img_path = os.path.join(self.images_dir, row['image_name'])\n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = Image.fromarray(img)\n",
    "                    img = transform(img)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nWarning: Failed to load image for SKU {sku}: {e}\")\n",
    "                    img = torch.zeros(3, 224, 224)\n",
    "                \n",
    "                sku_cache[sku] = {\n",
    "                    'image': img,\n",
    "                    'name_tokens': name_tokens[0],  # Remove batch dimension\n",
    "                    'desc_tokens': desc_tokens[0],  # Remove batch dimension\n",
    "                }\n",
    "            \n",
    "            pbar.update(1)\n",
    "        \n",
    "        pbar.close()\n",
    "        return sku_cache\n",
    "    \n",
    "    def _generate_pairs_metadata(self, split_df, **kwargs):\n",
    "        \"\"\"Generate pair metadata with progress tracking.\"\"\"\n",
    "        print(\"Generating pairs with parameters:\")\n",
    "        for k, v in kwargs.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        random.seed(kwargs.get('random_seed', 42))\n",
    "            \n",
    "        pairs = []\n",
    "        \n",
    "        with tqdm(total=len(split_df), desc=\"Generating pairs\") as pbar:\n",
    "            for _, row in split_df.iterrows():\n",
    "                query_sku = row['sku_query']\n",
    "                pos_skus = row['sku_pos'] if isinstance(row['sku_pos'], list) else []\n",
    "                hard_neg_skus = row['sku_hard_neg'] if isinstance(row['sku_hard_neg'], list) else []\n",
    "                soft_neg_skus = row['sku_soft_neg'] if isinstance(row['sku_soft_neg'], list) else []\n",
    "                \n",
    "                # Generate positive pairs\n",
    "                pos_pairs = self._generate_positive_pairs_fixed(\n",
    "                    query_sku, pos_skus, \n",
    "                    max_pairs=kwargs.get('max_pos_pairs_per_query')\n",
    "                )\n",
    "                pairs.extend(pos_pairs)\n",
    "                \n",
    "                # Calculate negatives based on actual positive count\n",
    "                actual_pos_count = len(pos_pairs)\n",
    "                total_negatives_needed = int(actual_pos_count * kwargs.get('pos_neg_ratio', 1.0))\n",
    "                num_hard_neg = int(total_negatives_needed * kwargs.get('hard_soft_ratio', 0.5))\n",
    "                num_soft_neg = total_negatives_needed - num_hard_neg\n",
    "                \n",
    "                # Generate negative pairs\n",
    "                pairs.extend(self._generate_hard_negative_pairs(\n",
    "                    query_sku, pos_skus, hard_neg_skus, num_hard_neg\n",
    "                ))\n",
    "                pairs.extend(self._generate_soft_negative_pairs(\n",
    "                    query_sku, pos_skus, soft_neg_skus, num_soft_neg\n",
    "                ))\n",
    "                \n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                    'pos': len(pos_pairs),\n",
    "                    'hard': num_hard_neg,\n",
    "                    'soft': num_soft_neg\n",
    "                })\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        sku_first = pair['sku_first']\n",
    "        sku_second = pair['sku_second']\n",
    "        \n",
    "        # Get cached data (no I/O operations!)\n",
    "        data_first = self.sku_cache[sku_first]\n",
    "        data_second = self.sku_cache[sku_second]\n",
    "        \n",
    "        return {\n",
    "            'image_first': data_first['image'],\n",
    "            'name_first': data_first['name_tokens'],\n",
    "            'desc_first': data_first['desc_tokens'],\n",
    "            'image_second': data_second['image'],\n",
    "            'name_second': data_second['name_tokens'],\n",
    "            'desc_second': data_second['desc_tokens'],\n",
    "            'label': torch.tensor(pair['label'], dtype=torch.float32),\n",
    "            'pair_type': pair['pair_type']\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get detailed dataset statistics (alternative interface).\"\"\"\n",
    "        batch_stats = self.get_batch_stats()\n",
    "        \n",
    "        stats = {\n",
    "            'total_pairs': batch_stats['total_pairs'],\n",
    "            'unique_skus': len(self.sku_cache),\n",
    "            'pair_types': {\n",
    "                'positive': batch_stats['positives'],\n",
    "                'hard_negative': batch_stats['hard_negatives'],\n",
    "                'soft_negative': batch_stats['soft_negatives']\n",
    "            },\n",
    "            'memory_usage': {\n",
    "                'images': sum(img['image'].nelement() * img['image'].element_size() \n",
    "                            for img in self.sku_cache.values()) / 1024**2,  # MB\n",
    "                'tokens': sum((t['name_tokens'].nelement() + t['desc_tokens'].nelement()) * \n",
    "                            t['name_tokens'].element_size() \n",
    "                            for t in self.sku_cache.values()) / 1024**2  # MB\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def print_detailed_stats(self):\n",
    "        \"\"\"Print comprehensive dataset statistics.\"\"\"\n",
    "        stats = self.get_batch_stats()\n",
    "        \n",
    "        print(\"=== PrecomputedPairDataset Statistics ===\")\n",
    "        print(f\"📊 Dataset Overview:\")\n",
    "        print(f\"  Queries: {stats['queries']}\")\n",
    "        print(f\"  Total pairs: {stats['total_pairs']}\")\n",
    "        print(f\"  Avg pairs per query: {stats['avg_pairs_per_query']:.1f}\")\n",
    "        \n",
    "        print(f\"\\n🎯 Pair Breakdown:\")\n",
    "        print(f\"  Positives: {stats['positives']} ({stats['positives']/max(stats['total_pairs'],1)*100:.1f}%)\")\n",
    "        print(f\"  Hard negatives: {stats['hard_negatives']} ({stats['hard_negatives']/max(stats['total_pairs'],1)*100:.1f}%)\")\n",
    "        print(f\"  Soft negatives: {stats['soft_negatives']} ({stats['soft_negatives']/max(stats['total_pairs'],1)*100:.1f}%)\")\n",
    "        print(f\"  Total negatives: {stats['total_negatives']}\")\n",
    "        \n",
    "        print(f\"\\n⚖️ Ratios Achieved:\")\n",
    "        print(f\"  Target neg:pos ratio: {self.pos_neg_ratio:.2f}\")\n",
    "        print(f\"  Actual neg:pos ratio: {stats['actual_neg_pos_ratio']:.2f}\")\n",
    "        print(f\"  Target hard:soft ratio: {self.hard_soft_ratio:.2f}\")\n",
    "        print(f\"  Actual hard:soft ratio: {stats['actual_hard_soft_ratio']:.2f}\")\n",
    "        \n",
    "        print(f\"\\n📈 Per-Query Averages:\")\n",
    "        print(f\"  Avg positives per query: {stats['avg_pos_per_query']:.1f}\")\n",
    "        print(f\"  Avg negatives per query: {stats['avg_neg_per_query']:.1f}\")\n",
    "        print(f\"  Max pos pairs setting: {self.max_pos_pairs_per_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17b84c",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "542e2bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check that each image_name is present in the images_dir\n",
    "# images_dir = os.path.join(DATA_PATH, IMG_DATASET_NAME)\n",
    "# missing_images = []\n",
    "# for img_name in source_df['image_name']:\n",
    "#     img_path = os.path.join(images_dir, img_name)\n",
    "#     if not os.path.isfile(img_path):\n",
    "#         missing_images.append(img_name)\n",
    "\n",
    "# if missing_images:\n",
    "#     print(f\"Missing {len(missing_images)} images:\")\n",
    "#     print(missing_images[:10])  # Show up to 10 missing images\n",
    "# else:\n",
    "#     print(\"All images are present in the images_dir.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_threshold_tracking(model, optimizer, criterion, epochs_num, train_loader, \n",
    "                                valid_loader=None, device='cpu', print_epoch=False, \n",
    "                                models_dir=None, metric='f1', source_df=None, images_dir=None,\n",
    "                                precompute_pairs=False):\n",
    "    \"\"\"\n",
    "    Training function that handles both precomputed and on-the-fly data formats.\n",
    "    \n",
    "    Args:\n",
    "        precompute_pairs: If True, expects PrecomputedPairDataset format with pre-loaded tensors.\n",
    "                         If False, expects PairGenerationDataset format and converts SKUs to tensors.\n",
    "    \"\"\"\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    assert metric in ('f1', 'pos_acc'), \"metric must be 'f1' or 'pos_acc'\"\n",
    "\n",
    "    model.to(device)\n",
    "    train_losses, val_losses, thr_history = [], [], []\n",
    "    best_valid_metric, best_threshold = float('-inf'), None\n",
    "    best_weights = None\n",
    "\n",
    "    # Only initialize tokenizers if not using precomputed pairs\n",
    "    if not precompute_pairs:\n",
    "        tokenizers = Tokenizers()\n",
    "        transform = get_transform()\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"max\",\n",
    "        factor=0.1,\n",
    "        patience=SHEDULER_PATIENCE,\n",
    "        threshold=1e-4,\n",
    "        threshold_mode='rel'\n",
    "    )\n",
    "\n",
    "    if models_dir:\n",
    "        Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, epochs_num + 1):\n",
    "        # ---- training ----\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "            if precompute_pairs:\n",
    "                # PrecomputedPairDataset format - data is already tensors\n",
    "                im1 = batch['image_first'].to(device)\n",
    "                n1 = batch['name_first'].to(device)\n",
    "                d1 = batch['desc_first'].to(device)\n",
    "                im2 = batch['image_second'].to(device)\n",
    "                n2 = batch['name_second'].to(device)\n",
    "                d2 = batch['desc_second'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "            else:\n",
    "                # PairGenerationDataset format - convert SKUs to tensors\n",
    "                sku_first = batch['sku_first']\n",
    "                sku_second = batch['sku_second'] \n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # Convert SKUs to model inputs\n",
    "                im1, n1, d1 = sku_to_model_inputs(sku_first.tolist(), source_df, images_dir, tokenizers, transform)\n",
    "                im2, n2, d2 = sku_to_model_inputs(sku_second.tolist(), source_df, images_dir, tokenizers, transform)\n",
    "                \n",
    "                # Move to device\n",
    "                im1, n1, d1 = im1.to(device), n1.to(device), d1.to(device)\n",
    "                im2, n2, d2 = im2.to(device), n2.to(device), d2.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out1, out2 = model(im1, n1, d1, im2, n2, d2)\n",
    "            loss = criterion(out1, out2, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "        train_losses.append(total_train_loss / len(train_loader))\n",
    "\n",
    "        # ---- evaluation & checkpointing ----\n",
    "        if print_epoch and valid_loader is not None:\n",
    "            pos_acc, neg_acc, avg_acc, f1_val, val_loss, val_thr = evaluation(\n",
    "                model, criterion, valid_loader, epoch, device=device,\n",
    "                split_name='val', threshold=None, margin=CONTRASTIVE_MARGIN,\n",
    "                steps=200, metric=metric, source_df=source_df, images_dir=images_dir,\n",
    "                precompute_pairs=precompute_pairs\n",
    "            )\n",
    "            val_losses.append(val_loss)\n",
    "            thr_history.append(val_thr)\n",
    "\n",
    "            # pick the metric value to step & compare\n",
    "            cur_metric = pos_acc if metric == 'pos_acc' else f1_val\n",
    "            scheduler.step(cur_metric)\n",
    "\n",
    "            # save checkpoint every epoch if requested\n",
    "            if models_dir:\n",
    "                # Create detailed filename with validation metrics\n",
    "                checkpoint_filename = (\n",
    "                    f\"siamese_contrastive_soft-neg_epoch={epoch}_\"\n",
    "                    f\"val-f1={f1_val:.3f}_val-pos-acc={pos_acc:.3f}_val-neg-acc={neg_acc:.3f}_\"\n",
    "                    f\"{'_' + MODEL_NAME_POSTFIX if MODEL_NAME_POSTFIX else ''}_\"\n",
    "                    f\"{'_' + PRELOAD_MODEL_NAME if PRELOAD_MODEL_NAME else ''}_\"\n",
    "                    f\"best-{metric:.3f}-threshold={val_thr:.3f}.pt\"\n",
    "                )\n",
    "                \n",
    "                if isinstance(model, torch.nn.DataParallel):\n",
    "                    torch.save(model.module.state_dict(),\n",
    "                            Path(models_dir) / checkpoint_filename)\n",
    "                else:\n",
    "                    torch.save(model.state_dict(),\n",
    "                            Path(models_dir) / checkpoint_filename)\n",
    "\n",
    "            # update best if improved\n",
    "            if cur_metric > best_valid_metric:\n",
    "                best_valid_metric = cur_metric\n",
    "                best_threshold = val_thr\n",
    "                if isinstance(model, torch.nn.DataParallel):\n",
    "                    best_weights = deepcopy(model.module.state_dict())\n",
    "                else:\n",
    "                    best_weights = deepcopy(model.state_dict())\n",
    "\n",
    "        print(f'Epoch {epoch} done.')\n",
    "\n",
    "    print(f\"Best evaluation {metric}: {best_valid_metric:.3f}  (thr={best_threshold:.3f})\")\n",
    "    return train_losses, val_losses, best_valid_metric, best_weights, best_threshold\n",
    "\n",
    "\n",
    "def get_optimal_num_workers():\n",
    "    \"\"\"Calculate optimal number of workers based on system specs.\"\"\"\n",
    "    import psutil\n",
    "    \n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    num_cpus = psutil.cpu_count(logical=False)  # Physical cores\n",
    "    \n",
    "    if num_gpus > 1:\n",
    "        # For multi-GPU: fewer workers per GPU to avoid context switching\n",
    "        workers = min(4, num_cpus // num_gpus)\n",
    "    else:\n",
    "        # For single GPU: can use more workers\n",
    "        workers = min(8, num_cpus)\n",
    "    \n",
    "    return max(2, workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fe2fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run(\n",
    "    limit_train_pos_pairs_per_query=None,\n",
    "    limit_val_pos_pairs_per_query=None,\n",
    "    limit_test_pos_pairs_per_query=None,\n",
    "    batch_size_per_device=None,\n",
    "    precompute_pairs=False,\n",
    "    images_dir=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Updated _run() function that tracks best thresholds per epoch to avoid duplicate evaluation.\n",
    "    \"\"\"\n",
    "    assert images_dir is not None\n",
    "\n",
    "    # Handle batch size scaling for multi-GPU\n",
    "    if batch_size_per_device is None:\n",
    "        batch_size_per_device = batch_size_per_device\n",
    "    \n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 1:\n",
    "        print(f\"Using {num_gpus} GPUs with DataParallel\")\n",
    "        original_batch_size = batch_size_per_device\n",
    "        batch_size = batch_size_per_device * num_gpus\n",
    "        print(f\"Scaled batch size from {original_batch_size} to {batch_size}\")\n",
    "    else:\n",
    "        batch_size = batch_size_per_device\n",
    "\n",
    "    # ---------- 1) build DataLoaders with PairGenerationDataset ----------\n",
    "    \n",
    "    if LIMIT_QUERIES:\n",
    "        sampled_queries = pairwise_mapping_df['sku_query'].drop_duplicates().sample(n=LIMIT_QUERIES, random_state=RANDOM_SEED)\n",
    "        actual_pairwise_mapping_df = pairwise_mapping_df[pairwise_mapping_df['sku_query'].isin(sampled_queries)]\n",
    "    else:\n",
    "        actual_pairwise_mapping_df = pairwise_mapping_df\n",
    "\n",
    "    splits_dataset = split_query_groups(\n",
    "        actual_pairwise_mapping_df,\n",
    "        test_size=TEST_RATIO,\n",
    "        val_size=VAL_RATIO,\n",
    "        random_state=RANDOM_SEED,\n",
    "    )\n",
    "\n",
    "    splits = {'train': splits_dataset['train'],\n",
    "            'val': splits_dataset['val'], \n",
    "            'test': splits_dataset['test']}\n",
    "    loaders = {}\n",
    "\n",
    "    SPLIT_LIMITS = {\n",
    "        'train': limit_train_pos_pairs_per_query,\n",
    "        'val': limit_val_pos_pairs_per_query,\n",
    "        'test': limit_test_pos_pairs_per_query\n",
    "    }\n",
    "\n",
    "    optimal_workers = get_optimal_num_workers()\n",
    "    print(f\"Using {optimal_workers} workers for data loading\")\n",
    "\n",
    "    for split_name, split_df in splits.items():\n",
    "        if precompute_pairs:\n",
    "            dataset = PrecomputedPairDataset(\n",
    "                split_df=splits_dataset['train'],\n",
    "                source_df=source_df,\n",
    "                images_dir=images_dir,\n",
    "                max_pos_pairs_per_query=LIMIT_TRAIN_POS_PAIRS_PER_QUERY,\n",
    "                pos_neg_ratio=POS_NEG_RATIO,\n",
    "                hard_soft_ratio=HARD_SOFT_RATIO\n",
    "            )\n",
    "\n",
    "            # Print dataset statistics\n",
    "            stats = dataset.get_stats()\n",
    "            print(\"\\nDataset Statistics:\")\n",
    "            print(f\"Total pairs: {stats['total_pairs']}\")\n",
    "            print(f\"Unique SKUs: {stats['unique_skus']}\")\n",
    "            print(\"\\nPair Types:\")\n",
    "            for pair_type, count in stats['pair_types'].items():\n",
    "                print(f\"  {pair_type}: {count}\")\n",
    "            print(\"\\nMemory Usage:\")\n",
    "            print(f\"  Images: {stats['memory_usage']['images']:.1f} MB\")\n",
    "            print(f\"  Tokens: {stats['memory_usage']['tokens']:.1f} MB\")\n",
    "        else:\n",
    "            dataset = PairGenerationDataset(\n",
    "                split_df=split_df,\n",
    "                cluster_emb_table=cluster_emb_table,\n",
    "                source_df=source_df,\n",
    "                images_dir=images_dir,\n",
    "                max_pos_pairs_per_query=SPLIT_LIMITS[split_name],\n",
    "                pos_neg_ratio=POS_NEG_RATIO,\n",
    "                hard_soft_ratio=HARD_SOFT_RATIO,\n",
    "                random_seed=RANDOM_SEED\n",
    "            )\n",
    "        \n",
    "        stats = dataset.get_batch_stats()\n",
    "        print(f\"{split_name.upper()} Dataset - Total: {stats['total_pairs']}, \"\n",
    "            f\"Pos: {stats['positives']}, Hard Neg: {stats['hard_negatives']}, \"\n",
    "            f\"Soft Neg: {stats['soft_negatives']}\")\n",
    "\n",
    "        loaders[split_name] = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=(split_name == 'train'),\n",
    "            num_workers=optimal_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=4,  # Pre-fetch more batches\n",
    "            drop_last=True,     # Avoid small last batches\n",
    "        )\n",
    "\n",
    "    train_loader = loaders['train']\n",
    "    valid_loader = loaders['val']\n",
    "    test_loader = loaders['test']\n",
    "\n",
    "    # ---------- 2) model / optimizer / criterion ----------\n",
    "    print('\\n===== Starting training =====')\n",
    "    print(\"Loading model and optimizer…\")\n",
    "    model = SiameseRuCLIP(\n",
    "        DEVICE,\n",
    "        NAME_MODEL_NAME,\n",
    "        DESCRIPTION_MODEL_NAME,\n",
    "        PRELOAD_MODEL_NAME,\n",
    "        models_dir=DATA_PATH + RESULTS_DIR,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "    print(\"Loaded model and optimizer.\")\n",
    "    \n",
    "    # ADD MULTI-GPU SUPPORT\n",
    "    if num_gpus > 1:\n",
    "        print(f\"Using {num_gpus} GPUs with DataParallel\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # ADD MISSING CRITERION AND OPTIMIZER\n",
    "    criterion = ContrastiveLoss(margin=CONTRASTIVE_MARGIN).to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=LR, \n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    # ---------- 3) training with threshold tracking ----------\n",
    "    with tempfile.TemporaryDirectory() as tmp_ckpt_dir:\n",
    "        (train_losses, val_losses,\n",
    "        best_metric_val, best_weights, \n",
    "        best_threshold) = train_with_threshold_tracking(\n",
    "            model, optimizer, criterion,\n",
    "            EPOCHS, train_loader, valid_loader,\n",
    "            print_epoch=True, device=DEVICE,\n",
    "            models_dir=tmp_ckpt_dir,\n",
    "            metric=BEST_CKPT_METRIC,\n",
    "            source_df=source_df,\n",
    "            images_dir=images_dir,\n",
    "            precompute_pairs=precompute_pairs  # Set this based on your dataset type\n",
    "        )\n",
    "\n",
    "    print(f\"→ Best evaluation {BEST_CKPT_METRIC}: {best_metric_val:.3f} (threshold: {best_threshold:.3f})\")\n",
    "\n",
    "    # ---------- 4) loss curves ----------\n",
    "    if len(train_losses) >= 2:\n",
    "        epochs_ax = list(range(2, len(train_losses) + 1))\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(epochs_ax, train_losses[1:], label='Train Loss')\n",
    "        ax.plot(epochs_ax, val_losses[1:],   label='Val   Loss')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title('Training & evaluation Loss by Epoch')\n",
    "        ax.legend()\n",
    "        if MLFLOW_URI:\n",
    "            mlflow.log_figure(fig, 'loss_by_epoch.png')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # ---------- 5) final TEST (using best threshold) ----------\n",
    "    model.load_state_dict(best_weights)\n",
    "    print(f\"Using best threshold: {best_threshold:.3f}\")\n",
    "    \n",
    "    (test_pos_acc, test_neg_acc,\n",
    "     test_acc, test_f1,\n",
    "     test_loss, _) = evaluation(\n",
    "        model, criterion, test_loader,\n",
    "        epoch='test', device=DEVICE,\n",
    "        split_name='test',\n",
    "        threshold=best_threshold,\n",
    "        metric=BEST_CKPT_METRIC,\n",
    "        source_df=source_df,\n",
    "        images_dir=images_dir,\n",
    "        precompute_pairs=precompute_pairs\n",
    "    )\n",
    "\n",
    "    test_metric = test_pos_acc if BEST_CKPT_METRIC == 'pos_acc' else test_f1\n",
    "    print(f\"Test {BEST_CKPT_METRIC}: {test_metric:.3f}\")\n",
    "\n",
    "    # ---------- 6) save checkpoint ----------\n",
    "    filename = (\n",
    "        f\"siamese_contrastive_soft-neg_epoch={EPOCHS}_\"\n",
    "        f\"test-f1={test_f1:.3f}_test-pos-acc={test_pos_acc:.3f}_test-neg-acc={test_neg_acc:.3f}_\"\n",
    "        f\"{'_' + MODEL_NAME_POSTFIX if MODEL_NAME_POSTFIX else ''}_\"\n",
    "        f\"{'_' + PRELOAD_MODEL_NAME if PRELOAD_MODEL_NAME else ''}_\"\n",
    "        f\"best-{BEST_CKPT_METRIC:.3f}-threshold={best_threshold:.3f}.pt\"\n",
    "    )\n",
    "    final_path = Path(DATA_PATH + RESULTS_DIR) / filename\n",
    "    final_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Handle DataParallel state dict saving\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        torch.save(best_weights, final_path)\n",
    "    else:\n",
    "        torch.save(best_weights, final_path)\n",
    "    \n",
    "    print(f\"Saved best‐{BEST_CKPT_METRIC} checkpoint to:\\n{final_path}\")\n",
    "\n",
    "    if MLFLOW_URI:\n",
    "        mlflow.log_metric(\"test_pos_accuracy\", test_pos_acc)\n",
    "        mlflow.log_metric(\"test_neg_accuracy\", test_neg_acc)\n",
    "        mlflow.log_metric(\"test_accuracy\",     test_acc)\n",
    "        mlflow.log_metric(\"test_f1_score\",     test_f1)\n",
    "        mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4943cba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 workers for data loading\n",
      "\n",
      "=== Initializing PrecomputedPairDataset ===\n",
      "\n",
      "1. Generating pairs metadata...\n",
      "Generating pairs with parameters:\n",
      "  max_pos_pairs_per_query: 2\n",
      "  pos_neg_ratio: 1.0\n",
      "  hard_soft_ratio: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs: 100%|██████████| 2/2 [00:00<00:00, 819.52it/s, pos=2, hard=1, soft=1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8 pairs\n",
      "\n",
      "2. Starting data precomputation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 unique SKUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall progress: 100%|██████████| 15/15 [00:00<00:00, 26.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Ready ===\n",
      "Total pairs: 8\n",
      "Unique SKUs cached: 15\n",
      "\n",
      "Dataset Statistics:\n",
      "Total pairs: 8\n",
      "Unique SKUs: 15\n",
      "\n",
      "Pair Types:\n",
      "  positive: 4\n",
      "  hard_negative: 2\n",
      "  soft_negative: 2\n",
      "\n",
      "Memory Usage:\n",
      "  Images: 8.6 MB\n",
      "  Tokens: 0.0 MB\n",
      "TRAIN Dataset - Total: 8, Pos: 4, Hard Neg: 2, Soft Neg: 2\n",
      "\n",
      "=== Initializing PrecomputedPairDataset ===\n",
      "\n",
      "1. Generating pairs metadata...\n",
      "Generating pairs with parameters:\n",
      "  max_pos_pairs_per_query: 2\n",
      "  pos_neg_ratio: 1.0\n",
      "  hard_soft_ratio: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs: 100%|██████████| 2/2 [00:00<00:00, 395.20it/s, pos=2, hard=1, soft=1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8 pairs\n",
      "\n",
      "2. Starting data precomputation...\n",
      "Found 15 unique SKUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall progress: 100%|██████████| 15/15 [00:00<00:00, 31.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Ready ===\n",
      "Total pairs: 8\n",
      "Unique SKUs cached: 15\n",
      "\n",
      "Dataset Statistics:\n",
      "Total pairs: 8\n",
      "Unique SKUs: 15\n",
      "\n",
      "Pair Types:\n",
      "  positive: 4\n",
      "  hard_negative: 2\n",
      "  soft_negative: 2\n",
      "\n",
      "Memory Usage:\n",
      "  Images: 8.6 MB\n",
      "  Tokens: 0.0 MB\n",
      "VAL Dataset - Total: 8, Pos: 4, Hard Neg: 2, Soft Neg: 2\n",
      "\n",
      "=== Initializing PrecomputedPairDataset ===\n",
      "\n",
      "1. Generating pairs metadata...\n",
      "Generating pairs with parameters:\n",
      "  max_pos_pairs_per_query: 2\n",
      "  pos_neg_ratio: 1.0\n",
      "  hard_soft_ratio: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating pairs: 100%|██████████| 2/2 [00:00<00:00, 421.64it/s, pos=2, hard=1, soft=1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8 pairs\n",
      "\n",
      "2. Starting data precomputation...\n",
      "Found 15 unique SKUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall progress: 100%|██████████| 15/15 [00:00<00:00, 30.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Ready ===\n",
      "Total pairs: 8\n",
      "Unique SKUs cached: 15\n",
      "\n",
      "Dataset Statistics:\n",
      "Total pairs: 8\n",
      "Unique SKUs: 15\n",
      "\n",
      "Pair Types:\n",
      "  positive: 4\n",
      "  hard_negative: 2\n",
      "  soft_negative: 2\n",
      "\n",
      "Memory Usage:\n",
      "  Images: 8.6 MB\n",
      "  Tokens: 0.0 MB\n",
      "TEST Dataset - Total: 8, Pos: 4, Hard Neg: 2, Soft Neg: 2\n",
      "\n",
      "===== Starting training =====\n",
      "Loading model and optimizer…\n",
      "Loaded model and optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 8/8 [00:11<00:00,  1.40s/it]\n",
      "Evaluation on val: 100%|██████████| 8/8 [00:02<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] Epoch 1 – loss: 1.1622, P Acc: 0.250, N Acc: 1.000, Avg Acc: 0.625, F1: 0.400, thr*: 0.641 (optimised: f1)\n",
      "Epoch 1 done.\n",
      "Best evaluation f1: 0.400  (thr=0.641)\n",
      "→ Best evaluation f1: 0.400 (threshold: 0.641)\n",
      "Using best threshold: 0.641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation on test: 100%|██████████| 8/8 [00:02<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] Epoch test – loss: 1.1622, P Acc: 0.250, N Acc: 1.000, Avg Acc: 0.625, F1: 0.400, thr*: 0.641 (optimised: f1)\n",
      "Test f1: 0.400\n",
      "Saved best‐f1 checkpoint to:\n",
      "data/train_results/siamese_contrastive_test-f1=0.400_test-pos-acc=0.250_test-neg-acc=1.000_splitting-by-query_cc12m_rubert_tiny_ep_1.pt_best-0.4-threshold=0.641.pt\n"
     ]
    }
   ],
   "source": [
    "_run(\n",
    "    limit_train_pos_pairs_per_query=LIMIT_TRAIN_POS_PAIRS_PER_QUERY,\n",
    "    limit_val_pos_pairs_per_query=LIMIT_VAL_POS_PAIRS_PER_QUERY,\n",
    "    limit_test_pos_pairs_per_query=LIMIT_TEST_POS_PAIRS_PER_QUERY,\n",
    "    batch_size_per_device=BATCH_SIZE_PER_DEVICE,\n",
    "    precompute_pairs=True,\n",
    "    images_dir = os.path.join(DATA_PATH, IMG_DATASET_NAME)\n",
    "\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

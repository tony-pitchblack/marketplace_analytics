{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcaee945",
   "metadata": {},
   "source": [
    "# Installs & tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9b411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import mlflow\n",
    "except ImportError:\n",
    "    !pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65f9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import dotenv\n",
    "except ImportError:\n",
    "    !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0c3f67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Kaggle environment. Skipping Kaggle secrets.\n",
      "Trying to load HF_TOKEN from .env.\n",
      "Success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Log into huggingface via Kaggle Secrets or .env\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import huggingface_hub\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "    user_secrets = UserSecretsClient()\n",
    "    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not Kaggle environment. Skipping Kaggle secrets.\")\n",
    "    print(\"Trying to load HF_TOKEN from .env.\")\n",
    "    load_dotenv()\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "    print(\"Success!\")\n",
    "\n",
    "huggingface_hub.login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a0467",
   "metadata": {},
   "source": [
    "# Choose notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be91836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "## CHOOSE MODEL PARAMETERS #################################################\n",
    "\n",
    "MODEL_NAME_POSTFIX='splitting-by-query'\n",
    "NAME_MODEL_NAME = 'cointegrated/rubert-tiny' # 'DeepPavlov/distilrubert-tiny-cased-conversational-v1'\n",
    "DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny'\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "RESULTS_DIR = 'train_results/'\n",
    "\n",
    "# BATCH_SIZE=60 # uses 14.5GiB of 1 GPU\n",
    "# NUM_WORKERS=2 # TODO: use multiple GPU, tune number of workers\n",
    "# NUM_DEBUG_SAMPLES=None\n",
    "# EPOCHS=10 # epochs > 8 => overfit; NOTE: can train for longer since we take best validation checkpoint anyway\n",
    "\n",
    "BATCH_SIZE=1\n",
    "NUM_WORKERS=0\n",
    "NUM_DEBUG_SAMPLES=2\n",
    "EPOCHS=2\n",
    "\n",
    "PRELOAD_MODEL_NAME = 'cc12m_rubert_tiny_ep_1.pt' # preload ruclip\n",
    "# PRELOAD_MODEL_NAME = None\n",
    "POS_WEIGHT = 4.0 # TODO: infer from data\n",
    "\n",
    "VALIDATION_SPLIT=.05\n",
    "TEST_SPLIT=.1\n",
    "RANDOM_SEED=42\n",
    "LR=9e-5\n",
    "MOMENTUM=0.9\n",
    "WEIGHT_DECAY=1e-2\n",
    "CONTRASTIVE_MARGIN=1.5\n",
    "CONTRASTIVE_THRESHOLD=0.3\n",
    "SHEDULER_PATIENCE=3 # in epochs\n",
    "\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d9e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHOOSE DATA #########################################################\n",
    "\n",
    "# # These table files need 'image_name_first', 'image_name_second' constructed from sku to be usable in current pipeline\n",
    "# TABLE_DATASET_FILE = 'tables_labeled/processed/labeled_1.3k_with-options.csv'\n",
    "# TABLE_DATASET_FILE = 'tables_labeled/processed/labeled_56k_with-options.csv'\n",
    "# IMG_DATASET_NAME = 'images_7k'\n",
    "# STRATIFY_COLS = None\n",
    "\n",
    "# TABLE_DATASET_FILE = 'tables_labeled/processed/labeled_5k_with-options.csv'\n",
    "# IMG_DATASET_NAME = 'images_7k' \n",
    "# STRATIFY_COLS = None\n",
    "\n",
    "# TABLE_DATASET_FILE = 'tables_WB_OZ_100/WB_OZ_100.csv'\n",
    "# TABLE_DATASET_FILE = 'tables_WB_OZ_100/WB_OZ_100_conjugated.csv'\n",
    "# TABLE_DATASET_FILE = 'tables_WB_OZ_100/WB_OZ_100_conjugated_shuffled_seed=42_fraction=1.csv'\n",
    "# TABLE_DATASET_FILE = 'tables_WB_OZ_100/WB_OZ_100_conjugated_shuffled_seed=42_fraction=0.5.csv'\n",
    "# IMG_DATASET_NAME = 'images_WB_OZ_100'\n",
    "# STRATIFY_COLS = None\n",
    "\n",
    "TABLE_DATASET_FILE = 'tables_OZ_geo_5500/processed/regex-pairwise-dataset_num-queries=20_num-pairs=6226_patterns-dict-hash=6dbf9b3ef9568e60cd959f87be7e3b26.csv'\n",
    "IMG_DATASET_NAME = 'images_OZ_geo_5500'\n",
    "STRATIFY_COLS = ['sku_first', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b5eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOGGING PARAMS ######################################################################\n",
    "\n",
    "# MLFLOW_URI = \"http://176.56.185.96:5000\"\n",
    "# MLFLOW_URI = \"http://localhost:5000\"\n",
    "MLFLOW_URI = None\n",
    "\n",
    "MLFLOW_EXPERIMENT = \"siamese/1fold\"\n",
    "\n",
    "TELEGRAM_TOKEN = None\n",
    "# TELEGRAM_TOKEN = '' # set token to get notifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b40bc83",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00017085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "from timm import create_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "# import transformers\n",
    "# from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer,\\\n",
    "#         get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# import json\n",
    "# from itertools import product\n",
    "\n",
    "# import datasets\n",
    "# from datasets import Dataset, concatenate_datasets\n",
    "# import argparse\n",
    "import requests\n",
    "\n",
    "# from io import BytesIO\n",
    "# from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "# import more_itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import mlflow\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3494173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tg_report(text, token=None) -> None:\n",
    "    method = 'sendMessage'\n",
    "    chat_id = 324956476\n",
    "    _ = requests.post(\n",
    "            url='https://api.telegram.org/bot{0}/{1}'.format(token, method),\n",
    "            data={'chat_id': chat_id, 'text': text} \n",
    "        ).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34706f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuCLIPtiny(nn.Module):\n",
    "    def __init__(self, name_model_name):\n",
    "        super().__init__()\n",
    "        self.visual = create_model('convnext_tiny',\n",
    "                                   pretrained=False, # TODO: берём претрейн\n",
    "                                   num_classes=0,\n",
    "                                   in_chans=3)  # out 768\n",
    "\n",
    "        self.transformer = AutoModel.from_pretrained(name_model_name)\n",
    "        name_model_output_shape = self.transformer.config.hidden_size  # dynamically get hidden size\n",
    "        self.final_ln = torch.nn.Linear(name_model_output_shape, 768)  # now uses the transformer hidden size\n",
    "        self.logit_scale = torch.nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.stem[0].weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        x = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = x.last_hidden_state[:, 0, :]\n",
    "        x = self.final_ln(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(input_ids, attention_mask)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84518fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        _convert_image_to_rgb,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]), ])\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "class Tokenizers:\n",
    "    def __init__(self):\n",
    "        self.name_tokenizer = AutoTokenizer.from_pretrained(NAME_MODEL_NAME)\n",
    "        self.desc_tokenizer = AutoTokenizer.from_pretrained(DESCRIPTION_MODEL_NAME)\n",
    "\n",
    "    def tokenize_name(self, texts, max_len=77):\n",
    "        tokenized = self.name_tokenizer.batch_encode_plus(texts,\n",
    "                                                     truncation=True,\n",
    "                                                     add_special_tokens=True,\n",
    "                                                     max_length=max_len,\n",
    "                                                     padding='max_length',\n",
    "                                                     return_attention_mask=True,\n",
    "                                                     return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "    \n",
    "    def tokenize_description(self, texts, max_len=77):\n",
    "        tokenized = self.desc_tokenizer(texts,\n",
    "                                        truncation=True,\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=max_len,\n",
    "                                        padding='max_length',\n",
    "                                        return_attention_mask=True,\n",
    "                                        return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "\n",
    "class SiameseRuCLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df=None, labels=None, df_path=None, images_dir=DATA_PATH+'images/'):\n",
    "        # loads data either from path using `df_path` or directly from `df` argument\n",
    "        self.df = pd.read_csv(df_path) if df_path is not None else df\n",
    "        self.labels = labels\n",
    "        self.images_dir = images_dir\n",
    "        self.tokenizers = Tokenizers()\n",
    "        self.transform = get_transform()\n",
    "        # \n",
    "        self.max_len = 77\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        name_tokens = self.tokenizers.tokenize_name([str(row.name_first), \n",
    "                                               str(row.name_second)], max_len=self.max_len)\n",
    "        name_first = name_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        name_second = name_tokens[:, 1, :]\n",
    "        desc_tokens = self.tokenizers.tokenize_description([str(row.description_first), \n",
    "                                               str(row.description_second)])\n",
    "        desc_first = desc_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        desc_second = desc_tokens[:, 1, :]\n",
    "        im_first = cv2.imread(os.path.join(self.images_dir, row.image_name_first))\n",
    "        im_first = cv2.cvtColor(im_first, cv2.COLOR_BGR2RGB)\n",
    "        im_first = Image.fromarray(im_first)\n",
    "        im_first = self.transform(im_first)\n",
    "        im_second = cv2.imread(os.path.join(self.images_dir, row.image_name_second))\n",
    "        im_second = cv2.cvtColor(im_second, cv2.COLOR_BGR2RGB)\n",
    "        im_second = Image.fromarray(im_second)\n",
    "        im_second = self.transform(im_second)\n",
    "        label = self.labels[idx]\n",
    "        return im_first, name_first, desc_first, im_second, name_second, desc_second, label\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2d263be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "class SiameseRuCLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 device: str,\n",
    "                 name_model_name: str,\n",
    "                 description_model_name: str,\n",
    "                 preload_model_name: str = None,\n",
    "                 models_dir: str = None):\n",
    "        \"\"\"\n",
    "        Initializes the SiameseRuCLIP model.\n",
    "        Required parameters:\n",
    "          - models_dir: directory containing saved checkpoints.\n",
    "          - name_model_name: model name for text (name) branch.\n",
    "          - description_model_name: model name for description branch.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        device = torch.device(device)\n",
    "\n",
    "        # Initialize RuCLIPtiny\n",
    "        self.ruclip = RuCLIPtiny(name_model_name)\n",
    "        if preload_model_name is not None:\n",
    "            std = torch.load(\n",
    "                os.path.join(models_dir, preload_model_name),\n",
    "                weights_only=True,\n",
    "                map_location=device\n",
    "            )\n",
    "            self.ruclip.load_state_dict(std)\n",
    "            self.ruclip.eval()\n",
    "        self.ruclip = self.ruclip.to(device)\n",
    "\n",
    "        # Initialize the description transformer\n",
    "        self.description_transformer = AutoModel.from_pretrained(description_model_name)\n",
    "        self.description_transformer = self.description_transformer.to(device)\n",
    "\n",
    "        # Determine dimensionality\n",
    "        vision_dim = self.ruclip.visual.num_features\n",
    "        name_dim = self.ruclip.final_ln.out_features\n",
    "        desc_dim = self.description_transformer.config.hidden_size\n",
    "        self.hidden_dim = vision_dim + name_dim + desc_dim\n",
    "\n",
    "        # Define MLP head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim // 2, self.hidden_dim // 4),\n",
    "        ).to(device)\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.ruclip.encode_image(image)\n",
    "\n",
    "    def encode_name(self, name):\n",
    "        return self.ruclip.encode_text(name[:, 0, :], name[:, 1, :])\n",
    "\n",
    "    def encode_description(self, desc):\n",
    "        last_hidden_states = self.description_transformer(desc[:, 0, :], desc[:, 1, :]).last_hidden_state\n",
    "        attention_mask = desc[:, 1, :]\n",
    "        return average_pool(last_hidden_states, attention_mask)\n",
    "\n",
    "    def get_final_embedding(self, im, name, desc):\n",
    "        image_emb = self.encode_image(im)\n",
    "        name_emb = self.encode_name(name)\n",
    "        desc_emb = self.encode_description(desc)\n",
    "\n",
    "        # Concatenate the embeddings and forward through the head\n",
    "        combined_emb = torch.cat([image_emb, name_emb, desc_emb], dim=1)\n",
    "        final_embedding = self.head(combined_emb)\n",
    "        return final_embedding\n",
    "\n",
    "    def forward(self, im1, name1, desc1, im2, name2, desc2):\n",
    "        out1 = self.get_final_embedding(im1, name1, desc1)\n",
    "        out2 = self.get_final_embedding(im2, name2, desc2)\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "987d1ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # old\n",
    "# class ContrastiveLoss(torch.nn.Module):\n",
    "#     def __init__(self, margin=2.0):\n",
    "#         super(ContrastiveLoss, self).__init__()\n",
    "#         self.margin = margin\n",
    "        \n",
    "#     def __name__(self,):\n",
    "#         return 'ContrastiveLoss'\n",
    "\n",
    "#     def forward(self, output1, output2, label):\n",
    "#         euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "#         pos = (1-label) * torch.pow(euclidean_distance, 2)\n",
    "#         neg = label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "#         loss_contrastive = torch.mean( pos + neg )\n",
    "#         return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36cc0798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin: float = 1.5, pos_weight: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.margin      = margin\n",
    "        self.pos_weight  = pos_weight\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        d   = F.pairwise_distance(output1, output2)\n",
    "        pos = (1 - label) * d.pow(2)                            # duplicates (label==0)\n",
    "        neg = label * F.relu(self.margin - d).pow(2)            # different (label==1)\n",
    "        return (self.pos_weight * pos + neg).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe81dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot epoch after each train epoch in `train()`\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_epoch(loss_history, filename=\"data/runs_artifacts/epoch_loss.png\") -> None:\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Iteration number\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(loss_history, 'b')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)  # Save the plot to a file\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9c6281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pair(output1, output2, target, threshold):\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    # меньше границы, там где будет True — конкуренты\n",
    "    cond = euclidean_distance < threshold\n",
    "    pos_sum = 0\n",
    "    neg_sum = 0\n",
    "    pos_acc = 0\n",
    "    neg_acc = 0\n",
    "\n",
    "    for i in range(len(cond)):\n",
    "        # 1 значит не конкуренты\n",
    "        if target[i]:\n",
    "            neg_sum+=1\n",
    "            # 0 в cond значит дальше друг от друга чем threshold\n",
    "            if not cond[i]:\n",
    "                neg_acc+=1\n",
    "        elif not target[i]:\n",
    "            pos_sum+=1\n",
    "            if cond[i]:\n",
    "                pos_acc+=1\n",
    "\n",
    "    return pos_acc, pos_sum, neg_acc, neg_sum\n",
    "\n",
    "def predict(out1, out2, threshold=CONTRASTIVE_THRESHOLD):\n",
    "    # вернёт 1 если похожи\n",
    "    return F.pairwise_distance(out1, out2) < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "319a25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def validation(model, criterion, data_loader, epoch,\n",
    "               device='cpu', split_name='validation',\n",
    "               threshold=None,     # ← default: search automatically\n",
    "               margin=1.5,         # search range 0‒margin\n",
    "               steps=200):\n",
    "    \"\"\"\n",
    "    Runs one pass over `data_loader`, returning:\n",
    "      pos_acc, neg_acc, avg_acc, f1, avg_loss, best_thr\n",
    "    If `threshold` is None, the function sweeps `steps` evenly-spaced\n",
    "    thresholds in [0, margin] and picks the one that maximises F1.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_d, all_lbl = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=f\"{split_name}\"):\n",
    "            im1, n1, d1, im2, n2, d2, lbl = batch\n",
    "            im1, n1, d1, im2, n2, d2, lbl = (\n",
    "                im1.to(device), n1.to(device), d1.to(device),\n",
    "                im2.to(device), n2.to(device), d2.to(device),\n",
    "                lbl.to(device)\n",
    "            )\n",
    "            out1, out2 = model(im1, n1, d1, im2, n2, d2)\n",
    "            total_loss += criterion(out1, out2, lbl).item()\n",
    "            all_d   .append(F.pairwise_distance(out1, out2).cpu())\n",
    "            all_lbl .append(lbl.cpu())\n",
    "\n",
    "    distances = torch.cat(all_d)\n",
    "    labels    = torch.cat(all_lbl)               # 0 = duplicate, 1 = different\n",
    "    avg_loss  = total_loss / len(data_loader)\n",
    "\n",
    "    # ----- choose threshold if not given -----\n",
    "    if threshold is None:\n",
    "        grid = np.linspace(0.0, margin, steps)\n",
    "        best_f1, best_thr = -1.0, 0.0\n",
    "        y_true = (labels.numpy() == 0).astype(int)   # 1 = positive\n",
    "        for t in grid:\n",
    "            y_pred = (distances.numpy() < t).astype(int)\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_thr = f1, t\n",
    "        threshold = best_thr\n",
    "    else:\n",
    "        best_thr = threshold\n",
    "\n",
    "    # ----- metrics with the chosen threshold -----\n",
    "    preds = (distances < best_thr).long()\n",
    "    pos_mask, neg_mask = (labels == 0), (labels == 1)\n",
    "\n",
    "    pos_acc = (preds[pos_mask] == 1).float().mean().item() if pos_mask.any() else 0.0\n",
    "    neg_acc = (preds[neg_mask] == 0).float().mean().item() if neg_mask.any() else 0.0\n",
    "    avg_acc = (pos_acc + neg_acc) / 2.0\n",
    "    f1      = f1_score((labels.numpy()==0).astype(int),\n",
    "                       preds.numpy(), zero_division=0)\n",
    "\n",
    "    report = (f\"[{split_name}] Epoch {epoch} – \"\n",
    "              f\"loss: {avg_loss:.4f}, \"\n",
    "              f\"P Acc: {pos_acc:.3f}, \"\n",
    "              f\"N Acc: {neg_acc:.3f}, \"\n",
    "              f\"Avg Acc: {avg_acc:.3f}, \"\n",
    "              f\"F1: {f1:.3f}, \"\n",
    "              f\"thr*: {best_thr:.3f}\")\n",
    "    print(report)\n",
    "    make_tg_report(report, TELEGRAM_TOKEN)\n",
    "\n",
    "    if MLFLOW_URI and split_name == 'validation':\n",
    "        mlflow.log_metric(\"valid_f1_score\", f1, step=epoch)\n",
    "\n",
    "    return pos_acc, neg_acc, avg_acc, f1, avg_loss, best_thr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ff968d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion,\n",
    "          epochs_num, train_loader, valid_loader=None,\n",
    "          device='cpu', print_epoch=False,\n",
    "          models_dir=None):\n",
    "    \"\"\"\n",
    "    Trains for `epochs_num` epochs, sweeping the threshold on the\n",
    "    validation split each epoch.  Returns:\n",
    "      train_losses, val_losses, best_valid_f1, best_weights, thr_history\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    train_losses, val_losses, thr_history = [], [], []\n",
    "    best_valid_f1, best_threshold = float('-inf'), None\n",
    "    best_weights = None\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\",\n",
    "        factor=0.1, patience=SHEDULER_PATIENCE,\n",
    "        threshold=1e-4, threshold_mode='rel'\n",
    "    )\n",
    "\n",
    "    if models_dir:\n",
    "        Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, epochs_num + 1):\n",
    "        # ---------- training ----------\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"train {epoch}/{epochs_num}\"):\n",
    "            im1, n1, d1, im2, n2, d2, lbl = [t.to(device) for t in batch]\n",
    "            optimizer.zero_grad()\n",
    "            out1, out2 = model(im1, n1, d1, im2, n2, d2)\n",
    "            loss = criterion(out1, out2, lbl)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(total_train_loss / len(train_loader))\n",
    "\n",
    "        # ---------- validation ----------\n",
    "        if print_epoch and valid_loader is not None:\n",
    "            (_, _, _, val_f1,\n",
    "             avg_val_loss, val_thr) = validation(\n",
    "                 model, criterion, valid_loader,\n",
    "                 epoch, device, split_name='validation',\n",
    "                 threshold=None)          # sweep automatically\n",
    "\n",
    "            val_losses.append(avg_val_loss)\n",
    "            thr_history.append(val_thr)\n",
    "            scheduler.step(val_f1)\n",
    "\n",
    "            # checkpoint every epoch (optional)\n",
    "            if models_dir:\n",
    "                torch.save(model.state_dict(),\n",
    "                           Path(models_dir) / f\"checkpoint_epoch_{epoch}.pt\")\n",
    "\n",
    "            # keep best epoch by F1\n",
    "            if val_f1 > best_valid_f1:\n",
    "                best_valid_f1  = val_f1\n",
    "                best_threshold = val_thr\n",
    "                best_weights   = deepcopy(model.state_dict())\n",
    "\n",
    "    print(f\"Best validation F1: {best_valid_f1:.3f}  (thr={best_threshold:.3f})\")\n",
    "    return (train_losses, val_losses,\n",
    "            best_valid_f1, best_weights, thr_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481940bc",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226754b",
   "metadata": {},
   "source": [
    "## Download data from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b8cde88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192698c699154faf8afdc07fc5ec9c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download models' weights & text/image datasets\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ID = \"INDEEPA/clip-siamese\"\n",
    "LOCAL_DIR = Path(\"data/train_results\")\n",
    "LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type='dataset',\n",
    "    local_dir='data',\n",
    "    allow_patterns=[\n",
    "        \"train_results/cc12m*.pt\",\n",
    "        TABLE_DATASET_FILE,\n",
    "        f\"{IMG_DATASET_NAME}.zip\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "!unzip -n -q data/{IMG_DATASET_NAME}.zip -d data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d35d53",
   "metadata": {},
   "source": [
    "## Split data by query sku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "387fca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique query sku: 20\n"
     ]
    }
   ],
   "source": [
    "TABLE_DATASET_PATH = DATA_PATH + TABLE_DATASET_FILE\n",
    "\n",
    "labeled = pd.read_csv(TABLE_DATASET_PATH)\n",
    "print(f\"Unique query sku: {labeled.sku_query.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c414d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/val/test\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "def split_pairwise(\n",
    "    df: pd.DataFrame,\n",
    "    test_size: float = 0.20,\n",
    "    random_state: int | None = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Leakage-free DEV / TEST split for a pair-wise SKU dataset.\n",
    "    Input `df` must have columns ['sku_query','sku_candidate','label'].\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    test_rows, dev_rows   = [], []\n",
    "    test_entities: set[str] = set()\n",
    "\n",
    "    # ---- iterate over each original query SKU ------------------------------\n",
    "    for q_sku, grp in df.groupby(\"sku_query\"):\n",
    "        pos_idx = grp.index[grp.label == 1].tolist()\n",
    "        neg_idx = grp.index[grp.label == 0].tolist()\n",
    "\n",
    "        # --- (1) sample TEST rows -------------------------------------------\n",
    "        n_pos = int(np.ceil(test_size * len(pos_idx))) if pos_idx else 0\n",
    "        n_neg = int(np.ceil(test_size * len(neg_idx))) if neg_idx else 0\n",
    "\n",
    "        pos_test = rng.choice(pos_idx, size=n_pos, replace=False) if n_pos else []\n",
    "        neg_test = rng.choice(neg_idx, size=n_neg, replace=False) if n_neg else []\n",
    "\n",
    "        test_rows.extend(pos_test)\n",
    "        test_rows.extend(neg_test)\n",
    "\n",
    "        # register every entity that just entered TEST\n",
    "        test_entities.add(q_sku)\n",
    "        test_entities.update(df.loc[pos_test, \"sku_candidate\"])\n",
    "        test_entities.update(df.loc[neg_test, \"sku_candidate\"])\n",
    "\n",
    "        # --- (2) build DEV from remaining rows ------------------------------\n",
    "        remain_pos = list(set(pos_idx) - set(pos_test))\n",
    "        remain_neg = list(set(neg_idx) - set(neg_test))\n",
    "\n",
    "        if remain_pos:\n",
    "            # choose substitute query (one of the remaining positives)\n",
    "            sub_idx  = int(rng.choice(remain_pos))\n",
    "            sub_sku  = df.loc[sub_idx, \"sku_candidate\"]\n",
    "\n",
    "            for idx in remain_pos:\n",
    "                if idx == sub_idx:            # skip (sub,sub) self-pair\n",
    "                    continue\n",
    "                row = df.loc[idx].copy()\n",
    "                row[\"sku_query\"] = sub_sku\n",
    "                dev_rows.append(row)\n",
    "\n",
    "            for idx in remain_neg:\n",
    "                row = df.loc[idx].copy()\n",
    "                row[\"sku_query\"] = sub_sku\n",
    "                dev_rows.append(row)\n",
    "\n",
    "    # ---- materialise the splits -------------------------------------------\n",
    "    test_df = df.loc[test_rows].reset_index(drop=True)\n",
    "    dev_df  = pd.DataFrame(dev_rows).reset_index(drop=True)\n",
    "\n",
    "    # ---- (3) final purge: remove any row touching a TEST entity ------------\n",
    "    mask = ~(dev_df[\"sku_query\"].isin(test_entities) |\n",
    "             dev_df[\"sku_candidate\"].isin(test_entities))\n",
    "    dev_df = dev_df[mask].reset_index(drop=True)\n",
    "\n",
    "    return dev_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e00ad1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 2510 175 644\n",
      "Unique sku per split: 13 15 20\n"
     ]
    }
   ],
   "source": [
    "# split into train/val/test\n",
    "\n",
    "dev_df,  test_df  = split_pairwise(labeled,  test_size=TEST_SPLIT, random_state=42)\n",
    "train_df, val_df  = split_pairwise(dev_df,   test_size=VALIDATION_SPLIT, random_state=42)\n",
    "\n",
    "print('Split sizes:', len(train_df), len(val_df), len(test_df))\n",
    "print('Unique sku per split:', train_df.sku_query.nunique(), val_df.sku_query.nunique(), test_df.sku_query.nunique(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "efc1fa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       hard_negative  positive  total\n",
      "split                                \n",
      "train           2156       354   2510\n",
      "val              146        29    175\n",
      "test             551        93    644\n"
     ]
    }
   ],
   "source": [
    "# Print positive/hard_negative pairs count per each split \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# collect your splits in a dict\n",
    "splits = {\n",
    "    'train': train_df,\n",
    "    'val':   val_df,\n",
    "    'test':  test_df,\n",
    "}\n",
    "\n",
    "# build the summary_df records\n",
    "records = []\n",
    "for name, df in splits.items():\n",
    "    vc = df['label'].value_counts()\n",
    "    records.append({\n",
    "        'split':    name,\n",
    "        'hard_negative': vc.get(0, 0),\n",
    "        'positive': vc.get(1, 0),\n",
    "        'total':    len(df),\n",
    "    })\n",
    "\n",
    "# create a DataFrame and set the split name as index\n",
    "summary_df = (\n",
    "    pd.DataFrame(records)\n",
    "      .set_index('split')\n",
    "      .astype(int)\n",
    ")\n",
    "\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1bed0574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All sanity checks passed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def sanity_checks(train: pd.DataFrame, val: pd.DataFrame, test: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Verify that:\n",
    "      1) Each query SKU appears in exactly one split\n",
    "      2) No SKU (query or candidate) overlaps across splits\n",
    "      3) No duplicate pairs across splits\n",
    "      4) Each split has at least one positive and one hard_negative\n",
    "    \"\"\"\n",
    "    # 1) Query-level disjointness\n",
    "    q_train = set(train[\"sku_query\"])\n",
    "    q_val   = set(val  [\"sku_query\"])\n",
    "    q_test  = set(test [\"sku_query\"])\n",
    "    assert not (q_train & q_val),   f\"Query SKU overlap train↔val: {q_train & q_val}\"\n",
    "    assert not (q_train & q_test),  f\"Query SKU overlap train↔test: {q_train & q_test}\"\n",
    "    assert not (q_val   & q_test),  f\"Query SKU overlap val↔test:   {q_val   & q_test}\"\n",
    "    \n",
    "    # 2) Global SKU disjointness (query OR candidate)\n",
    "    def all_skus(df):\n",
    "        return set(df[\"sku_query\"]) | set(df[\"sku_candidate\"])\n",
    "    s_train, s_val, s_test = all_skus(train), all_skus(val), all_skus(test)\n",
    "    assert not (s_train & s_val),   f\"SKU overlap train↔val: {s_train & s_val}\"\n",
    "    assert not (s_train & s_test),  f\"SKU overlap train↔test: {s_train & s_test}\"\n",
    "    assert not (s_val   & s_test),  f\"SKU overlap val↔test:   {s_val   & s_test}\"\n",
    "    \n",
    "    # 3) Unique pairs\n",
    "    all_pairs = pd.concat([train, val, test], ignore_index=True)\n",
    "    dupes = all_pairs.duplicated(subset=[\"sku_query\",\"sku_candidate\",\"label\"])\n",
    "    assert not dupes.any(), f\"Found {dupes.sum()} duplicate pairs across splits\"\n",
    "    \n",
    "    # 4) Label coverage in each split\n",
    "    for name, df in [(\"train\", train), (\"val\", val), (\"test\", test)]:\n",
    "        labels = df[\"label\"].unique()\n",
    "        assert set(labels) == {0,1}, f\"{name} split has labels {labels}, expected {{0,1}}\"\n",
    "    \n",
    "    print(\"✅ All sanity checks passed!\")\n",
    "\n",
    "sanity_checks(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b096d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename for compatibility with torch.Dataset\n",
    "\n",
    "def rename_cols(df):\n",
    "    df = df.rename(columns={\n",
    "        col: col.replace(\"_query\", \"_first\").replace(\"_candidate\", \"_second\")\n",
    "        for col in df.columns\n",
    "        if \"_query\" in col or \"_candidate\" in col\n",
    "    })\n",
    "    return df\n",
    "\n",
    "train_df = rename_cols(train_df)\n",
    "val_df = rename_cols(val_df)\n",
    "test_df = rename_cols(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0626ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for later usage\n",
    "\n",
    "folder_name = f'test={TEST_SPLIT}_val={VALIDATION_SPLIT}'\n",
    "dataset_name = Path(TABLE_DATASET_PATH).parts[1]\n",
    "\n",
    "common_file_prefix = (\n",
    "    Path(DATA_PATH) / dataset_name / 'processed' /\n",
    "    'pairwise-splits' / folder_name\n",
    ")\n",
    "common_file_prefix.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_df.to_csv(common_file_prefix / 'train.csv', index=False)\n",
    "val_df.to_csv(common_file_prefix / 'val.csv', index=False)\n",
    "test_df.to_csv(common_file_prefix / 'test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4aa24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take samples for each split to debug on smaller subset if necessary\n",
    "\n",
    "def sample_split(df: pd.DataFrame, num_samples: int | None, random_state: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    If num_samples is set, take up to that many random rows;\n",
    "    otherwise just shuffle the entire DataFrame.\n",
    "    Always resets the index.\n",
    "    \"\"\"\n",
    "    if num_samples is not None:\n",
    "        n = min(num_samples, len(df))\n",
    "        out = df.sample(n=n, random_state=random_state)\n",
    "    else:\n",
    "        out = df.sample(frac=1, random_state=random_state)\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "actual_train_df = sample_split(train_df, NUM_DEBUG_SAMPLES, RANDOM_SEED)\n",
    "actual_val_df   = sample_split(val_df,   NUM_DEBUG_SAMPLES, RANDOM_SEED)\n",
    "actual_test_df  = sample_split(test_df,  NUM_DEBUG_SAMPLES, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1d87f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: very important; set inverse target for siamese net:\n",
    "# label=0 (distance is minimal) -> new_label=1 (pair is similar)\n",
    "# label=1 (distance is maximal) -> new_label=0 (pair is dissimilar)\n",
    "\n",
    "actual_train_df[\"label\"] = 1 - actual_train_df[\"label\"]\n",
    "actual_val_df[\"label\"]   = 1 - actual_val_df[\"label\"]\n",
    "actual_test_df[\"label\"]  = 1 - actual_test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17b84c",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "96a54d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score                 # ← new\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def best_threshold(distances: torch.Tensor,\n",
    "                   labels:    torch.Tensor,\n",
    "                   steps:     int = 200,\n",
    "                   margin:    float = 1.5):\n",
    "    \"\"\"\n",
    "    Sweep `steps` evenly-spaced thresholds between 0 and `margin`\n",
    "    and return the one that maximises duplicate-class F1.\n",
    "    Labels: 0 = duplicate (positive), 1 = different (negative).\n",
    "    \"\"\"\n",
    "    d   = distances.detach().cpu().numpy()\n",
    "    y   = labels.detach().cpu().numpy()\n",
    "    thr = np.linspace(0.0, margin, steps)\n",
    "\n",
    "    best_f1, best_thr = -1.0, 0.0\n",
    "    for t in thr:\n",
    "        y_pred = (d < t).astype(int)          # 1 = duplicate prediction\n",
    "        f1     = f1_score(1 - y, y_pred)      # make 1 = positive for sklearn\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, t\n",
    "    return best_thr, best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6cb34367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation: 100%|██████████| 2/2 [00:00<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[validation] Epoch best – loss: 0.0000, P Acc: 0.000, N Acc: 1.000, Avg Acc: 0.500, F1: 0.000, thr*: 0.000\n",
      "Chosen threshold from validation: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 2/2 [00:00<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] Epoch test – loss: 0.0000, P Acc: 0.000, N Acc: 1.000, Avg Acc: 0.500, F1: 0.000, thr*: 0.000\n",
      "Test F1-score on best model: 0.000\n",
      "Saved best‐F1 checkpoint to data/train_results/siamese_contrastive_test-f1=0.000_splitting-by-query_cc12m_rubert_tiny_ep_1.pt_best-threshold=0.0.pt\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "def _run():\n",
    "    images_dir = os.path.join(DATA_PATH, IMG_DATASET_NAME)\n",
    "\n",
    "    # ---------- 1) build DataLoaders ----------\n",
    "    splits  = {'train': actual_train_df,\n",
    "               'validation': actual_val_df,\n",
    "               'test': actual_test_df}\n",
    "    loaders = {}\n",
    "\n",
    "    for split_name, df in splits.items():\n",
    "        labels = df[\"label\"].values\n",
    "        ds     = SiameseRuCLIPDataset(df.drop(columns=\"label\"),\n",
    "                                      labels,\n",
    "                                      images_dir=images_dir)\n",
    "\n",
    "        if split_name == \"train\":                 # balanced batches\n",
    "            cls_cnt        = np.bincount(labels, minlength=2)\n",
    "            cls_weights    = 1.0 / cls_cnt\n",
    "            sample_weights = cls_weights[labels]\n",
    "            sampler        = WeightedRandomSampler(sample_weights,\n",
    "                                                  num_samples=len(sample_weights),\n",
    "                                                  replacement=True)\n",
    "            loaders[split_name] = DataLoader(\n",
    "                    ds, batch_size=BATCH_SIZE, sampler=sampler,\n",
    "                    num_workers=NUM_WORKERS)\n",
    "        else:                                     # deterministic\n",
    "            loaders[split_name] = DataLoader(\n",
    "                    ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                    num_workers=NUM_WORKERS)\n",
    "\n",
    "    train_loader = loaders['train']\n",
    "    valid_loader = loaders['validation']\n",
    "    test_loader  = loaders['test']\n",
    "\n",
    "    # ---------- 2) model / optimiser ----------\n",
    "    print(\"Loading model and optimizer…\")\n",
    "    model = SiameseRuCLIP(\n",
    "        DEVICE, NAME_MODEL_NAME,\n",
    "        DESCRIPTION_MODEL_NAME,\n",
    "        PRELOAD_MODEL_NAME,\n",
    "        DATA_PATH + RESULTS_DIR,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    criterion = ContrastiveLoss(\n",
    "        margin=CONTRASTIVE_MARGIN,\n",
    "        pos_weight=POS_WEIGHT\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # ---------- 3) training ----------\n",
    "    with tempfile.TemporaryDirectory() as tmp_ckpt_dir:\n",
    "        (train_losses, val_losses,\n",
    "         best_f1, best_weights, _) = train(   # new 5-value return\n",
    "            model, optimizer, criterion,\n",
    "            EPOCHS, train_loader, valid_loader,\n",
    "            print_epoch=True, device=DEVICE,\n",
    "            models_dir=tmp_ckpt_dir\n",
    "        )\n",
    "\n",
    "    print(f\"→ Best validation F1: {best_f1:.3f}\")\n",
    "\n",
    "    # ---------- 4) loss curves ----------\n",
    "    epochs_ax = list(range(1, len(train_losses) + 1))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs_ax, train_losses, label='Train Loss')\n",
    "    ax.plot(epochs_ax, val_losses,   label='Val   Loss')\n",
    "    ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training & Validation Loss by Epoch')\n",
    "    ax.legend()\n",
    "    if MLFLOW_URI:\n",
    "        mlflow.log_figure(fig, 'loss_by_epoch.png')\n",
    "    display.clear_output(wait=True); display.display(fig); plt.close(fig)\n",
    "\n",
    "    # ---------- 5) pick threshold for the *best* model ----------\n",
    "    model.load_state_dict(best_weights)\n",
    "    ( _, _, _, _, _, best_thr) = validation(   # sweep again to get thr\n",
    "        model, criterion, valid_loader,\n",
    "        epoch='best', device=DEVICE,\n",
    "        split_name='validation', threshold=None)\n",
    "\n",
    "    print(f\"Chosen threshold from validation: {best_thr:.3f}\")\n",
    "\n",
    "    # ---------- 6) final TEST ----------\n",
    "    (test_pos_acc, test_neg_acc,\n",
    "     test_acc, test_f1,\n",
    "     test_loss, _) = validation(\n",
    "        model, criterion, test_loader,\n",
    "        epoch='test', device=DEVICE,\n",
    "        split_name='test', threshold=best_thr)\n",
    "\n",
    "    print(f\"Test F1-score on best model: {test_f1:.3f}\")\n",
    "\n",
    "    # ---------- 7) save checkpoint ----------\n",
    "    filename = (\n",
    "        f\"siamese_contrastive_test-f1={test_f1:.3f}\"\n",
    "        f\"{'_' + MODEL_NAME_POSTFIX if MODEL_NAME_POSTFIX else ''}\"\n",
    "        f\"{'_' + PRELOAD_MODEL_NAME  if PRELOAD_MODEL_NAME else ''}\"\n",
    "        f'_best-threshold={best_thr}'\n",
    "        \".pt\"\n",
    "    )\n",
    "    final_path = Path(DATA_PATH + RESULTS_DIR) / filename\n",
    "    final_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(best_weights, final_path)\n",
    "    print(f\"Saved best‐F1 checkpoint to {final_path}\")\n",
    "\n",
    "    if MLFLOW_URI:\n",
    "        mlflow.log_metric(\"test_pos_accuracy\", test_pos_acc)\n",
    "        mlflow.log_metric(\"test_neg_accuracy\", test_neg_acc)\n",
    "        mlflow.log_metric(\"test_accuracy\",     test_acc)\n",
    "        mlflow.log_metric(\"test_f1_score\",     test_f1)\n",
    "        mlflow.end_run()\n",
    "\n",
    "_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70470186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

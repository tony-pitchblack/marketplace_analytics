{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d9b411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "be91836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "## CHOOSE MODEL PARAMETERS #################################################\n",
    "\n",
    "HIDDEN_DIM = 3*768\n",
    "DATA_PATH = 'data/'\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NAME_MODEL_NAME = 'cointegrated/rubert-tiny' # 'DeepPavlov/distilrubert-tiny-cased-conversational-v1'\n",
    "DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny'\n",
    "\n",
    "# BATCH_SIZE=90\n",
    "# NUM_WORKERS=8\n",
    "# NUM_DEBUG_SAMPLES=None\n",
    "# EPOCHS=20\n",
    "\n",
    "BATCH_SIZE=1\n",
    "NUM_WORKERS=0\n",
    "NUM_DEBUG_SAMPLES=4 # minimum 4 samples (2 per each of train/val) for stratified split\n",
    "EPOCHS=1\n",
    "\n",
    "EMB_SIZE=768\n",
    "VALIDATION_SPLIT=.25\n",
    "SHUFFLE_DATASET=True\n",
    "RANDOM_SEED=42\n",
    "LR=9e-5\n",
    "MOMENTUM=0.9\n",
    "WEIGHT_DECAY=1e-2\n",
    "CONTRASTIVE_MARGIN=1.5\n",
    "CONTRASTIVE_THRESHOLD=0.3\n",
    "SHEDULER_PATIENCE=3 # in epochs\n",
    "SMTH='1gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f3d9e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHOOSE DATA #########################################################\n",
    "\n",
    "# # These table files need 'image_name_first', 'image_name_second' constructed from sku to be usable in current pipeline\n",
    "# TABLE_DATASET_FILE = 'tables_labeled/processed/labeled_1.3k_with-options.csv'\n",
    "# TABLE_DATASET_FILE = 'tables_labeled/processed/labeled_56k_with-options.csv'\n",
    "# IMG_DATASET_NAME = 'images_7k'\n",
    "# STRATIFY_COLS = None\n",
    "\n",
    "# TABLE_DATASET_FILE = 'tables_labeled/processed/labeled_5k_with-options.csv'\n",
    "# IMG_DATASET_NAME = 'images_7k' \n",
    "# STRATIFY_COLS = None\n",
    "\n",
    "# TABLE_DATASET_FILE = 'tables_WB_OZ_100/WB_OZ_100.csv'\n",
    "# TABLE_DATASET_FILE = 'tables_WB_OZ_100/WB_OZ_100_conjugated.csv'\n",
    "# TABLE_DATASET_FILE = 'tables_WB_OZ_100/WB_OZ_100_conjugated_shuffled_seed=42_fraction=1.csv'\n",
    "# TABLE_DATASET_FILE = 'tables_WB_OZ_100/WB_OZ_100_conjugated_shuffled_seed=42_fraction=0.5.csv'\n",
    "# IMG_DATASET_NAME = 'images_WB_OZ_100'\n",
    "# STRATIFY_COLS = None\n",
    "\n",
    "TABLE_DATASET_FILE = 'tables_OZ_geo_5500/processed/regex-pairwise-dataset_num-queries=20_num-pairs=6226_patterns-dict-hash=6dbf9b3ef9568e60cd959f87be7e3b26.csv'\n",
    "IMG_DATASET_NAME = 'images_OZ_geo_5500'\n",
    "STRATIFY_COLS = ['sku_first', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOGGING PARAMS ######################################################################\n",
    "\n",
    "# MLFLOW_URI = \"http://176.56.185.96:5000\"\n",
    "MLFLOW_URI = ''\n",
    "\n",
    "MLFLOW_EXPERIMENT = \"siamese/1fold\"\n",
    "\n",
    "TELEGRAM_TOKEN = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00017085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "from timm import create_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "# import transformers\n",
    "# from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer,\\\n",
    "#         get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# import json\n",
    "# from itertools import product\n",
    "\n",
    "# import datasets\n",
    "# from datasets import Dataset, concatenate_datasets\n",
    "# import argparse\n",
    "import requests\n",
    "\n",
    "# from io import BytesIO\n",
    "# from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "# import more_itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3494173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tg_report(text, token) -> None:\n",
    "    method = 'sendMessage'\n",
    "    chat_id = 324956476\n",
    "    _ = requests.post(\n",
    "            url='https://api.telegram.org/bot{0}/{1}'.format(token, method),\n",
    "            data={'chat_id': chat_id, 'text': text} \n",
    "        ).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84518fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuCLIPtiny(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.visual = create_model('convnext_tiny',\n",
    "                                   pretrained=False, # TODO: берём претрейн\n",
    "                                   num_classes=0,\n",
    "                                   in_chans=3)  # out 768\n",
    "\n",
    "        self.transformer = AutoModel.from_pretrained(NAME_MODEL_NAME)\n",
    "        name_model_output_shape = self.transformer.config.hidden_size  # dynamically get hidden size\n",
    "        self.final_ln = torch.nn.Linear(name_model_output_shape, 768)  # now uses the transformer hidden size\n",
    "        self.logit_scale = torch.nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.stem[0].weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        x = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = x.last_hidden_state[:, 0, :]\n",
    "        x = self.final_ln(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(input_ids, attention_mask)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text\n",
    "    \n",
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        _convert_image_to_rgb,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]), ])\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "class Tokenizers:\n",
    "    def __init__(self):\n",
    "        self.name_tokenizer = AutoTokenizer.from_pretrained(NAME_MODEL_NAME)\n",
    "        self.desc_tokenizer = AutoTokenizer.from_pretrained(DESCRIPTION_MODEL_NAME)\n",
    "\n",
    "    def tokenize_name(self, texts, max_len=77):\n",
    "        tokenized = self.name_tokenizer.batch_encode_plus(texts,\n",
    "                                                     truncation=True,\n",
    "                                                     add_special_tokens=True,\n",
    "                                                     max_length=max_len,\n",
    "                                                     padding='max_length',\n",
    "                                                     return_attention_mask=True,\n",
    "                                                     return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "    \n",
    "    def tokenize_description(self, texts, max_len=77):\n",
    "        tokenized = self.desc_tokenizer(texts,\n",
    "                                        truncation=True,\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=max_len,\n",
    "                                        padding='max_length',\n",
    "                                        return_attention_mask=True,\n",
    "                                        return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2d263be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseRuCLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df=None, labels=None, df_path=None, images_dir=DATA_PATH+'images/'):\n",
    "        # loads data either from path using `df_path` or directly from `df` argument\n",
    "        self.df = pd.read_csv(df_path) if df_path is not None else df\n",
    "        self.labels = labels\n",
    "        self.images_dir = images_dir\n",
    "        self.tokenizers = Tokenizers()\n",
    "        self.transform = get_transform()\n",
    "        # \n",
    "        self.max_len = 77\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        name_tokens = self.tokenizers.tokenize_name([str(row.name_first), \n",
    "                                               str(row.name_second)], max_len=self.max_len)\n",
    "        name_first = name_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        name_second = name_tokens[:, 1, :]\n",
    "        desc_tokens = self.tokenizers.tokenize_description([str(row.description_first), \n",
    "                                               str(row.description_second)])\n",
    "        desc_first = desc_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        desc_second = desc_tokens[:, 1, :]\n",
    "        im_first = cv2.imread(os.path.join(self.images_dir, row.image_name_first))\n",
    "        im_first = cv2.cvtColor(im_first, cv2.COLOR_BGR2RGB)\n",
    "        im_first = Image.fromarray(im_first)\n",
    "        im_first = self.transform(im_first)\n",
    "        im_second = cv2.imread(os.path.join(self.images_dir, row.image_name_second))\n",
    "        im_second = cv2.cvtColor(im_second, cv2.COLOR_BGR2RGB)\n",
    "        im_second = Image.fromarray(im_second)\n",
    "        im_second = self.transform(im_second)\n",
    "        label = self.labels[idx]\n",
    "        return im_first, name_first, desc_first, im_second, name_second, desc_second, label\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.df)\n",
    "    \n",
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        last_hidden = last_hidden_states.masked_fill(\n",
    "            ~attention_mask[..., None].bool(), 0.0\n",
    "        )\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "class SiameseRuCLIP(nn.Module):\n",
    "    def __init__(self, preload_ruclip=True, device='cpu', hidden_dim=HIDDEN_DIM, models_dir=DATA_PATH + 'train_results/'):\n",
    "        super().__init__()\n",
    "        self.ruclip = RuCLIPtiny()\n",
    "        if preload_ruclip:\n",
    "            preload_model_name = 'cc12m_rubert_tiny_ep_1.pt' #'cc12m_ddp_4mill_ep_4.pt'\n",
    "            std = torch.load(models_dir + preload_model_name, weights_only=True, map_location=device)\n",
    "            self.ruclip.load_state_dict(std)\n",
    "            self.ruclip = self.ruclip.to(device)\n",
    "            self.ruclip.eval()\n",
    "        self.description_transformer = AutoModel.from_pretrained(DESCRIPTION_MODEL_NAME)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            # # nn.BatchNorm1d(hidden_dim),\n",
    "            # nn.Dropout(0.3), \n",
    "            # nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            # nn.ReLU(), \n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "        )\n",
    "        \n",
    "    def encode_description(self, desc):\n",
    "        # desc is [input_ids, attention_mask]\n",
    "        last_hidden_states = self.description_transformer(desc[:, 0, :], desc[:, 1, :]).last_hidden_state\n",
    "        attention_mask = desc[:, 1, :]\n",
    "        # TODO: нужно ли делать пулинг, посмотреть на результаты\n",
    "        return average_pool(last_hidden_states, attention_mask)\n",
    "    \n",
    "    def forward(self, im1, name1, desc1, im2, name2, desc2):\n",
    "        image_emb1 = self.ruclip.encode_image(im1)\n",
    "        image_emb2 = self.ruclip.encode_image(im2)\n",
    "        name_emb1 = self.ruclip.encode_text(name1[:, 0, :], name1[:, 1, :])\n",
    "        name_emb2 = self.ruclip.encode_text(name2[:, 0, :], name2[:, 1, :])\n",
    "        desc_emb1 = self.ruclip.encode_text(desc1[:, 0, :], desc1[:, 1, :])\n",
    "        desc_emb2 = self.ruclip.encode_text(desc2[:, 0, :], desc2[:, 1, :])\n",
    "        # desc_emb1 = self.encode_description(desc1) \n",
    "        # desc_emb2 = self.encode_description(desc2)\n",
    "        first_emb = torch.cat([image_emb1, name_emb1, desc_emb1], dim=1)\n",
    "        second_emb = torch.cat([image_emb2, name_emb2, desc_emb2], dim=1)\n",
    "        out1 = self.head(first_emb)\n",
    "        out2 = self.head(second_emb)\n",
    "        return out1, out2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "987d1ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def __name__(self,):\n",
    "        return 'ContrastiveLoss'\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        pos = (1-label) * torch.pow(euclidean_distance, 2)\n",
    "        neg = label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        loss_contrastive = torch.mean( pos + neg )\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4952b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, \n",
    "          epochs_num, train_loader, valid_loader=None, \n",
    "          score=f1_score, device='cpu', print_epoch=False) -> None:\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    counter = []\n",
    "    loss_history = [] \n",
    "    it_number = 0\n",
    "    best_valid_score = 0\n",
    "    best_weights = None\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"max\",\n",
    "                                            factor=0.1, patience=SHEDULER_PATIENCE,\n",
    "                                            threshold=0.0001,\n",
    "                                            threshold_mode='rel', cooldown=0,\n",
    "                                            min_lr=0, eps=1e-08)\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs_num):\n",
    "        print(\"Epoch：\", epoch)\n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            im1, name1, desc1, im2, name2, desc2, label = data \n",
    "            im1, name1, desc1, im2, name2, desc2, label = im1.to(device), name1.to(device), desc1.to(device), im2.to(device), name2.to(device), desc2.to(device), label.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            out1, out2 = model(im1, name1, desc1, im2, name2, desc2)\n",
    "            loss = criterion(out1, out2, label)\n",
    "            if MLFLOW_URI:\n",
    "                mlflow.log_metric(\"train_loss\", f\"{loss:2f}\", step=epoch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 1 == 0: # show changes of loss value after each 10 batches\n",
    "                # it_number += 5\n",
    "                counter.append(it_number)\n",
    "                loss_history.append(loss.item())\n",
    "        # test after each epoch\n",
    "        if print_epoch:\n",
    "            valid_score = validation(model, criterion, valid_loader, epoch, device)\n",
    "            if MLFLOW_URI:\n",
    "                mlflow.log_metric(\"valid_accuracy\", f\"{valid_score:2f}\", step=epoch)\n",
    "            plot_epoch(loss_history)\n",
    "            print(f'Current train loss: {loss}')\n",
    "            # print(f'Current {score.__name__}: {valid_score}')\n",
    "            lr_to_log = optimizer.param_groups[0]['lr']\n",
    "            if MLFLOW_URI:\n",
    "                mlflow.log_metric(\"lr\", f\"{lr_to_log:2f}\", step=epoch)\n",
    "            scheduler.step(valid_score)\n",
    "            if valid_score > best_valid_score:\n",
    "                best_valid_score = valid_score\n",
    "                best_weights = model.state_dict()\n",
    "    return best_valid_score, best_weights\n",
    "\n",
    "def validation(model, criterion, valid_loader, epoch, device='cpu') -> float:\n",
    "    valid_loss = 0\n",
    "    val_pos_accuracy = 0\n",
    "    val_neg_accuracy = 0\n",
    "    num_pos = 0\n",
    "    num_neg = 0\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for data in tqdm(valid_loader):\n",
    "            im1, name1, desc1, im2, name2, desc2, label = data \n",
    "            im1, name1, desc1, im2, name2, desc2, label = (\n",
    "                im1.to(device), name1.to(device), desc1.to(device),\n",
    "                im2.to(device), name2.to(device), desc2.to(device), label.to(device)\n",
    "            )\n",
    "            out1, out2 = model(im1, name1, desc1, im2, name2, desc2) \n",
    "            loss = criterion(out1, out2, label)\n",
    "            pos_acc, pos_sum, neg_acc, neg_sum = evaluate_pair(out1, out2, label, CONTRASTIVE_THRESHOLD)\n",
    "            val_pos_accuracy += pos_acc\n",
    "            val_neg_accuracy += neg_acc\n",
    "            num_pos += pos_sum\n",
    "            num_neg += neg_sum\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    val_pos_accuracy = val_pos_accuracy / num_pos if num_pos > 0 else 0.0\n",
    "    val_neg_accuracy = val_neg_accuracy / num_neg if num_neg > 0 else 0.0\n",
    "    valid_loss = valid_loss / len(valid_loader) if len(valid_loader) > 0 else 0.0\n",
    "\n",
    "    report = (\n",
    "        f\"Epoch: {epoch}, Validation loss: {valid_loss:.3f}, \"\n",
    "        f\"P Acc: {val_pos_accuracy:.3f}, N Acc: {val_neg_accuracy:.3f} \" + SMTH + '\\n'\n",
    "    )\n",
    "    print(report)\n",
    "    make_tg_report(report, TELEGRAM_TOKEN)\n",
    "    \n",
    "    return (val_pos_accuracy + val_neg_accuracy) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e34b5c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def plot_epoch(loss_history, filename=\"data/runs_artifacts/epoch_loss.png\") -> None:\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Iteration number\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(loss_history, 'b')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)  # Save the plot to a file\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a9c6281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pair(output1, output2, target, threshold):\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    # меньше границы, там где будет True — конкуренты\n",
    "    cond = euclidean_distance < threshold\n",
    "    pos_sum = 0\n",
    "    neg_sum = 0\n",
    "    pos_acc = 0\n",
    "    neg_acc = 0\n",
    "\n",
    "    for i in range(len(cond)):\n",
    "        # 1 значит не конкуренты\n",
    "        if target[i]:\n",
    "            neg_sum+=1\n",
    "            # 0 в cond значит дальше друг от друга чем threshold\n",
    "            if not cond[i]:\n",
    "                neg_acc+=1\n",
    "        elif not target[i]:\n",
    "            pos_sum+=1\n",
    "            if cond[i]:\n",
    "                pos_acc+=1\n",
    "\n",
    "    return pos_acc, pos_sum, neg_acc, neg_sum\n",
    "\n",
    "def predict(out1, out2, threshold=CONTRASTIVE_THRESHOLD):\n",
    "    # вернёт 1 если похожи\n",
    "    return F.pairwise_distance(out1, out2) < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e285db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = DATA_PATH + IMG_DATASET_NAME\n",
    "labeled = pd.read_csv(DATA_PATH + TABLE_DATASET_FILE)\n",
    "\n",
    "# 1) Sample (or shuffle) and reset index\n",
    "if NUM_DEBUG_SAMPLES is not None:\n",
    "    n = min(NUM_DEBUG_SAMPLES, len(labeled))\n",
    "    labeled = labeled.sample(n=n, random_state=RANDOM_SEED)\n",
    "else:\n",
    "    labeled = labeled.sample(frac=1, random_state=RANDOM_SEED)\n",
    "\n",
    "labeled = labeled.reset_index(drop=True)\n",
    "\n",
    "# 2) Stratified (or plain) split on the sampled DataFrame\n",
    "if STRATIFY_COLS:\n",
    "    # build composite key\n",
    "    stratify = labeled[STRATIFY_COLS].astype(str).agg('_'.join, axis=1)\n",
    "\n",
    "    # find any strata of size 1 and force them into train\n",
    "    vc = stratify.value_counts()\n",
    "    single = vc[vc == 1].index\n",
    "    mask_single = stratify.isin(single)\n",
    "\n",
    "    # split only the “common” groups\n",
    "    train_common, valid_index = train_test_split(\n",
    "        labeled.index[~mask_single],\n",
    "        test_size=VALIDATION_SPLIT,\n",
    "        stratify=stratify[~mask_single],\n",
    "        random_state=RANDOM_SEED,\n",
    "        shuffle=True\n",
    "    )\n",
    "    # union back the singletons\n",
    "    train_index = labeled.index[mask_single].union(train_common)\n",
    "else:\n",
    "    train_index, valid_index = train_test_split(\n",
    "        labeled.index,\n",
    "        test_size=VALIDATION_SPLIT,\n",
    "        random_state=RANDOM_SEED,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "def _run():\n",
    "    X, y = labeled.drop(columns='label'), labeled.label.values\n",
    "    y = 1 - y \n",
    "    best_valid = 0\n",
    "\n",
    "    print('Loading model and data...', end=' ')\n",
    "    train_dataset = SiameseRuCLIPDataset(X.iloc[train_index], y[train_index], images_dir=images_dir)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    valid_dataset = SiameseRuCLIPDataset(X.iloc[valid_index], y[valid_index], images_dir=images_dir)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    model = SiameseRuCLIP(device=DEVICE)\n",
    "    criterion = ContrastiveLoss(margin=CONTRASTIVE_MARGIN)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    print('done.')\n",
    "\n",
    "    if MLFLOW_URI:\n",
    "        # mlflow pipeline\n",
    "        params = {\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"learning_rate\": LR,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            'weight_decay': WEIGHT_DECAY, \n",
    "            'num_workers': NUM_WORKERS,\n",
    "            \"loss_function\": criterion.__class__.__name__,\n",
    "            'scheduler': ReduceLROnPlateau.__class__.__name__,\n",
    "            'patience': SHEDULER_PATIENCE, \n",
    "            \"optimizer\": optimizer.__class__.__name__,\n",
    "        }\n",
    "\n",
    "        mlflow.log_params(params)\n",
    "        with open(\"siam_summary.txt\", \"w\") as f:\n",
    "            f.write(str(summary(model)))\n",
    "        mlflow.log_artifact(\"siam_summary.txt\")\n",
    "        ###########\n",
    "\n",
    "    best_valid_score, best_weights = train(model, optimizer, criterion,\n",
    "        EPOCHS, train_loader, valid_loader, \n",
    "        score=f1_score, print_epoch=True, device=DEVICE\n",
    "    )\n",
    "\n",
    "    if best_valid_score > best_valid:\n",
    "        best_valid = best_valid_score\n",
    "        torch.save(best_weights, f'siamese_contrastive_{SMTH}.pt')\n",
    "\n",
    "    # save model w/ signature\n",
    "    batch = next(iter(valid_loader))\n",
    "    im1_ex, name1_ex, desc1_ex, im2_ex, name2_ex, desc2_ex, _ = [\n",
    "        t.to(DEVICE).cpu().numpy() for t in batch\n",
    "    ]\n",
    "    out1_ex, out2_ex = model(\n",
    "        torch.from_numpy(im1_ex).to(DEVICE),\n",
    "        torch.from_numpy(name1_ex).to(DEVICE),\n",
    "        torch.from_numpy(desc1_ex).to(DEVICE),\n",
    "        torch.from_numpy(im2_ex).to(DEVICE),\n",
    "        torch.from_numpy(name2_ex).to(DEVICE),\n",
    "        torch.from_numpy(desc2_ex).to(DEVICE),\n",
    "    )\n",
    "    out1_ex = out1_ex.cpu().detach().numpy()\n",
    "    out2_ex = out2_ex.cpu().detach().numpy()\n",
    "\n",
    "    # Build dicts of NumPy arrays\n",
    "    signature_input  = {\n",
    "        \"im1\":  im1_ex,\n",
    "        \"name1\":name1_ex,\n",
    "        \"desc1\":desc1_ex,\n",
    "        \"im2\":  im2_ex,\n",
    "        \"name2\":name2_ex,\n",
    "        \"desc2\":desc2_ex,\n",
    "    }\n",
    "    signature_output = {\n",
    "        \"out1\": out1_ex,\n",
    "        \"out2\": out2_ex,\n",
    "    }\n",
    "\n",
    "    # Infer signature\n",
    "    signature = infer_signature(signature_input, signature_output)\n",
    "\n",
    "    # Log only with signature (no input_example)\n",
    "    if MLFLOW_URI:\n",
    "        mlflow.pytorch.log_model(\n",
    "            model,\n",
    "            artifact_path=\"model\",\n",
    "            signature=signature\n",
    "        )\n",
    "\n",
    "    if TELEGRAM_TOKEN:\n",
    "        make_tg_report(f'Лучший валид лосс: {best_valid:.3f}'+SMTH, TELEGRAM_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6511a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current train loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "if MLFLOW_URI:\n",
    "    mlflow.set_tracking_uri(uri=MLFLOW_URI)\n",
    "    mlflow.set_experiment(MLFLOW_EXPERIMENT)\n",
    "    mlflow.enable_system_metrics_logging()\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        _run()\n",
    "else:\n",
    "    _run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

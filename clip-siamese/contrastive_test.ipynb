{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "from timm import create_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import transformers\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer,\\\n",
    "        get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import json\n",
    "from itertools import product\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import argparse\n",
    "import requests\n",
    "\n",
    "from io import BytesIO\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants \n",
    "DATA_PATH = 'data/'\n",
    "HIDDEN_DIM = 2*768+312\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "TABLE_FILE_NAME = 'new_labeled_v4.csv'\n",
    "# TABLE_FILE_NAME = 'new_labeled_v5.csv'\n",
    "# TABLE_FILE_NAME = 'new_labeled'\n",
    "\n",
    "IMG_DIR_NAME = 'images7k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "labeled = pd.read_csv(DATA_PATH + TABLE_FILE_NAME)\n",
    "images_dir = DATA_PATH + IMG_DIR_NAME\n",
    "\n",
    "y = labeled.label.values\n",
    "X = labeled.drop(columns='label').copy()\n",
    "\n",
    "test_ds = SiameseRuCLIPDataset(X, y, images_dir=images_dir)\n",
    "test_ld = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labeled = pd.read_csv('data/'+'new_labeled_v4.csv') # new_labeled\n",
    "labeled = labeled.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "border = int(labeled.shape[0]*(1-.2))\n",
    "train_index, valid_index = labeled.iloc[:border].index, labeled.iloc[border:].index\n",
    "X, y = labeled.drop(columns='label'), labeled.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 1], shape=(44910,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/images_prelabeled'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/images_prelabeled\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/images_prelabeled'"
     ]
    }
   ],
   "source": [
    "os.listdir('data/images_prelabeled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/images_prelabeled'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m downloaded_images = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/images_prelabeled\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name.find(\u001b[33m'\u001b[39m\u001b[33mcopy\u001b[39m\u001b[33m'\u001b[39m) > -\u001b[32m1\u001b[39m:\n\u001b[32m      4\u001b[39m         os.remove(\u001b[33m'\u001b[39m\u001b[33mdata/images_prelabeled/\u001b[39m\u001b[33m'\u001b[39m+name)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/images_prelabeled'"
     ]
    }
   ],
   "source": [
    "downloaded_images = set()\n",
    "for name in os.listdir('data/images_prelabeled'):\n",
    "    if name.find('copy') > -1:\n",
    "        os.remove('data/images_prelabeled/'+name)\n",
    "        continue\n",
    "    dot_ind = name.find('.')\n",
    "    sku = int(name[:dot_ind])\n",
    "    downloaded_images.add(sku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_syntetic_data = pd.read_csv('new_syntetic_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntetic_data = pd.read_csv('new_syntetic_data.csv')\n",
    "new_labeled_v2 = pd.read_csv('data/new_labeled_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labeled_v3 = pd.concat([new_labeled_v2, syntetic_data]).dropna(ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labeled_v3.to_csv('data/new_labeled_v4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_inds = new_labeled_v3[new_labeled_v3.sku_first == 223703337].index\n",
    "new_labeled_v3 = new_labeled_v3[~new_labeled_v3.index.isin(problems_inds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://basket-15.wbbasket.ru/vol2237/part223703/223703337/images/big/1.webp'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parser = DataParser()\n",
    "data_parser.make_img_url(223703337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/images_7k/223703337.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/images_7k/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m223703337.jpg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/clip-siamese/lib/python3.13/site-packages/PIL/Image.py:3465\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3462\u001b[39m     filename = os.fspath(fp)\n\u001b[32m   3464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[32m-> \u001b[39m\u001b[32m3465\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3466\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3467\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/images_7k/223703337.jpg'"
     ]
    }
   ],
   "source": [
    "Image.open('data/images_7k/' + '223703337.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "223703337 in new_labeled_v3.sku_first.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/'\n",
    "HIDDEN_DIM = 2*768+312\n",
    "TRAIN_RESULTS_PATH = DATA_PATH\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NAME_MODEL_NAME = 'cointegrated/rubert-tiny' # 'DeepPavlov/distilrubert-tiny-cased-conversational-v1'\n",
    "DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny' # TODO sergeyzh/rubert-tiny-turbo deepvk/USER-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuCLIPtiny(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.visual = create_model('convnext_tiny',\n",
    "                                   pretrained=False, # TODO: берём претрейн\n",
    "                                   num_classes=0,\n",
    "                                   in_chans=3)  # out 768\n",
    "        # text_config = DistilBertConfig(**{\"vocab_size\": 30522,\n",
    "        #                                   \"max_position_embeddings\": 512,\n",
    "        #                                   \"n_layers\": 3,\n",
    "        #                                   \"n_heads\": 12,\n",
    "        #                                   \"dim\": 264,\n",
    "        #                                   \"hidden_dim\": 792,\n",
    "        #                                   \"model_type\": \"distilbert\"})\n",
    "        # self.transformer = DistilBertModel(text_config)\n",
    "        self.transformer = AutoModel.from_pretrained(NAME_MODEL_NAME) # 312\n",
    "        self.final_ln = torch.nn.Linear(312, 768) # 312 -> 768\n",
    "        self.logit_scale = torch.nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.stem[0].weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        x = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = x.last_hidden_state[:, 0, :]\n",
    "        x = self.final_ln(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(input_ids, attention_mask)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        _convert_image_to_rgb,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]), ])\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "class Tokenizers:\n",
    "    def __init__(self):\n",
    "        self.name_tokenizer = AutoTokenizer.from_pretrained(NAME_MODEL_NAME)\n",
    "        self.desc_tokenizer = AutoTokenizer.from_pretrained(DESCRIPTION_MODEL_NAME)\n",
    "\n",
    "    def tokenize_name(self, texts, max_len=77):\n",
    "        tokenized = self.name_tokenizer.batch_encode_plus(texts,\n",
    "                                                     truncation=True,\n",
    "                                                     add_special_tokens=True,\n",
    "                                                     max_length=max_len,\n",
    "                                                     padding='max_length',\n",
    "                                                     return_attention_mask=True,\n",
    "                                                     return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "    \n",
    "    def tokenize_description(self, texts, max_len=77):\n",
    "        tokenized = self.desc_tokenizer(texts,\n",
    "                                        truncation=True,\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=max_len,\n",
    "                                        padding='max_length',\n",
    "                                        return_attention_mask=True,\n",
    "                                        return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseRuCLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df=None, labels=None, df_path=None, images_dir=DATA_PATH+'images/'):\n",
    "        # loads data either from path using `df_path` or directly from `df` argument\n",
    "        self.df = pd.read_csv(df_path) if df_path is not None else df\n",
    "        self.labels = labels\n",
    "        self.images_dir = images_dir\n",
    "        self.tokenizers = Tokenizers()\n",
    "        self.transform = get_transform()\n",
    "        # \n",
    "        self.max_len = 77\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        name_tokens = self.tokenizers.tokenize_name([str(row.name_first), \n",
    "                                               str(row.name_second)], max_len=self.max_len)\n",
    "        name_first = name_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        name_second = name_tokens[:, 1, :]\n",
    "        desc_tokens = self.tokenizers.tokenize_description([str(row.description_first), \n",
    "                                               str(row.description_second)])\n",
    "        desc_first = desc_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        desc_second = desc_tokens[:, 1, :]\n",
    "        im_first = cv2.imread(os.path.join(self.images_dir, row.image_name_first))\n",
    "        im_first = cv2.cvtColor(im_first, cv2.COLOR_BGR2RGB)\n",
    "        im_first = Image.fromarray(im_first)\n",
    "        im_first = self.transform(im_first)\n",
    "        im_second = cv2.imread(os.path.join(self.images_dir, row.image_name_second))\n",
    "        im_second = cv2.cvtColor(im_second, cv2.COLOR_BGR2RGB)\n",
    "        im_second = Image.fromarray(im_second)\n",
    "        im_second = self.transform(im_second)\n",
    "        label = self.labels[idx]\n",
    "        return im_first, name_first, desc_first, im_second, name_second, desc_second, label\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        last_hidden = last_hidden_states.masked_fill(\n",
    "            ~attention_mask[..., None].bool(), 0.0\n",
    "        )\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "class SiameseRuCLIP(nn.Module):\n",
    "    def __init__(self, preload_ruclip=False, device='cpu', hidden_dim=HIDDEN_DIM, models_dir=DATA_PATH + 'train_results/'):\n",
    "        super().__init__()\n",
    "        self.ruclip = RuCLIPtiny()\n",
    "        if preload_ruclip:\n",
    "            preload_model_name = 'cc12m_rubert_tiny_ep_1.pt' #'cc12m_ddp_4mill_ep_4.pt'\n",
    "            std = torch.load(models_dir + preload_model_name, weights_only=True, map_location=device)\n",
    "            self.ruclip.load_state_dict(std)\n",
    "            self.ruclip = self.ruclip.to(device)\n",
    "            self.ruclip.eval()\n",
    "        self.description_transformer = AutoModel.from_pretrained(DESCRIPTION_MODEL_NAME)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            # # nn.BatchNorm1d(hidden_dim),\n",
    "            # nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            # nn.ReLU(), \n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "        )\n",
    "        \n",
    "    def encode_description(self, desc):\n",
    "        # desc is [input_ids, attention_mask]\n",
    "        last_hidden_states = self.description_transformer(desc[:, 0, :], desc[:, 1, :]).last_hidden_state\n",
    "        attention_mask = desc[:, 1, :]\n",
    "        # TODO: нужно ли делать пулинг, посмотреть на результаты\n",
    "        return average_pool(last_hidden_states, attention_mask)\n",
    "    \n",
    "    def forward(self, im1, name1, desc1, im2, name2, desc2):\n",
    "        image_emb1 = self.ruclip.encode_image(im1)\n",
    "        image_emb2 = self.ruclip.encode_image(im2)\n",
    "        name_emb1 = self.ruclip.encode_text(name1[:, 0, :], name1[:, 1, :])\n",
    "        name_emb2 = self.ruclip.encode_text(name2[:, 0, :], name2[:, 1, :])\n",
    "        desc_emb1 = self.encode_description(desc1) \n",
    "        desc_emb2 = self.encode_description(desc2)\n",
    "        first_emb = torch.cat([image_emb1, name_emb1, desc_emb1], dim=1)\n",
    "        second_emb = torch.cat([image_emb2, name_emb2, desc_emb2], dim=1)\n",
    "        out1 = self.head(first_emb)\n",
    "        out2 = self.head(second_emb)\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        pos = (1-label) * torch.pow(euclidean_distance, 2)\n",
    "        neg = (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        loss_contrastive = torch.mean( pos + neg )\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRASTIVE_MARGIN=1.5\n",
    "CONTRASTIVE_THRESHOLD=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, \n",
    "          epochs_num, train_loader, valid_loader=None, \n",
    "          score=f1_score, device='cpu', print_epoch=False) -> None:\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    counter = []\n",
    "    loss_history = [] \n",
    "    it_number = 0\n",
    "    best_valid_score = 0\n",
    "    best_valid_score = 100\n",
    "    best_weights = None\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\",\n",
    "                                                               factor=0.1, patience=2,\n",
    "                                                               threshold=0.0001,\n",
    "                                                               threshold_mode='rel', cooldown=0,\n",
    "                                                               min_lr=0, eps=1e-08)\n",
    "    for epoch in range(epochs_num):\n",
    "        print(\"Epoch：\", epoch)\n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            im1, name1, desc1, im2, name2, desc2, label = data \n",
    "            im1, name1, desc1, im2, name2, desc2, label = im1.to(device), name1.to(device), desc1.to(device), im2.to(device), name2.to(device), desc2.to(device), label.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            out1, out2 = model(im1, name1, desc1, im2, name2, desc2)\n",
    "            loss = criterion(out1, out2, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 1 == 0: # show changes of loss value after each 10 batches\n",
    "                # it_number += 5\n",
    "                counter.append(it_number)\n",
    "                loss_history.append(loss.item())\n",
    "        # test after each epoch\n",
    "        if print_epoch:\n",
    "            valid_score = validation(model, criterion, valid_loader, score, device)\n",
    "            plot_epoch(loss_history)\n",
    "            print(f'Current loss: {loss}')\n",
    "            # print(f'Current {score.__name__}: {valid_score}')\n",
    "            scheduler.step(valid_score)\n",
    "            if valid_score < best_valid_score:\n",
    "                best_valid_score = valid_score\n",
    "                best_weights = model.state_dict()\n",
    "    return best_valid_score, best_weights\n",
    "\n",
    "def evaluate_pair(output1, output2, target, threshold):\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    # меньше границы, там где будет True — конкуренты\n",
    "    cond = euclidean_distance < threshold\n",
    "    pos_sum = 0\n",
    "    neg_sum = 0\n",
    "    pos_acc = 0\n",
    "    neg_acc = 0\n",
    "\n",
    "    for i in range(len(cond)):\n",
    "        # 1 значит не конкуренты\n",
    "        if target[i]:\n",
    "            neg_sum+=1\n",
    "            # 0 в cond значит дальше друг от друга чем threshold\n",
    "            if not cond[i]:\n",
    "                neg_acc+=1\n",
    "        elif not target[i]:\n",
    "            pos_sum+=1\n",
    "            if cond[i]:\n",
    "                pos_acc+=1\n",
    "\n",
    "    return pos_acc, pos_sum, neg_acc, neg_sum\n",
    "\n",
    "def validation(model, criterion, valid_loader, score, device='cpu') -> float:\n",
    "    valid_loss = 0\n",
    "    val_pos_accuracy = 0\n",
    "    val_neg_accuracy = 0\n",
    "    num_pos = 0\n",
    "    num_neg = 0\n",
    "    with torch.no_grad(): \n",
    "        # model.eval()\n",
    "        for data in tqdm(valid_loader):\n",
    "            im1, name1, desc1, im2, name2, desc2, label = data \n",
    "            im1, name1, desc1, im2, name2, desc2, label = im1.to(device), name1.to(device), desc1.to(device), im2.to(device), name2.to(device), desc2.to(device), label.to(device)\n",
    "            out1, out2 = model(im1, name1, desc1, im2, name2, desc2) \n",
    "            loss = criterion(out1, out2, label)\n",
    "            pos_acc, pos_sum, neg_acc, neg_sum = evaluate_pair(out1, out2, label, CONTRASTIVE_THRESHOLD)\n",
    "            val_pos_accuracy+=pos_acc\n",
    "            val_neg_accuracy+=neg_acc\n",
    "            num_pos+=pos_sum\n",
    "            num_neg+=neg_sum\n",
    "            valid_loss += loss.item()\n",
    "            # print(predicted)\n",
    "            # print(label)\n",
    "            # predicted = predicted.cpu().numpy()\n",
    "            # valid_loss += score(label, predicted)\n",
    "    val_pos_accuracy /= num_pos\n",
    "    val_neg_accuracy /= num_neg\n",
    "    valid_loss /= len(valid_loader)\n",
    "    print(\"Validation loss :{} \\t\\t\\t P Acc : {}, N Acc: {}\\n\".format(valid_loss, val_pos_accuracy, val_neg_accuracy))\n",
    "    return valid_loss\n",
    "\n",
    "def plot_epoch(loss_history)->None:\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Iteration number\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(loss_history, 'b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = DATA_PATH + 'images_5k/'\n",
    "labeled = pd.read_csv(DATA_PATH + 'new_labeled.csv')\n",
    "labeled = labeled.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=70\n",
    "EPOCHS=10\n",
    "EMB_SIZE=768\n",
    "VALIDATION_SPLIT=.2\n",
    "SHUFFLE_DATASET=True\n",
    "RANDOM_SEED=42\n",
    "NUM_WORKERS=6\n",
    "LR=9e-5\n",
    "MOMENTUM=0.9\n",
    "N_SPLITS=3\n",
    "WEIGHT_DECAY=1e-3\n",
    "CONTRASTIVE_MARGIN=1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss: 0.2764648497104645\n",
      "Current f1_score: 0.39869005357225734\n",
      "Epoch： 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45faa5be805485fb2357bc4f701f16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m ContrastiveLoss(margin\u001b[38;5;241m=\u001b[39mCONTRASTIVE_MARGIN)\n\u001b[1;32m     16\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLR, weight_decay\u001b[38;5;241m=\u001b[39mWEIGHT_DECAY)\n\u001b[0;32m---> 17\u001b[0m     best_valid_score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf1_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     nn_scores\u001b[38;5;241m.\u001b[39mappend(best_valid_score)\n\u001b[1;32m     21\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msiamese_contrastive.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, epochs_num, train_loader, valid_loader, score, device, print_epoch)\u001b[0m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out1, out2, label)\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 24\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# show changes of loss value after each 10 batches\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# it_number += 5\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     counter\u001b[38;5;241m.\u001b[39mappend(it_number)\n",
      "File \u001b[0;32m~/pyenvs/ruclip/lib/python3.12/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/pyenvs/ruclip/lib/python3.12/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/pyenvs/ruclip/lib/python3.12/site-packages/torch/optim/adamw.py:227\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    217\u001b[0m         group,\n\u001b[1;32m    218\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         state_steps,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 227\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/pyenvs/ruclip/lib/python3.12/site-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pyenvs/ruclip/lib/python3.12/site-packages/torch/optim/adamw.py:767\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 767\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pyenvs/ruclip/lib/python3.12/site-packages/torch/optim/adamw.py:529\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    526\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_params, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m weight_decay)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 529\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_lerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[1;32m    532\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_addcmul_(\n\u001b[1;32m    533\u001b[0m     device_exp_avg_sqs, device_grads, device_grads, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2\n\u001b[1;32m    534\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=N_SPLITS)\n",
    "nn_scores = list()\n",
    "X, y = labeled.drop(columns='label'), labeled.label.values\n",
    "y = 1 - y \n",
    "best_valid = 100\n",
    "for train_index, valid_index in skf.split(X, y): # разбивка по индексам\n",
    "    train_dataset = SiameseRuCLIPDataset(X.iloc[train_index], y[train_index], images_dir=images_dir)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "    valid_dataset = SiameseRuCLIPDataset(X.iloc[valid_index], y[valid_index], images_dir=images_dir)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    model = SiameseRuCLIP(device=DEVICE)\n",
    "    # for i, child in enumerate(model.children()):\n",
    "    #     for param in model.ruclip.parameters():\n",
    "    #         param.requires_grad = False\n",
    "    # model.ruclip.visual.stages[3].blocks[2].mlp.fc2.requires_grad = True\n",
    "    criterion = ContrastiveLoss(margin=CONTRASTIVE_MARGIN)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    best_valid_score, best_weights = train(model, optimizer, criterion,\n",
    "        EPOCHS, train_loader, valid_loader, \n",
    "        score=f1_score, print_epoch=True, device=DEVICE)\n",
    "    nn_scores.append(best_valid_score)\n",
    "    if best_valid_score < best_valid:\n",
    "        best_valid = best_valid_score\n",
    "        torch.save(best_weights, 'siamese_contrastive.pt')\n",
    "\n",
    "np.mean(nn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(out1, out2, threshold=CONTRASTIVE_THRESHOLD):\n",
    "    # вернёт 1 если похожи\n",
    "    return F.pairwise_distance(out1, out2) < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist = nn.PairwiseDistance(p=2)\n",
    "output = F.pairwise_distance(input1, input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log into services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import dotenv\n",
    "except ImportError:\n",
    "    !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/anton/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use tokens from .env\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import huggingface_hub\n",
    "import wandb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "huggingface_hub.login(token=HF_TOKEN)\n",
    "\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "wandb.login(key=WANDB_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "from timm import create_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "# from torchinfo import summary\n",
    "import transformers\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer,\\\n",
    "        get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import json\n",
    "from itertools import product\n",
    "\n",
    "# import datasets\n",
    "# from datasets import Dataset, concatenate_datasets\n",
    "import argparse\n",
    "import requests\n",
    "\n",
    "from io import BytesIO\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import more_itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE_DATASET_FILE = 'new_labeled.csv'\n",
    "# IMG_DATASET_NAME = 'images_7k'\n",
    "\n",
    "# TABLE_DATASET_FILE = 'WB_OZ_100.csv'\n",
    "# TABLE_DATASET_FILE = 'WB_OZ_100_conjugated.csv'\n",
    "# TABLE_DATASET_FILE = 'WB_OZ_100_conjugated_shuffled_seed=42_fraction=1.csv'\n",
    "TABLE_DATASET_FILE = 'WB_OZ_100_conjugated_shuffled_seed=42_fraction=0.5.csv'\n",
    "IMG_DATASET_NAME = 'images_WB_OZ_100'\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RuCLIPtiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuCLIPtiny(nn.Module):\n",
    "    def __init__(self, name_model_name: str):\n",
    "        \"\"\"\n",
    "        Initializes the RuCLIPtiny module using the provided name model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.visual = create_model('convnext_tiny',\n",
    "                                   pretrained=False,  # set True if you want pretrained weights\n",
    "                                   num_classes=0,\n",
    "                                   in_chans=3)       # output: e.g. 768-dim features\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(name_model_name)\n",
    "        name_model_output_size = self.transformer.config.hidden_size  # inferred dynamically\n",
    "        self.final_ln = nn.Linear(name_model_output_size, 768)         # project to 768 dims\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * torch.log(torch.tensor(1/0.07)))\n",
    "    \n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.stem[0].weight.dtype\n",
    "\n",
    "    def encode_image(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # use the CLS token (first token)\n",
    "        x = x.last_hidden_state[:, 0, :]\n",
    "        x = self.final_ln(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, image: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(input_ids, attention_mask)\n",
    "        # Normalize features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        return logits_per_image, logits_per_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        lambda image: image.convert(\"RGB\"),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "class Tokenizers:\n",
    "    def __init__(self, name_model_name: str, description_model_name: str):\n",
    "        self.name_tokenizer = AutoTokenizer.from_pretrained(name_model_name)\n",
    "        self.desc_tokenizer = AutoTokenizer.from_pretrained(description_model_name)\n",
    "\n",
    "    def tokenize_name(self, texts, max_len=77):\n",
    "        tokenized = self.name_tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "\n",
    "    def tokenize_description(self, texts, max_len=77):\n",
    "        tokenized = self.desc_tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseRuCLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir: str, name_model_name: str, description_model_name: str, df=None, labels=None, df_path=None):\n",
    "        \"\"\"\n",
    "        Dataset requires the concrete models' names for tokenization.\n",
    "        \"\"\"\n",
    "        assert os.path.isdir(images_dir), f\"Image dir does not exist: '{self.images_dir}'\"\n",
    "\n",
    "        self.df = pd.read_csv(df_path) if df_path is not None else df\n",
    "        self.labels = labels\n",
    "        self.images_dir = images_dir\n",
    "        self.tokenizers = Tokenizers(name_model_name, description_model_name)\n",
    "        self.transform = get_transform()\n",
    "        self.max_len = 77\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Tokenize names\n",
    "        name_tokens = self.tokenizers.tokenize_name([str(row.name_first), str(row.name_second)], max_len=self.max_len)\n",
    "        name_first = name_tokens[:, 0, :]  # [input_ids, attention_mask]\n",
    "        name_second = name_tokens[:, 1, :]\n",
    "        # Tokenize descriptions\n",
    "        desc_tokens = self.tokenizers.tokenize_description([str(row.description_first), str(row.description_second)])\n",
    "        desc_first = desc_tokens[:, 0, :]\n",
    "        desc_second = desc_tokens[:, 1, :]\n",
    "        # Process images\n",
    "        im_first_path = os.path.join(self.images_dir, row.image_name_first)\n",
    "        im_first = cv2.imread(im_first_path)\n",
    "        im_first = cv2.cvtColor(im_first, cv2.COLOR_BGR2RGB)\n",
    "        im_first = Image.fromarray(im_first)\n",
    "        im_first = self.transform(im_first)\n",
    "        im_second_path = os.path.join(self.images_dir, row.image_name_first)\n",
    "        im_second = cv2.imread(os.path.join(im_second_path))\n",
    "        im_second = cv2.cvtColor(im_second, cv2.COLOR_BGR2RGB)\n",
    "        im_second = Image.fromarray(im_second)\n",
    "        im_second = self.transform(im_second)\n",
    "        label = self.labels[idx]\n",
    "        return im_first, name_first, desc_first, im_second, name_second, desc_second, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SiameseRuCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "class SiameseRuCLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 device: str,\n",
    "                 name_model_name: str,\n",
    "                 description_model_name: str,\n",
    "                 models_dir: str = None,\n",
    "                 preload_ruclip: bool = False,\n",
    "                 preload_model_name: str = None):\n",
    "        \"\"\"\n",
    "        Initializes the SiameseRuCLIP model.\n",
    "        Required parameters:\n",
    "          - models_dir: directory containing saved checkpoints.\n",
    "          - name_model_name: model name for text (name) branch.\n",
    "          - description_model_name: model name for description branch.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Convert the device string to torch.device.\n",
    "        device = torch.device(device)\n",
    "\n",
    "        # Initialize RuCLIPtiny with the provided name model.\n",
    "        self.ruclip = RuCLIPtiny(name_model_name)\n",
    "        if preload_ruclip:\n",
    "            std = torch.load(\n",
    "                os.path.join(models_dir, preload_model_name),\n",
    "                weights_only=True,\n",
    "                map_location=device\n",
    "            )\n",
    "            self.ruclip.load_state_dict(std)\n",
    "            self.ruclip.eval()\n",
    "        # Explicitly move ruclip to the device.\n",
    "        self.ruclip = self.ruclip.to(device)\n",
    "        \n",
    "        # Initialize description transformer with the provided description model and move it.\n",
    "        self.description_transformer = AutoModel.from_pretrained(description_model_name)\n",
    "        self.description_transformer = self.description_transformer.to(device)\n",
    "        \n",
    "        # Infer dimensions automatically from inner modules.\n",
    "        vision_dim = self.ruclip.visual.num_features           # e.g., 768 from ConvNeXt tiny\n",
    "        name_dim = self.ruclip.final_ln.out_features             # e.g., 768 after projection\n",
    "        desc_dim = self.description_transformer.config.hidden_size  # e.g., 312 for cointegrated/rubert-tiny\n",
    "        hidden_dim = vision_dim + name_dim + desc_dim        # e.g., 768+768+312 = 1848\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Build the MLP head and move it explicitly.\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "        ).to(device)\n",
    "        \n",
    "    def encode_description(self, desc):\n",
    "        # desc is [input_ids, attention_mask]\n",
    "        last_hidden_states = self.description_transformer(desc[:, 0, :], desc[:, 1, :]).last_hidden_state\n",
    "        attention_mask = desc[:, 1, :]\n",
    "        # TODO: нужно ли делать пулинг, посмотреть на результаты\n",
    "        return average_pool(last_hidden_states, attention_mask)\n",
    "    \n",
    "    def forward(self, im1, name1, desc1, im2, name2, desc2):\n",
    "        image_emb1 = self.ruclip.encode_image(im1)\n",
    "        image_emb2 = self.ruclip.encode_image(im2)\n",
    "        name_emb1 = self.ruclip.encode_text(name1[:, 0, :], name1[:, 1, :])\n",
    "        name_emb2 = self.ruclip.encode_text(name2[:, 0, :], name2[:, 1, :])\n",
    "        # desc_emb1 = self.ruclip.encode_text(desc1[:, 0, :], desc1[:, 1, :])\n",
    "        # desc_emb2 = self.ruclip.encode_text(desc2[:, 0, :], desc2[:, 1, :])\n",
    "        desc_emb1 = self.encode_description(desc1) \n",
    "        desc_emb2 = self.encode_description(desc2)\n",
    "        first_emb = torch.cat([image_emb1, name_emb1, desc_emb1], dim=1)\n",
    "        second_emb = torch.cat([image_emb2, name_emb2, desc_emb2], dim=1)\n",
    "        out1 = self.head(first_emb)\n",
    "        out2 = self.head(second_emb)\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1806d43084cf431ea76bbf83f6850238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Download models' weights & text/image datasets\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ID = \"INDEEPA/clip-siamese\"\n",
    "LOCAL_DIR = Path(\"data/train_results\")\n",
    "LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type='dataset',\n",
    "    local_dir='data',\n",
    "    allow_patterns=[\n",
    "        \"train_results/siamese_contrastive*.pt\",\n",
    "        TABLE_DATASET_FILE,\n",
    "        f\"{IMG_DATASET_NAME}.zip\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "IMG_DATASET_NAME = 'images_WB_OZ_100'\n",
    "!unzip -o -q data/{IMG_DATASET_NAME}.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"siamese-contrastive\"\n",
    "\n",
    "model_configs = [ \n",
    "    dict(\n",
    "        MODEL_CKPT = 'siamese_contrastive.pt',\n",
    "\n",
    "        NAME_MODEL_NAME = 'cointegrated/rubert-tiny',\n",
    "        # NAME_MODEL_NAME = 'DeepPavlov/distilrubert-tiny-cased-conversational-v1',\n",
    "\n",
    "        DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny',\n",
    "        # DESCRIPTION_MODEL_NAME = 'sergeyzh/rubert-tiny-turbo',\n",
    "\n",
    "        CONTRASTIVE_THRESHOLD=0.3,\n",
    "    ),\n",
    "    \n",
    "    dict(\n",
    "        MODEL_CKPT = 'siamese_contrastive_7k.pt',\n",
    "\n",
    "        NAME_MODEL_NAME = 'cointegrated/rubert-tiny',\n",
    "        # NAME_MODEL_NAME = 'DeepPavlov/distilrubert-tiny-cased-conversational-v1',\n",
    "\n",
    "        DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny',\n",
    "\n",
    "        CONTRASTIVE_THRESHOLD=0.3,\n",
    "    ),\n",
    "    \n",
    "    \n",
    "    # dict(\n",
    "    #     MODEL_CKPT = 'siamese_contrastive_1gpu.pt',\n",
    "\n",
    "    #     NAME_MODEL_NAME = 'cointegrated/rubert-tiny',\n",
    "    #     # NAME_MODEL_NAME = 'DeepPavlov/distilrubert-tiny-cased-conversational-v1',\n",
    "\n",
    "    #     # DESCRIPTION_MODEL_NAME = 'sergeyzh/rubert-tiny-turbo',\n",
    "    #     DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny',\n",
    "\n",
    "    #     CONTRASTIVE_THRESHOLD=0.3,\n",
    "    # ),\n",
    "]\n",
    "\n",
    "# model_config = model_configs[-1] # choose a particular config for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "labeled = pd.read_csv(DATA_PATH + TABLE_DATASET_FILE)\n",
    "images_dir = DATA_PATH + IMG_DATASET_NAME\n",
    "\n",
    "y = 1 - labeled.label.values # target = minimal euclidean distance = 0\n",
    "X = labeled.drop(columns='label').copy()\n",
    "\n",
    "def load_data(model_config):\n",
    "    test_ds = SiameseRuCLIPDataset(\n",
    "        images_dir,\n",
    "        model_config['NAME_MODEL_NAME'], \n",
    "        model_config['DESCRIPTION_MODEL_NAME'], \n",
    "        X, y\n",
    "    )\n",
    "    test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "    return test_dl\n",
    "\n",
    "# test_dl = load_data(model_config)\n",
    "# im1, name1, desc1, im2, name2, desc2, label = next(iter(test_dl))\n",
    "# im1, name1, desc1, im2, name2, desc2, label = im1.to(DEVICE), name1.to(DEVICE), desc1.to(DEVICE), im2.to(DEVICE), name2.to(DEVICE), desc2.to(DEVICE), label.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# model_name = \"cointegrated/rubert-tiny\"  \n",
    "# model_name = \"DeepPavlov/distilrubert-tiny-cased-conversational-v1\"\n",
    "\n",
    "# _tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# _model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# text = \"Привет, как дела?\"\n",
    "# inputs = _tokenizer(text, return_tensors=\"pt\")\n",
    "# outputs = _model(**inputs)\n",
    "\n",
    "# print(outputs.last_hidden_state.shape)  # Shape: (1, seq_len, 312)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def load_model(model_config):\n",
    "    ckpt_name = model_config['MODEL_CKPT']\n",
    "    model_ckpt_path = Path(DATA_PATH) / 'train_results' / ckpt_name\n",
    "    std = torch.load(model_ckpt_path, map_location=DEVICE)\n",
    "\n",
    "    # Initialize the model using the configuration.\n",
    "    model = SiameseRuCLIP(\n",
    "        name_model_name=model_config[\"NAME_MODEL_NAME\"],\n",
    "        description_model_name=model_config[\"DESCRIPTION_MODEL_NAME\"],\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(std)\n",
    "    return model\n",
    "\n",
    "# model = load_model(model_config)\n",
    "# out1, out2 = model(im1, name1, desc1, im2, name2, desc2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation score\n",
    "\n",
    "def evaluate_pair(output1, output2, target, threshold):\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    # меньше границы, там где будет True — конкуренты\n",
    "    cond = euclidean_distance < threshold\n",
    "    pos_sum = 0\n",
    "    neg_sum = 0\n",
    "    pos_acc = 0\n",
    "    neg_acc = 0\n",
    "\n",
    "    for i in range(len(cond)):\n",
    "        # 1 значит не конкуренты\n",
    "        if target[i]:\n",
    "            neg_sum+=1\n",
    "            # 0 в cond значит дальше друг от друга чем threshold\n",
    "            if not cond[i]:\n",
    "                neg_acc+=1\n",
    "        elif not target[i]:\n",
    "            pos_sum+=1\n",
    "            if cond[i]:\n",
    "                pos_acc+=1\n",
    "\n",
    "    return pos_acc, pos_sum, neg_acc, neg_sum\n",
    "\n",
    "def validation(model, valid_loader, threshold, device=DEVICE) -> float:\n",
    "    # valid_loss = 0\n",
    "    val_pos_accuracy = 0\n",
    "    val_neg_accuracy = 0\n",
    "    num_pos = 0\n",
    "    num_neg = 0\n",
    "    with torch.no_grad(): \n",
    "        # model.eval()\n",
    "        for data in tqdm(valid_loader):\n",
    "            im1, name1, desc1, im2, name2, desc2, label = data \n",
    "            im1, name1, desc1, im2, name2, desc2, label = im1.to(device), name1.to(device), desc1.to(device), im2.to(device), name2.to(device), desc2.to(device), label.to(device)\n",
    "            out1, out2 = model(im1, name1, desc1, im2, name2, desc2) \n",
    "            # loss = criterion(out1, out2, label)\n",
    "            pos_acc, pos_sum, neg_acc, neg_sum = evaluate_pair(out1, out2, label, threshold)\n",
    "            val_pos_accuracy+=pos_acc\n",
    "            val_neg_accuracy+=neg_acc\n",
    "            num_pos+=pos_sum\n",
    "            num_neg+=neg_sum\n",
    "            # valid_loss += loss.item()\n",
    "    val_pos_accuracy = val_pos_accuracy / num_pos if num_pos > 0 else None\n",
    "    val_neg_accuracy = val_neg_accuracy / num_neg if num_neg > 0 else None\n",
    "    # valid_loss /= len(valid_loader)\n",
    "    return val_pos_accuracy, val_neg_accuracy\n",
    "\n",
    "# test_pos_accuracy, test_neg_accuracy = validation(\n",
    "#     model, test_dl,\n",
    "#     model_config['CONTRASTIVE_THRESHOLD']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "def log_to_wandb(model_config, test_pos_accuracy, test_neg_accuracy):\n",
    "    wandb.init(\n",
    "        project=\"product-matching\",\n",
    "        entity=\"overfit1010\",\n",
    "        name=f\"test-{model_config['MODEL_CKPT']}\",\n",
    "        config={\n",
    "            \"table_dataset_file\": TABLE_DATASET_FILE,\n",
    "            \"img_dataset_name\": IMG_DATASET_NAME,\n",
    "            \"model_ckpt\": model_config['MODEL_CKPT'],\n",
    "            \"name_model_name\": model_config['NAME_MODEL_NAME'],\n",
    "            \"description_model_name\": model_config['DESCRIPTION_MODEL_NAME'],\n",
    "            \"threshold\": model_config['CONTRASTIVE_THRESHOLD'],\n",
    "            \"model_type\": MODEL_TYPE,\n",
    "            \"run_type\": \"test\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if test_pos_accuracy is not None and test_neg_accuracy is not None:\n",
    "        test_weighted_accuracy = (test_pos_accuracy + test_neg_accuracy) / 2\n",
    "    else:\n",
    "        test_weighted_accuracy = None\n",
    "\n",
    "    wandb.summary[\"test.pos_acc\"] = test_pos_accuracy\n",
    "    wandb.summary[\"test.neg_acc\"] = test_neg_accuracy\n",
    "    wandb.summary[\"test.weighted_acc\"] = test_weighted_accuracy\n",
    "    wandb.finish()\n",
    "\n",
    "# log_to_wandb(model_config, test_pos_accuracy, test_neg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4843ac1f98264c70b933e7651d8bfb1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/anton/marketplace/clip-siamese/wandb/run-20250403_194228-twxjkbuo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/overfit1010/product-matching/runs/twxjkbuo' target=\"_blank\">test-siamese_contrastive.pt</a></strong> to <a href='https://wandb.ai/overfit1010/product-matching' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/overfit1010/product-matching' target=\"_blank\">https://wandb.ai/overfit1010/product-matching</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/overfit1010/product-matching/runs/twxjkbuo' target=\"_blank\">https://wandb.ai/overfit1010/product-matching/runs/twxjkbuo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test.neg_acc</td><td>0.96</td></tr><tr><td>test.pos_acc</td><td>0.82</td></tr><tr><td>test.weighted_acc</td><td>0.89</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test-siamese_contrastive.pt</strong> at: <a href='https://wandb.ai/overfit1010/product-matching/runs/twxjkbuo' target=\"_blank\">https://wandb.ai/overfit1010/product-matching/runs/twxjkbuo</a><br> View project at: <a href='https://wandb.ai/overfit1010/product-matching' target=\"_blank\">https://wandb.ai/overfit1010/product-matching</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250403_194228-twxjkbuo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11c0fe0f7124b088b9c87b479e4c594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/anton/marketplace/clip-siamese/wandb/run-20250403_194302-q6qdma6h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/overfit1010/product-matching/runs/q6qdma6h' target=\"_blank\">test-siamese_contrastive_7k.pt</a></strong> to <a href='https://wandb.ai/overfit1010/product-matching' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/overfit1010/product-matching' target=\"_blank\">https://wandb.ai/overfit1010/product-matching</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/overfit1010/product-matching/runs/q6qdma6h' target=\"_blank\">https://wandb.ai/overfit1010/product-matching/runs/q6qdma6h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test.neg_acc</td><td>0.68</td></tr><tr><td>test.pos_acc</td><td>0.94</td></tr><tr><td>test.weighted_acc</td><td>0.81</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test-siamese_contrastive_7k.pt</strong> at: <a href='https://wandb.ai/overfit1010/product-matching/runs/q6qdma6h' target=\"_blank\">https://wandb.ai/overfit1010/product-matching/runs/q6qdma6h</a><br> View project at: <a href='https://wandb.ai/overfit1010/product-matching' target=\"_blank\">https://wandb.ai/overfit1010/product-matching</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250403_194302-q6qdma6h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_config in model_configs:\n",
    "    model = load_model(model_config)\n",
    "    test_dl = load_data(model_config)\n",
    "    test_pos_accuracy, test_neg_accuracy = validation(\n",
    "        model, test_dl,\n",
    "        model_config['CONTRASTIVE_THRESHOLD']\n",
    "    )\n",
    "    log_to_wandb(model_config, test_pos_accuracy, test_neg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

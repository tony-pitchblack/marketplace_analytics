{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e25795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU</th>\n",
       "      <th>Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Схема</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Niche</th>\n",
       "      <th>Seller</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Balance FBS</th>\n",
       "      <th>Warehouses count</th>\n",
       "      <th>...</th>\n",
       "      <th>FBS</th>\n",
       "      <th>Base price</th>\n",
       "      <th>Category Position</th>\n",
       "      <th>Categories Last Count</th>\n",
       "      <th>Sales Per Day Average</th>\n",
       "      <th>Turnover</th>\n",
       "      <th>Frozen stocks</th>\n",
       "      <th>Frozen stocks cost</th>\n",
       "      <th>Frozen stocks percent</th>\n",
       "      <th>is_primary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>936454663</td>\n",
       "      <td>Карта мира географическая политическая интерак...</td>\n",
       "      <td>Канцелярские товары/Бумажная продукция/Карты мира</td>\n",
       "      <td>FBO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Дом и сад/Декор и интерьер/Картины и панно/Гра...</td>\n",
       "      <td>GooDaY</td>\n",
       "      <td>346</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4990</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>66.54</td>\n",
       "      <td>112</td>\n",
       "      <td>202832</td>\n",
       "      <td>32.37</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SKU                                               Name  \\\n",
       "0  936454663  Карта мира географическая политическая интерак...   \n",
       "\n",
       "                                            Category Схема Brand  \\\n",
       "0  Канцелярские товары/Бумажная продукция/Карты мира   FBO   NaN   \n",
       "\n",
       "                                               Niche  Seller  Balance  \\\n",
       "0  Дом и сад/Декор и интерьер/Картины и панно/Гра...  GooDaY      346   \n",
       "\n",
       "   Balance FBS  Warehouses count  ...  FBS  Base price  Category Position  \\\n",
       "0            0                12  ...    0        4990                  1   \n",
       "\n",
       "   Categories Last Count  Sales Per Day Average  Turnover  Frozen stocks  \\\n",
       "0                      1                    5.2     66.54            112   \n",
       "\n",
       "   Frozen stocks cost  Frozen stocks percent  is_primary  \n",
       "0              202832                  32.37       False  \n",
       "\n",
       "[1 rows x 43 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'data/Карты_мира_озон.xlsm'\n",
    "source_df = pd.read_excel(file_path)\n",
    "source_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0fb8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cdn1.ozone.ru/s3/multimedia-k/6400404908.jpg'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_img_url = source_df['Thumb'].sample(1).iloc[0]\n",
    "sample_img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67461774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SKU',\n",
       " 'Name',\n",
       " 'Category',\n",
       " 'Схема',\n",
       " 'Brand',\n",
       " 'Niche',\n",
       " 'Seller',\n",
       " 'Balance',\n",
       " 'Balance FBS',\n",
       " 'Warehouses count',\n",
       " 'Comments',\n",
       " 'Final price',\n",
       " 'Max price',\n",
       " 'Min price',\n",
       " 'Average price',\n",
       " 'Median price',\n",
       " 'Цена с Ozon картой',\n",
       " 'Sales',\n",
       " 'Revenue',\n",
       " 'Revenue potential',\n",
       " 'Revenue average',\n",
       " 'Lost profit',\n",
       " 'Lost profit percent',\n",
       " 'URL',\n",
       " 'Thumb',\n",
       " 'Pics Count',\n",
       " 'Has Video',\n",
       " 'First Date',\n",
       " 'Days in website',\n",
       " 'Days in stock',\n",
       " 'Days with sales',\n",
       " 'Average if in stock',\n",
       " 'Rating',\n",
       " 'FBS',\n",
       " 'Base price',\n",
       " 'Category Position',\n",
       " 'Categories Last Count',\n",
       " 'Sales Per Day Average',\n",
       " 'Turnover',\n",
       " 'Frozen stocks',\n",
       " 'Frozen stocks cost',\n",
       " 'Frozen stocks percent',\n",
       " 'is_primary']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a3e0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     https://www.ozon.ru/context/detail/id/936454663/\n",
       "1     https://www.ozon.ru/context/detail/id/491279127/\n",
       "2     https://www.ozon.ru/context/detail/id/844750071/\n",
       "3    https://www.ozon.ru/context/detail/id/1737112880/\n",
       "4     https://www.ozon.ru/context/detail/id/216810859/\n",
       "Name: URL, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_df['URL'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffbbed90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.ozon.ru/context/detail/id/1594043933/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_url = source_df.sample(1)['URL'].item()\n",
    "sample_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34379af",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7ccdf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import random\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium_stealth import stealth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ab1e2",
   "metadata": {},
   "source": [
    "# Scraping Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71bc6f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/micromamba/envs/clip-siamese/lib/python3.13/site-packages/selenium/webdriver/remote/remote_connection.py:418: UserWarning: Embedding username and password in URL could be insecure, use ClientConfig instead\n",
      "  headers = self.get_remote_connection_headers(parsed_url, self._client_config.keep_alive)\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver import Remote\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Hard-coded Selenium endpoint URL (includes credentials & zone)\n",
    "SERVER_ADDR = \"https://brd-customer-hl_c581f9ac-zone-scraping_browser2:jbna7sqbowco@brd.superproxy.io:9515\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Function to instantiate a new scraping browser (Remote WebDriver)\n",
    "def init_scraping_browser():\n",
    "    options = Options()\n",
    "    # Add any desired options; for example, uncomment the next line for headless mode:\n",
    "    # options.add_argument(\"--headless\")\n",
    "    return Remote(command_executor=SERVER_ADDR, options=options)\n",
    "\n",
    "driver = init_scraping_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6743f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base scraping func\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Scraping function with verbose flag\n",
    "def extract_img_link_and_descr(driver, sample_url, verbose=False):\n",
    "    if verbose:\n",
    "        print(f\"Navigating to {sample_url}...\")\n",
    "    driver.get(sample_url)\n",
    "\n",
    "    # Wait for small image element to be clickable, then click it\n",
    "    small_img_elem = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \".ln_28.kk4_28.k7k_28\"))\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"Small image detected\")\n",
    "    small_img_elem.click()\n",
    "\n",
    "    # Wait for big image element and extract its src attribute\n",
    "    big_img_elem = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \".jz6_28.j6z_28\"))\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"Big image detected\")\n",
    "    img_link = big_img_elem.get_attribute(\"src\")\n",
    "\n",
    "    # Wait for the description element and extract its text\n",
    "    description_elem = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"k7y_28\"))\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"Description detected\")\n",
    "    description = description_elem.text\n",
    "\n",
    "    return img_link, description\n",
    "\n",
    "# sample_url = source_df.sample(1)['URL'].item()\n",
    "# img_link, description = extract_img_link_and_descr(\n",
    "#     driver, sample_url,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# print(f\"\\nImage link: {img_link}\")\n",
    "# print(f\"\\nDescription: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac828154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping func w/ logging\n",
    "\n",
    "import concurrent\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "import time\n",
    "\n",
    "\n",
    "def extract_descr(driver, sample_url, row_id, verbose=False, max_retries=3):\n",
    "    \"\"\"\n",
    "    Navigates to sample_url and extracts the description text from the element with class \"k7y_28\".\n",
    "    If a WebDriverException occurs (e.g. for out-of-stock pages), it returns a flag indicating out-of-stock.\n",
    "    Retries the extraction if a TimeoutException occurs.\n",
    "    Logs messages in capitalized letters with the row_id and attempt number.\n",
    "    \n",
    "    Returns:\n",
    "        (description, out_of_stock_flag)\n",
    "        - description: extracted text or None on failure.\n",
    "        - out_of_stock_flag: True if the page seems to be an out-of-stock page.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            if verbose:\n",
    "                print(f\"[{row_id}] - (ATTEMPT {attempt}) - NAVIGATING TO: {sample_url}\")\n",
    "            driver.get(sample_url)\n",
    "            \n",
    "            # Wait for the description element (class \"k7y_28\") and extract its text.\n",
    "            description_elem = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"k7y_28\"))\n",
    "            )\n",
    "            if verbose:\n",
    "                print(f\"[{row_id}] - (ATTEMPT {attempt}) - # DESCRIPTION DETECTED.\")\n",
    "            description = description_elem.text.strip()\n",
    "            return description, False\n",
    "\n",
    "        except TimeoutException:\n",
    "            if verbose:\n",
    "                print(f\"[{row_id}] - (ATTEMPT {attempt}) - # TIMEOUT OCCURRED. RETRYING...\")\n",
    "            time.sleep(1)\n",
    "            if attempt == max_retries:\n",
    "                if verbose:\n",
    "                    print(f\"[{row_id}] - (ATTEMPT {attempt}) - # MAX RETRIES REACHED. RETURNING FAILURE.\")\n",
    "                return None, False\n",
    "\n",
    "        except WebDriverException as e:\n",
    "            if verbose:\n",
    "                print(f\"[{row_id}] - (ATTEMPT {attempt}) - # WEB DRIVER ERROR: {repr(e)}\")\n",
    "                print(f\"[{row_id}] - (ATTEMPT {attempt}) - # TREATING AS OUT-OF-STOCK.\")\n",
    "            return None, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ad3e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple workers\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Function to run a single scraping session. Each session instantiates its own driver.\n",
    "def run_scraping_session(row_id, sample_url, verbose=False):\n",
    "    driver = init_scraping_browser()  # Ensure this function is defined to create a new driver instance.\n",
    "    try:\n",
    "        result = extract_img_link_and_descr(driver, sample_url, row_id, verbose=verbose)\n",
    "        return row_id, sample_url, result\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Main code to execute multiple scraping sessions in parallel.\n",
    "def main(n_workers):\n",
    "    # Sample multiple rows from source_df and get their indices along with URLs.\n",
    "    sampled_rows = source_df.sample(n_workers)\n",
    "    urls_dict = sampled_rows['URL'].to_dict()  # keys are the original row indices\n",
    "    \n",
    "    # Use a thread pool to run scraping sessions in parallel.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        future_to_index = {\n",
    "            executor.submit(run_scraping_session, idx, url, True): idx \n",
    "            for idx, url in urls_dict.items()\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(future_to_index):\n",
    "            idx = future_to_index[future]\n",
    "            try:\n",
    "                row_id, sample_url, (img_link, description) = future.result()\n",
    "                print(f\"\\n[RESULT {row_id}] - URL: {sample_url}\")\n",
    "                print(f\"# IMAGE LINK: {img_link}\")\n",
    "                print(f\"# DESCRIPTION: {description}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n[{idx}] - ERROR PROCESSING ROW: {e}\\n\")\n",
    "\n",
    "# main(n_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84e75b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent\n",
    "import numpy as np\n",
    "import threading\n",
    "from queue import Queue\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Assume source_df is your DataFrame with a 'URL' column.\n",
    "# Also assume that init_scraping_browser() and extract_img_link_and_descr()\n",
    "# are defined (the latter accepts parameters: driver, sample_url, row_id, verbose).\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Function to run a single scraping session. It returns the row index, URL, and scraped description.\n",
    "def run_scraping_session(row_id, sample_url, verbose=False):\n",
    "    driver = init_scraping_browser()  # Ensure your init_scraping_browser() returns a new driver instance.\n",
    "    try:\n",
    "        description, out_of_stock = extract_descr(driver, sample_url, row_id, verbose=verbose)\n",
    "        if out_of_stock:\n",
    "            description = np.nan\n",
    "        return row_id, sample_url, description\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "def writer_thread_func(result_queue, output_file):\n",
    "    schema = pa.schema([\n",
    "        ('SKU', pa.string()),\n",
    "        ('URL', pa.string()),\n",
    "        ('description', pa.string()),\n",
    "    ])\n",
    "    writer = None\n",
    "    while True:\n",
    "        result = result_queue.get()\n",
    "        if result is None:\n",
    "            result_queue.task_done()\n",
    "            break  # Sentinel value to exit.\n",
    "        sku, url, description = result\n",
    "        data = {\n",
    "            'SKU': [str(sku)],\n",
    "            'URL': [str(url)],\n",
    "            'description': [str(description) if description is not None else None],\n",
    "        }\n",
    "        try:\n",
    "            table = pa.Table.from_pydict(data, schema=schema)\n",
    "        except Exception as ex:\n",
    "            print(f\"Data conversion error for SKU {sku}: {data} -> {ex}\")\n",
    "            result_queue.task_done()\n",
    "            continue\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(output_file, schema)\n",
    "        writer.write_table(table)\n",
    "        result_queue.task_done()\n",
    "    if writer is not None:\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f71c0f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1584935571] - ERROR PROCESSING ROW: Message: Account is suspended\n",
      "\n",
      "[814767331] - ERROR PROCESSING ROW: Message: Account is suspended\n",
      "\n",
      "[1747641323] - ERROR PROCESSING ROW: Message: Account is suspended\n",
      "\n",
      "[1709708496] - ERROR PROCESSING ROW: Message: Account is suspended\n",
      "\n",
      "[1449500274] - ERROR PROCESSING ROW: Message: Account is suspended\n",
      "\n",
      "[1449544096] - ERROR PROCESSING ROW: Message: Account is suspended\n",
      "\n",
      "[279431159] - ERROR PROCESSING ROW: Message: Account is suspended\n",
      "\n",
      "[679329071] - ERROR PROCESSING ROW: Message: Account is suspended\n",
      "\n",
      "[1422792458] - ERROR PROCESSING ROW: Message: Account is suspended\n",
      "\n",
      "[779004014] - ERROR PROCESSING ROW: Message: Account is suspended\n",
      "\n",
      "Scraping completed and descriptions appended to: tmp/scraped_descriptions.parquet\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "def run_scraping_session(sku, sample_url, verbose=False):\n",
    "    driver = init_scraping_browser()  # Ensure your init_scraping_browser() returns a new driver instance.\n",
    "    try:\n",
    "        description, out_of_stock = extract_descr(driver, sample_url, sku, verbose=verbose)\n",
    "        if out_of_stock:\n",
    "            description = None  # Mark out-of-stock pages with None.\n",
    "        return sku, sample_url, description\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "def main(source_df):\n",
    "    from queue import Queue\n",
    "    result_queue = Queue()\n",
    "    output_file = \"tmp/scraped_descriptions.parquet\"\n",
    "    \n",
    "    # Start the writer thread.\n",
    "    writer_thread = threading.Thread(target=writer_thread_func, args=(result_queue, output_file))\n",
    "    writer_thread.start()\n",
    "    \n",
    "    # Create a dictionary mapping SKU to URL from source_df.\n",
    "    # Assumes source_df has columns \"SKU\" and \"URL\"\n",
    "    urls_dict = source_df.set_index(\"SKU\")[\"URL\"].to_dict()\n",
    "\n",
    "    # Process rows concurrently.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_sku = {\n",
    "            executor.submit(run_scraping_session, sku, url, True): sku \n",
    "            for sku, url in urls_dict.items()\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(future_to_sku):\n",
    "            sku = future_to_sku[future]\n",
    "            try:\n",
    "                sku, sample_url, description = future.result()\n",
    "                result_queue.put((sku, sample_url, description))\n",
    "            except Exception as e:\n",
    "                print(f\"[{sku}] - ERROR PROCESSING ROW: {e}\")\n",
    "\n",
    "    result_queue.join()\n",
    "    result_queue.put(None)\n",
    "    writer_thread.join()\n",
    "    print(\"Scraping completed and descriptions appended to:\", output_file)\n",
    "\n",
    "out_of_stock_indices = [\n",
    "    4009,\n",
    "    1388,\n",
    "    1446,\n",
    "    5696\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Run main with a sample of source_df entries.\n",
    "main(\n",
    "    # source_df,\n",
    "    source_df.sample(10)\n",
    "    # source_df.loc[out_of_stock_indices]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed7ca8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4009</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/1573132474/</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1446</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/1860011823/</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1388</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/1726150202/</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5696</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/578890261/</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                url description\n",
       "0   4009  https://www.ozon.ru/context/detail/id/1573132474/        None\n",
       "1   1446  https://www.ozon.ru/context/detail/id/1860011823/        None\n",
       "2   1388  https://www.ozon.ru/context/detail/id/1726150202/        None\n",
       "3   5696   https://www.ozon.ru/context/detail/id/578890261/        None"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('tmp/scraped_descriptions.parquet')\n",
    "df.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

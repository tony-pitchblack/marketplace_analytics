{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIMIT_BATCHES = None\n",
    "LIMIT_BATCHES = 6 # cpu smoke run\n",
    "\n",
    "MODEL_CFG_IDX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/'\n",
    "\n",
    "TABLE_DATASET_FILE = 'OZ_geo_5700_no_descriptions.csv'\n",
    "IMG_DATASET_NAME = 'images_OZ_geo_5700'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"siamese-contrastive\"\n",
    "\n",
    "model_configs = [ \n",
    "    dict(\n",
    "        MODEL_CKPT = 'siamese_contrastive.pt',\n",
    "\n",
    "        NAME_MODEL_NAME = 'cointegrated/rubert-tiny',\n",
    "        # NAME_MODEL_NAME = 'DeepPavlov/distilrubert-tiny-cased-conversational-v1',\n",
    "\n",
    "        DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny',\n",
    "        # DESCRIPTION_MODEL_NAME = 'sergeyzh/rubert-tiny-turbo',\n",
    "\n",
    "        CONTRASTIVE_THRESHOLD=0.3,\n",
    "    ),\n",
    "    \n",
    "    dict(\n",
    "        MODEL_CKPT = 'siamese_contrastive_7k.pt',\n",
    "\n",
    "        NAME_MODEL_NAME = 'cointegrated/rubert-tiny',\n",
    "        # NAME_MODEL_NAME = 'DeepPavlov/distilrubert-tiny-cased-conversational-v1',\n",
    "\n",
    "        DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny',\n",
    "\n",
    "        CONTRASTIVE_THRESHOLD=0.3,\n",
    "    ),\n",
    "    \n",
    "    \n",
    "    # dict(\n",
    "    #     MODEL_CKPT = 'siamese_contrastive_1gpu.pt',\n",
    "\n",
    "    #     NAME_MODEL_NAME = 'cointegrated/rubert-tiny',\n",
    "    #     # NAME_MODEL_NAME = 'DeepPavlov/distilrubert-tiny-cased-conversational-v1',\n",
    "\n",
    "    #     # DESCRIPTION_MODEL_NAME = 'sergeyzh/rubert-tiny-turbo',\n",
    "    #     DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny',\n",
    "\n",
    "    #     CONTRASTIVE_THRESHOLD=0.3,\n",
    "    # ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log into services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import dotenv\n",
    "except ImportError:\n",
    "    !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Use tokens from .env\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import huggingface_hub\n",
    "import wandb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "huggingface_hub.login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "from timm import create_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "# from torchinfo import summary\n",
    "import transformers\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer,\\\n",
    "        get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import json\n",
    "from itertools import product\n",
    "\n",
    "# import datasets\n",
    "# from datasets import Dataset, concatenate_datasets\n",
    "import argparse\n",
    "import requests\n",
    "\n",
    "from io import BytesIO\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import more_itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RuCLIPtiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuCLIPtiny(nn.Module):\n",
    "    def __init__(self, name_model_name: str):\n",
    "        \"\"\"\n",
    "        Initializes the RuCLIPtiny module using the provided name model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.visual = create_model('convnext_tiny',\n",
    "                                   pretrained=False,  # set True if you want pretrained weights\n",
    "                                   num_classes=0,\n",
    "                                   in_chans=3)       # output: e.g. 768-dim features\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(name_model_name)\n",
    "        name_model_output_size = self.transformer.config.hidden_size  # inferred dynamically\n",
    "        self.final_ln = nn.Linear(name_model_output_size, 768)         # project to 768 dims\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * torch.log(torch.tensor(1/0.07)))\n",
    "    \n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.stem[0].weight.dtype\n",
    "\n",
    "    def encode_image(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # use the CLS token (first token)\n",
    "        x = x.last_hidden_state[:, 0, :]\n",
    "        x = self.final_ln(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, image: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(input_ids, attention_mask)\n",
    "        # Normalize features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        return logits_per_image, logits_per_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        lambda image: image.convert(\"RGB\"),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "class Tokenizers:\n",
    "    def __init__(self, name_model_name: str, description_model_name: str):\n",
    "        self.name_tokenizer = AutoTokenizer.from_pretrained(name_model_name)\n",
    "        self.desc_tokenizer = AutoTokenizer.from_pretrained(description_model_name)\n",
    "\n",
    "    def tokenize_name(self, texts, max_len=77):\n",
    "        tokenized = self.name_tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "\n",
    "    def tokenize_description(self, texts, max_len=77):\n",
    "        tokenized = self.desc_tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class NameTokenizer:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize(self, texts, max_len=77):\n",
    "        tokenized = self.tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "\n",
    "\n",
    "class DescriptionTokenizer:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize(self, texts, max_len=77):\n",
    "        tokenized = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseRuCLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir: str, name_model_name: str, description_model_name: str, df=None, labels=None, df_path=None):\n",
    "        \"\"\"\n",
    "        Dataset requires the concrete models' names for tokenization.\n",
    "        \"\"\"\n",
    "        assert os.path.isdir(images_dir), f\"Image dir does not exist: '{self.images_dir}'\"\n",
    "\n",
    "        self.df = pd.read_csv(df_path) if df_path is not None else df\n",
    "        self.labels = labels\n",
    "        self.images_dir = images_dir\n",
    "        self.tokenizers = Tokenizers(name_model_name, description_model_name)\n",
    "        self.transform = get_transform()\n",
    "        self.max_len = 77\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Tokenize names\n",
    "        name_tokens = self.tokenizers.tokenize_name([str(row.name_first), str(row.name_second)], max_len=self.max_len)\n",
    "        name_first = name_tokens[:, 0, :]  # [input_ids, attention_mask]\n",
    "        name_second = name_tokens[:, 1, :]\n",
    "        # Tokenize descriptions\n",
    "        desc_tokens = self.tokenizers.tokenize_description([str(row.description_first), str(row.description_second)])\n",
    "        desc_first = desc_tokens[:, 0, :]\n",
    "        desc_second = desc_tokens[:, 1, :]\n",
    "        # Process images\n",
    "        im_first_path = os.path.join(self.images_dir, row.image_name_first)\n",
    "        im_first = cv2.imread(im_first_path)\n",
    "        im_first = cv2.cvtColor(im_first, cv2.COLOR_BGR2RGB)\n",
    "        im_first = Image.fromarray(im_first)\n",
    "        im_first = self.transform(im_first)\n",
    "        im_second_path = os.path.join(self.images_dir, row.image_name_first)\n",
    "        im_second = cv2.imread(os.path.join(im_second_path))\n",
    "        im_second = cv2.cvtColor(im_second, cv2.COLOR_BGR2RGB)\n",
    "        im_second = Image.fromarray(im_second)\n",
    "        im_second = self.transform(im_second)\n",
    "        label = self.labels[idx]\n",
    "        return im_first, name_first, desc_first, im_second, name_second, desc_second, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "class RuCLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir: str, name_model_name: str, df=None, df_path=None):\n",
    "        \"\"\"\n",
    "        Dataset for RuCLIP embeddings: returns (image, name) tuples.\n",
    "        Only requires the name model for tokenization.\n",
    "        \"\"\"\n",
    "        assert os.path.isdir(images_dir), f\"Image dir does not exist: '{images_dir}'\"\n",
    "\n",
    "        self.df = pd.read_csv(df_path) if df_path is not None else df\n",
    "        self.images_dir = images_dir\n",
    "        self.name_tokenizer = NameTokenizer(model_name=name_model_name)\n",
    "        self.transform = get_transform()\n",
    "        self.max_len = 77\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Tokenize name\n",
    "        name_tokens = self.name_tokenizer.tokenize([str(row['name'])], max_len=self.max_len)\n",
    "        name_input_ids = name_tokens[0, 0, :]  # Shape: (max_len,)\n",
    "        name_attention_mask = name_tokens[1, 0, :]  # Shape: (max_len,)\n",
    "        name = torch.stack([name_input_ids, name_attention_mask])  # Shape: (2, max_len)\n",
    "\n",
    "        # Load and transform image\n",
    "        image_path = os.path.join(self.images_dir, row.image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SiameseRuCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "class SiameseRuCLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 device: str,\n",
    "                 name_model_name: str,\n",
    "                 description_model_name: str,\n",
    "                 models_dir: str = None,\n",
    "                 preload_ruclip: bool = False,\n",
    "                 preload_model_name: str = None):\n",
    "        \"\"\"\n",
    "        Initializes the SiameseRuCLIP model.\n",
    "        Required parameters:\n",
    "          - models_dir: directory containing saved checkpoints.\n",
    "          - name_model_name: model name for text (name) branch.\n",
    "          - description_model_name: model name for description branch.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        device = torch.device(device)\n",
    "\n",
    "        # Initialize RuCLIPtiny\n",
    "        self.ruclip = RuCLIPtiny(name_model_name)\n",
    "        if preload_ruclip:\n",
    "            std = torch.load(\n",
    "                os.path.join(models_dir, preload_model_name),\n",
    "                weights_only=True,\n",
    "                map_location=device\n",
    "            )\n",
    "            self.ruclip.load_state_dict(std)\n",
    "            self.ruclip.eval()\n",
    "        self.ruclip = self.ruclip.to(device)\n",
    "\n",
    "        # Initialize the description transformer\n",
    "        self.description_transformer = AutoModel.from_pretrained(description_model_name)\n",
    "        self.description_transformer = self.description_transformer.to(device)\n",
    "\n",
    "        # Determine dimensionality\n",
    "        vision_dim = self.ruclip.visual.num_features\n",
    "        name_dim = self.ruclip.final_ln.out_features\n",
    "        desc_dim = self.description_transformer.config.hidden_size\n",
    "        self.hidden_dim = vision_dim + name_dim + desc_dim\n",
    "\n",
    "        # Define MLP head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim // 2, self.hidden_dim // 4),\n",
    "        ).to(device)\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.ruclip.encode_image(image)\n",
    "\n",
    "    def encode_name(self, name):\n",
    "        return self.ruclip.encode_text(name[:, 0, :], name[:, 1, :])\n",
    "\n",
    "    def encode_description(self, desc):\n",
    "        last_hidden_states = self.description_transformer(desc[:, 0, :], desc[:, 1, :]).last_hidden_state\n",
    "        attention_mask = desc[:, 1, :]\n",
    "        return average_pool(last_hidden_states, attention_mask)\n",
    "\n",
    "    def forward(self, im1, name1, desc1, im2, name2, desc2):\n",
    "        image_emb1 = self.encode_image(im1)\n",
    "        image_emb2 = self.encode_image(im2)\n",
    "        name_emb1 = self.encode_name(name1)\n",
    "        name_emb2 = self.encode_name(name2)\n",
    "        desc_emb1 = self.encode_description(desc1)\n",
    "        desc_emb2 = self.encode_description(desc2)\n",
    "\n",
    "        first_emb = torch.cat([image_emb1, name_emb1, desc_emb1], dim=1)\n",
    "        second_emb = torch.cat([image_emb2, name_emb2, desc_emb2], dim=1)\n",
    "\n",
    "        out1 = self.head(first_emb)\n",
    "        out2 = self.head(second_emb)\n",
    "        return out1, out2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e3c358ad1241108b3467a20c4b7293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Download models' weights & text/image datasets\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ID = \"INDEEPA/clip-siamese\"\n",
    "LOCAL_DIR = Path(\"data/train_results\")\n",
    "LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type='dataset',\n",
    "    local_dir='data',\n",
    "    allow_patterns=[\n",
    "        \"train_results/siamese_contrastive*.pt\",\n",
    "        TABLE_DATASET_FILE,\n",
    "        f\"{IMG_DATASET_NAME}.zip\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "!unzip -n -q data/{IMG_DATASET_NAME}.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "source_df = pd.read_csv(DATA_PATH + TABLE_DATASET_FILE)\n",
    "images_dir = DATA_PATH + IMG_DATASET_NAME\n",
    "\n",
    "def load_data(model_config):\n",
    "    test_ds = RuCLIPDataset(\n",
    "        images_dir,\n",
    "        model_config['NAME_MODEL_NAME'], \n",
    "        source_df\n",
    "    )\n",
    "    test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "    return test_dl\n",
    "\n",
    "model_config = model_configs[MODEL_CFG_IDX]  # choose a particular config for debugging\n",
    "test_dl = load_data(model_config)\n",
    "\n",
    "# # Get one batch from the dataloader\n",
    "# images, names = next(iter(test_dl))\n",
    "\n",
    "# # Move tensors to device\n",
    "# images = images.to(DEVICE)\n",
    "# names = names.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def load_model(model_config):\n",
    "    ckpt_name = model_config['MODEL_CKPT']\n",
    "    model_ckpt_path = Path(DATA_PATH) / 'train_results' / ckpt_name\n",
    "    std = torch.load(model_ckpt_path, map_location=DEVICE)\n",
    "\n",
    "    # Initialize the model using the configuration.\n",
    "    model = SiameseRuCLIP(\n",
    "        name_model_name=model_config[\"NAME_MODEL_NAME\"],\n",
    "        description_model_name=model_config[\"DESCRIPTION_MODEL_NAME\"],\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(std)\n",
    "    return model\n",
    "\n",
    "model = load_model(model_config)\n",
    "\n",
    "# img_emb = model.encode_image(images)\n",
    "# name_emb = model.encode_name(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5caf39e25394e978b3967d700640eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((48, 768), (48, 768), (48, 312))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def compute_embeddings(model, dataloader, device=DEVICE, limit_batches=None):\n",
    "    \"\"\"\n",
    "    Compute and return the embeddings for all samples in the dataloader, separately for images and names.\n",
    "\n",
    "    Args:\n",
    "        model: The embedding model (must implement encode_image and encode_name).\n",
    "        dataloader: DataLoader returning (image, name) pairs.\n",
    "        device: The device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        image_embeddings (np.ndarray): Embeddings for images.\n",
    "        name_embeddings (np.ndarray): Embeddings for names.\n",
    "    \"\"\"\n",
    "    all_image_embeddings = []\n",
    "    all_name_embeddings = []\n",
    "    all_descr_embeddings = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, names in tqdm(dataloader):\n",
    "            if limit_batches is not None and len(all_image_embeddings) == limit_batches:\n",
    "                break\n",
    "\n",
    "            images = images.to(device)\n",
    "            names = names.to(device)\n",
    "\n",
    "            image_emb = model.encode_image(images)\n",
    "            name_emb = model.encode_name(names)\n",
    "            descr_emb = model.encode_description(names)\n",
    "\n",
    "            all_image_embeddings.append(image_emb.cpu().numpy())\n",
    "            all_name_embeddings.append(name_emb.cpu().numpy())\n",
    "            all_descr_embeddings.append(descr_emb.cpu().numpy())\n",
    "\n",
    "    image_embeddings = np.concatenate(all_image_embeddings, axis=0)\n",
    "    name_embeddings = np.concatenate(all_name_embeddings, axis=0)\n",
    "    all_descr_embeddings = np.concatenate(all_descr_embeddings, axis=0)\n",
    "    \n",
    "    return image_embeddings, name_embeddings, all_descr_embeddings\n",
    "\n",
    "img_embs_all, name_embs_all, descr_embs_all = compute_embeddings(\n",
    "    model, test_dl,\n",
    "    limit_batches=LIMIT_BATCHES\n",
    ")\n",
    "\n",
    "img_embs_all.shape, name_embs_all.shape, descr_embs_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title find_top_k_similar\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_top_k_similar(query_embeddings, embedding_matrix, k=None, metric='euclidean-inverse', exclude_indices=None):\n",
    "    \"\"\"\n",
    "    Find top-k similar items using Euclidean distance with inverse similarity.\n",
    "    \n",
    "    Args:\n",
    "        query_embeddings (np.ndarray): Array of query embeddings, shape (batch, D).\n",
    "        embedding_matrix (np.ndarray): Array of all embeddings, shape (N, D).\n",
    "        k (int or None): Number of top matches to return, or None to return all valid sorted candidates.\n",
    "        metric (str): Must be 'euclidean-inverse'. Other metrics raise NotImplementedError.\n",
    "        exclude_indices (list, np.ndarray, or boolean mask, optional): Indices to exclude from search.\n",
    "\n",
    "    Returns:\n",
    "        top_k (np.ndarray): Indices of similar embeddings for each query, shape (batch, M).\n",
    "        scores (np.ndarray): Corresponding similarity scores (0-1 range, higher=more similar), shape (batch, M).\n",
    "    \"\"\"\n",
    "    if metric != 'euclidean-inverse':\n",
    "        raise NotImplementedError(f\"Metric '{metric}' is not implemented. Only 'euclidean-inverse' is supported.\")\n",
    "    \n",
    "    # Convert exclude_indices to a boolean mask if needed\n",
    "    mask = None\n",
    "    if exclude_indices is not None:\n",
    "        if isinstance(exclude_indices, (list, np.ndarray)):\n",
    "            exclude_indices = np.array(exclude_indices)\n",
    "            if exclude_indices.dtype != bool:\n",
    "                mask = np.zeros(embedding_matrix.shape[0], dtype=bool)\n",
    "                mask[exclude_indices] = True\n",
    "            else:\n",
    "                mask = exclude_indices\n",
    "        else:\n",
    "            raise ValueError(\"exclude_indices must be a list, np.ndarray, or boolean mask.\")\n",
    "    \n",
    "    # Compute Euclidean distances: shape (batch, N)\n",
    "    distances = np.linalg.norm(query_embeddings[:, None, :] - embedding_matrix[None, :, :], axis=2)\n",
    "    \n",
    "    # Set distances for excluded indices to +infinity so they are not selected\n",
    "    if mask is not None:\n",
    "        distances[:, mask] = np.inf\n",
    "    \n",
    "    # Convert distances to inverse similarities: similarity = 1 / (1 + distance)\n",
    "    # Handle inf distances by setting their similarity to 0\n",
    "    similarities = np.where(distances == np.inf, 0.0, 1.0 / (1.0 + distances))\n",
    "    \n",
    "    # Sort indices in descending order of similarity (higher similarity = better match)\n",
    "    sorted_idx = np.argsort(-similarities, axis=1)\n",
    "    \n",
    "    if k is None:\n",
    "        # Filter out zero similarities (from excluded indices) for each query\n",
    "        valid_indices = []\n",
    "        valid_scores = []\n",
    "        for i in range(similarities.shape[0]):\n",
    "            # Get indices where similarity is not 0 (i.e., not excluded)\n",
    "            valid_mask = similarities[i] > 0\n",
    "            valid_idx = np.where(valid_mask)[0]\n",
    "            # Sort valid indices by similarity (descending)\n",
    "            valid_idx = valid_idx[np.argsort(-similarities[i, valid_idx])]\n",
    "            valid_indices.append(valid_idx)\n",
    "            valid_scores.append(similarities[i, valid_idx])\n",
    "        \n",
    "        # Pad arrays to same length if needed\n",
    "        if valid_indices:  # Check if we have any valid indices\n",
    "            max_valid = max(len(x) for x in valid_indices)\n",
    "            top_k = np.array([np.pad(x, (0, max_valid - len(x)), 'constant', constant_values=-1) for x in valid_indices])\n",
    "            scores = np.array([np.pad(x, (0, max_valid - len(x)), 'constant', constant_values=0.0) for x in valid_scores])\n",
    "        else:\n",
    "            # Handle edge case where no valid indices exist\n",
    "            top_k = np.array([]).reshape(similarities.shape[0], 0)\n",
    "            scores = np.array([]).reshape(similarities.shape[0], 0)\n",
    "    else:\n",
    "        top_k = sorted_idx[:, :k]\n",
    "        scores = np.take_along_axis(similarities, top_k, axis=1)\n",
    "    \n",
    "    return top_k, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load queries\n",
    "\n",
    "QUERY_SELLER = 'ИНТЕРТРЕЙД'\n",
    "query_idx_all = source_df[source_df['seller'] == QUERY_SELLER].index.to_numpy()\n",
    "\n",
    "# Limit queries the actual number of embeddings\n",
    "max_emb_cnt = img_embs_all.shape[0]\n",
    "query_idx_all = query_idx_all[query_idx_all < max_emb_cnt]\n",
    "\n",
    "# For a batch of query embeddings with shape (batch, 768)\n",
    "query_img_embs = img_embs_all[query_idx_all]  # e.g., shape (3, 768)\n",
    "query_name_embs = name_embs_all[query_idx_all]\n",
    "query_descr_embs = descr_embs_all[query_idx_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k image indices per query (shape):\n",
      "(6, 30)\n"
     ]
    }
   ],
   "source": [
    "# Find top-k matches for descr\n",
    "top_k_descr, scores_descr = find_top_k_similar(\n",
    "    query_descr_embs, descr_embs_all, k=30, metric='euclidean-inverse',\n",
    "    exclude_indices=query_idx_all\n",
    ")\n",
    "\n",
    "print(\"Top-k image indices per query (shape):\")\n",
    "print(top_k_descr.shape)\n",
    "\n",
    "# print(\"Corresponding similarity scores:\")\n",
    "# print(scores_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image similarity scores (shape):\n",
      "(6, 48)\n"
     ]
    }
   ],
   "source": [
    "# Get all scores for images\n",
    "_, scores_img = find_top_k_similar(\n",
    "    query_img_embs, img_embs_all, k=None, metric='cosine',\n",
    "    exclude_indices=query_idx_all\n",
    ")\n",
    "\n",
    "print(\"Image similarity scores (shape):\")\n",
    "print(scores_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name similarity scores (shape):\n",
      "(6, 48)\n"
     ]
    }
   ],
   "source": [
    "# Get all scores for names\n",
    "_, scores_name = find_top_k_similar(\n",
    "    query_name_embs, name_embs_all, k=None, metric='cosine',\n",
    "    exclude_indices=query_idx_all\n",
    ")\n",
    "\n",
    "print(\"Name similarity scores (shape):\")\n",
    "print(scores_name.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined scores shape: (6, 48)\n"
     ]
    }
   ],
   "source": [
    "# Get combined scores of image + name\n",
    "\n",
    "# Define weights (they should sum to 1)\n",
    "weight_name = 0.7  # weight for name scores\n",
    "weight_img = 1 - weight_name   # weight for image scores\n",
    "\n",
    "# Compute the linear combination\n",
    "combined_scores = weight_name * scores_name + weight_img * scores_img\n",
    "\n",
    "print(\"Combined scores shape:\", combined_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 30)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter combined scores by top-k from description\n",
    "\n",
    "filtered_combined_scores = np.take_along_axis(combined_scores, top_k_descr, axis=1)\n",
    "filtered_combined_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take top-k within the filtered subset\n",
    "FINAL_TOP_K = 5\n",
    "\n",
    "# Compute the relative indices of top-k candidates from the filtered combined scores.\n",
    "relative_top_k = np.argsort(-filtered_combined_scores, axis=1)[:, :FINAL_TOP_K]\n",
    "relative_top_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 5) (6, 5)\n",
      "[[ 0  2  3  4  5]\n",
      " [ 7  9 10 11 12]\n",
      " [ 6  7  8  9 10]\n",
      " [ 7  9 10 11 12]\n",
      " [ 9 10 11 12 13]\n",
      " [ 3  5  6  7  9]]\n",
      "[[0.98533237 0.9757167  0.9734     0.9684659  0.96020365]\n",
      " [0.97292763 0.97076315 0.9707185  0.9705864  0.9704597 ]\n",
      " [0.75158215 0.7397786  0.652728   0.6320817  0.62593603]\n",
      " [0.98445165 0.9772097  0.97649187 0.9720524  0.9679757 ]\n",
      " [0.97400707 0.97213286 0.97202116 0.96628004 0.9623976 ]\n",
      " [0.9813541  0.97896326 0.9784416  0.9777859  0.9756112 ]]\n"
     ]
    }
   ],
   "source": [
    "# Map back relative final top-k to original final top-k\n",
    "final_scores = np.take_along_axis(filtered_combined_scores, relative_top_k, axis=1)\n",
    "final_top_k = np.take_along_axis(top_k_descr, relative_top_k, axis=1)\n",
    "print(final_top_k.shape, final_scores.shape)\n",
    "print(final_top_k)\n",
    "print(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU Query</th>\n",
       "      <th>Top-1 SKU</th>\n",
       "      <th>Top-2 SKU</th>\n",
       "      <th>Top-3 SKU</th>\n",
       "      <th>Top-4 SKU</th>\n",
       "      <th>Top-5 SKU</th>\n",
       "      <th>Top-1 Score</th>\n",
       "      <th>Top-2 Score</th>\n",
       "      <th>Top-3 Score</th>\n",
       "      <th>Top-4 Score</th>\n",
       "      <th>Top-5 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>491279127</td>\n",
       "      <td>936454663</td>\n",
       "      <td>844750071</td>\n",
       "      <td>1737112880</td>\n",
       "      <td>216810859</td>\n",
       "      <td>861723214</td>\n",
       "      <td>0.985332</td>\n",
       "      <td>0.975717</td>\n",
       "      <td>0.973400</td>\n",
       "      <td>0.968466</td>\n",
       "      <td>0.960204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491270369</td>\n",
       "      <td>438166622</td>\n",
       "      <td>861593242</td>\n",
       "      <td>490685952</td>\n",
       "      <td>861605997</td>\n",
       "      <td>1713026634</td>\n",
       "      <td>0.972928</td>\n",
       "      <td>0.970763</td>\n",
       "      <td>0.970719</td>\n",
       "      <td>0.970586</td>\n",
       "      <td>0.970460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>922230517</td>\n",
       "      <td>268682139</td>\n",
       "      <td>438166622</td>\n",
       "      <td>1712946829</td>\n",
       "      <td>861593242</td>\n",
       "      <td>490685952</td>\n",
       "      <td>0.751582</td>\n",
       "      <td>0.739779</td>\n",
       "      <td>0.652728</td>\n",
       "      <td>0.632082</td>\n",
       "      <td>0.625936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>491273791</td>\n",
       "      <td>438166622</td>\n",
       "      <td>861593242</td>\n",
       "      <td>490685952</td>\n",
       "      <td>861605997</td>\n",
       "      <td>1713026634</td>\n",
       "      <td>0.984452</td>\n",
       "      <td>0.977210</td>\n",
       "      <td>0.976492</td>\n",
       "      <td>0.972052</td>\n",
       "      <td>0.967976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>508611672</td>\n",
       "      <td>861593242</td>\n",
       "      <td>490685952</td>\n",
       "      <td>861605997</td>\n",
       "      <td>1713026634</td>\n",
       "      <td>1422373846</td>\n",
       "      <td>0.974007</td>\n",
       "      <td>0.972133</td>\n",
       "      <td>0.972021</td>\n",
       "      <td>0.966280</td>\n",
       "      <td>0.962398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SKU Query  Top-1 SKU  Top-2 SKU   Top-3 SKU   Top-4 SKU   Top-5 SKU  \\\n",
       "0  491279127  936454663  844750071  1737112880   216810859   861723214   \n",
       "1  491270369  438166622  861593242   490685952   861605997  1713026634   \n",
       "2  922230517  268682139  438166622  1712946829   861593242   490685952   \n",
       "3  491273791  438166622  861593242   490685952   861605997  1713026634   \n",
       "4  508611672  861593242  490685952   861605997  1713026634  1422373846   \n",
       "\n",
       "   Top-1 Score  Top-2 Score  Top-3 Score  Top-4 Score  Top-5 Score  \n",
       "0     0.985332     0.975717     0.973400     0.968466     0.960204  \n",
       "1     0.972928     0.970763     0.970719     0.970586     0.970460  \n",
       "2     0.751582     0.739779     0.652728     0.632082     0.625936  \n",
       "3     0.984452     0.977210     0.976492     0.972052     0.967976  \n",
       "4     0.974007     0.972133     0.972021     0.966280     0.962398  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for query_idx, top_k_indices_per_query, scores_per_query in zip(\n",
    "    query_idx_all, final_top_k, final_scores\n",
    "):\n",
    "    query_sku = source_df.loc[query_idx]['sku']\n",
    "    top_k_skus = source_df.loc[top_k_indices_per_query]['sku']\n",
    "\n",
    "    top_k_sku_results = {\n",
    "        f'Top-{rank_idx + 1} SKU': sku\n",
    "        for rank_idx, sku in enumerate(top_k_skus)\n",
    "    }\n",
    "\n",
    "    top_k_score_results = {\n",
    "        f'Top-{rank_idx + 1} Score': score\n",
    "        for rank_idx, score in enumerate(scores_per_query.tolist())\n",
    "    }\n",
    "    results.append({\n",
    "        'SKU Query': query_sku,\n",
    "        **top_k_sku_results,\n",
    "        **top_k_score_results\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output to:\n",
      "data/Карты_мира_Озон_всего=48_top-5_Seller=ИНТЕРТРЕЙД_модель=siamese_contrastive.pt_image-weight=0.3.csv\n"
     ]
    }
   ],
   "source": [
    "used_entries = img_embs_all.shape[0]\n",
    "\n",
    "file_name = (\n",
    "    f'Карты_мира_Озон_'\n",
    "    f'всего={used_entries}_'\n",
    "    f'top-{FINAL_TOP_K}_'\n",
    "    f'Seller={QUERY_SELLER}_'\n",
    "    f\"модель={model_config['MODEL_CKPT']}_\"\n",
    "    f\"image-weight={round(weight_img, 2)}\"\n",
    "    '.csv'\n",
    ")\n",
    "\n",
    "file_path = Path(DATA_PATH) / file_name\n",
    "\n",
    "print(\"Output to:\")\n",
    "print(file_path)\n",
    "\n",
    "results.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU Query</th>\n",
       "      <th>Top-1 SKU</th>\n",
       "      <th>Top-2 SKU</th>\n",
       "      <th>Top-3 SKU</th>\n",
       "      <th>Top-4 SKU</th>\n",
       "      <th>Top-5 SKU</th>\n",
       "      <th>Top-1 Score</th>\n",
       "      <th>Top-2 Score</th>\n",
       "      <th>Top-3 Score</th>\n",
       "      <th>Top-4 Score</th>\n",
       "      <th>Top-5 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>491279127</td>\n",
       "      <td>936454663</td>\n",
       "      <td>844750071</td>\n",
       "      <td>1737112880</td>\n",
       "      <td>216810859</td>\n",
       "      <td>861723214</td>\n",
       "      <td>0.985332</td>\n",
       "      <td>0.975717</td>\n",
       "      <td>0.973400</td>\n",
       "      <td>0.968466</td>\n",
       "      <td>0.960204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491270369</td>\n",
       "      <td>438166622</td>\n",
       "      <td>861593242</td>\n",
       "      <td>490685952</td>\n",
       "      <td>861605997</td>\n",
       "      <td>1713026634</td>\n",
       "      <td>0.972928</td>\n",
       "      <td>0.970763</td>\n",
       "      <td>0.970719</td>\n",
       "      <td>0.970586</td>\n",
       "      <td>0.970460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>922230517</td>\n",
       "      <td>268682139</td>\n",
       "      <td>438166622</td>\n",
       "      <td>1712946829</td>\n",
       "      <td>861593242</td>\n",
       "      <td>490685952</td>\n",
       "      <td>0.751582</td>\n",
       "      <td>0.739779</td>\n",
       "      <td>0.652728</td>\n",
       "      <td>0.632082</td>\n",
       "      <td>0.625936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>491273791</td>\n",
       "      <td>438166622</td>\n",
       "      <td>861593242</td>\n",
       "      <td>490685952</td>\n",
       "      <td>861605997</td>\n",
       "      <td>1713026634</td>\n",
       "      <td>0.984452</td>\n",
       "      <td>0.977210</td>\n",
       "      <td>0.976492</td>\n",
       "      <td>0.972052</td>\n",
       "      <td>0.967976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>508611672</td>\n",
       "      <td>861593242</td>\n",
       "      <td>490685952</td>\n",
       "      <td>861605997</td>\n",
       "      <td>1713026634</td>\n",
       "      <td>1422373846</td>\n",
       "      <td>0.974007</td>\n",
       "      <td>0.972133</td>\n",
       "      <td>0.972021</td>\n",
       "      <td>0.966280</td>\n",
       "      <td>0.962398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>922231521</td>\n",
       "      <td>1737112880</td>\n",
       "      <td>861723214</td>\n",
       "      <td>268682139</td>\n",
       "      <td>438166622</td>\n",
       "      <td>861593242</td>\n",
       "      <td>0.981354</td>\n",
       "      <td>0.978963</td>\n",
       "      <td>0.978442</td>\n",
       "      <td>0.977786</td>\n",
       "      <td>0.975611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SKU Query   Top-1 SKU  Top-2 SKU   Top-3 SKU   Top-4 SKU   Top-5 SKU  \\\n",
       "0  491279127   936454663  844750071  1737112880   216810859   861723214   \n",
       "1  491270369   438166622  861593242   490685952   861605997  1713026634   \n",
       "2  922230517   268682139  438166622  1712946829   861593242   490685952   \n",
       "3  491273791   438166622  861593242   490685952   861605997  1713026634   \n",
       "4  508611672   861593242  490685952   861605997  1713026634  1422373846   \n",
       "5  922231521  1737112880  861723214   268682139   438166622   861593242   \n",
       "\n",
       "   Top-1 Score  Top-2 Score  Top-3 Score  Top-4 Score  Top-5 Score  \n",
       "0     0.985332     0.975717     0.973400     0.968466     0.960204  \n",
       "1     0.972928     0.970763     0.970719     0.970586     0.970460  \n",
       "2     0.751582     0.739779     0.652728     0.632082     0.625936  \n",
       "3     0.984452     0.977210     0.976492     0.972052     0.967976  \n",
       "4     0.974007     0.972133     0.972021     0.966280     0.962398  \n",
       "5     0.981354     0.978963     0.978442     0.977786     0.975611  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge to original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>936454663</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/936454663/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491279127</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/491279127/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>844750071</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/844750071/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1737112880</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/1737112880/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216810859</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/216810859/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SKU                                                URL\n",
       "0   936454663   https://www.ozon.ru/context/detail/id/936454663/\n",
       "1   491279127   https://www.ozon.ru/context/detail/id/491279127/\n",
       "2   844750071   https://www.ozon.ru/context/detail/id/844750071/\n",
       "3  1737112880  https://www.ozon.ru/context/detail/id/1737112880/\n",
       "4   216810859   https://www.ozon.ru/context/detail/id/216810859/"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_df[['sku', 'url']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read results manually\n",
    "\n",
    "# # results_file_path = 'data/Карты_мира_Озон_всего=5703_top-5_Seller=ИНТЕРТРЕЙД_модель=siamese_contrastive_7k.pt_name-weight=0.7.csv'\n",
    "# results_file_path = 'data/Карты_мира_Озон_всего=5703_top-5_Seller=ИНТЕРТРЕЙД_модель=siamese_contrastive.pt_name-weight=0.7.csv'\n",
    "\n",
    "# results = pd.read_csv(results_file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU Query</th>\n",
       "      <th>Top-1 SKU</th>\n",
       "      <th>Top-2 SKU</th>\n",
       "      <th>Top-3 SKU</th>\n",
       "      <th>Top-4 SKU</th>\n",
       "      <th>Top-5 SKU</th>\n",
       "      <th>Top-1 Score</th>\n",
       "      <th>Top-2 Score</th>\n",
       "      <th>Top-3 Score</th>\n",
       "      <th>Top-4 Score</th>\n",
       "      <th>Top-5 Score</th>\n",
       "      <th>Query URL</th>\n",
       "      <th>Top-1 URL</th>\n",
       "      <th>Top-2 URL</th>\n",
       "      <th>Top-3 URL</th>\n",
       "      <th>Top-4 URL</th>\n",
       "      <th>Top-5 URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>491279127</td>\n",
       "      <td>844750071</td>\n",
       "      <td>1737112880</td>\n",
       "      <td>216810859</td>\n",
       "      <td>861723214</td>\n",
       "      <td>268682139</td>\n",
       "      <td>0.976887</td>\n",
       "      <td>0.974613</td>\n",
       "      <td>0.970675</td>\n",
       "      <td>0.969757</td>\n",
       "      <td>0.966751</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/491279127/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/844750071/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/1737112880/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/216810859/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/861723214/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/268682139/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491270369</td>\n",
       "      <td>861593242</td>\n",
       "      <td>490685952</td>\n",
       "      <td>861605997</td>\n",
       "      <td>1713026634</td>\n",
       "      <td>1422373846</td>\n",
       "      <td>0.971047</td>\n",
       "      <td>0.971002</td>\n",
       "      <td>0.970893</td>\n",
       "      <td>0.970808</td>\n",
       "      <td>0.970334</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/491270369/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/861593242/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/490685952/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/861605997/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/1713026634/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/1422373846/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>922230517</td>\n",
       "      <td>1422373846</td>\n",
       "      <td>499509545</td>\n",
       "      <td>950549378</td>\n",
       "      <td>844770867</td>\n",
       "      <td>536896417</td>\n",
       "      <td>0.932672</td>\n",
       "      <td>0.925902</td>\n",
       "      <td>0.925869</td>\n",
       "      <td>0.924192</td>\n",
       "      <td>0.922490</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/922230517/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/1422373846/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/499509545/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/950549378/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/844770867/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/536896417/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>491273791</td>\n",
       "      <td>499509545</td>\n",
       "      <td>950549378</td>\n",
       "      <td>844770867</td>\n",
       "      <td>536896417</td>\n",
       "      <td>1422369500</td>\n",
       "      <td>0.971599</td>\n",
       "      <td>0.971397</td>\n",
       "      <td>0.970715</td>\n",
       "      <td>0.969950</td>\n",
       "      <td>0.966699</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/491273791/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/499509545/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/950549378/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/844770867/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/536896417/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/1422369500/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>508611672</td>\n",
       "      <td>861605997</td>\n",
       "      <td>950549378</td>\n",
       "      <td>844770867</td>\n",
       "      <td>951285479</td>\n",
       "      <td>1422369500</td>\n",
       "      <td>0.974237</td>\n",
       "      <td>0.971716</td>\n",
       "      <td>0.969571</td>\n",
       "      <td>0.965520</td>\n",
       "      <td>0.964504</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/508611672/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/861605997/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/950549378/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/844770867/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/951285479/</td>\n",
       "      <td>https://www.ozon.ru/context/detail/id/1422369500/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SKU Query   Top-1 SKU   Top-2 SKU  Top-3 SKU   Top-4 SKU   Top-5 SKU  \\\n",
       "0  491279127   844750071  1737112880  216810859   861723214   268682139   \n",
       "1  491270369   861593242   490685952  861605997  1713026634  1422373846   \n",
       "2  922230517  1422373846   499509545  950549378   844770867   536896417   \n",
       "3  491273791   499509545   950549378  844770867   536896417  1422369500   \n",
       "4  508611672   861605997   950549378  844770867   951285479  1422369500   \n",
       "\n",
       "   Top-1 Score  Top-2 Score  Top-3 Score  Top-4 Score  Top-5 Score  \\\n",
       "0     0.976887     0.974613     0.970675     0.969757     0.966751   \n",
       "1     0.971047     0.971002     0.970893     0.970808     0.970334   \n",
       "2     0.932672     0.925902     0.925869     0.924192     0.922490   \n",
       "3     0.971599     0.971397     0.970715     0.969950     0.966699   \n",
       "4     0.974237     0.971716     0.969571     0.965520     0.964504   \n",
       "\n",
       "                                          Query URL  \\\n",
       "0  https://www.ozon.ru/context/detail/id/491279127/   \n",
       "1  https://www.ozon.ru/context/detail/id/491270369/   \n",
       "2  https://www.ozon.ru/context/detail/id/922230517/   \n",
       "3  https://www.ozon.ru/context/detail/id/491273791/   \n",
       "4  https://www.ozon.ru/context/detail/id/508611672/   \n",
       "\n",
       "                                           Top-1 URL  \\\n",
       "0   https://www.ozon.ru/context/detail/id/844750071/   \n",
       "1   https://www.ozon.ru/context/detail/id/861593242/   \n",
       "2  https://www.ozon.ru/context/detail/id/1422373846/   \n",
       "3   https://www.ozon.ru/context/detail/id/499509545/   \n",
       "4   https://www.ozon.ru/context/detail/id/861605997/   \n",
       "\n",
       "                                           Top-2 URL  \\\n",
       "0  https://www.ozon.ru/context/detail/id/1737112880/   \n",
       "1   https://www.ozon.ru/context/detail/id/490685952/   \n",
       "2   https://www.ozon.ru/context/detail/id/499509545/   \n",
       "3   https://www.ozon.ru/context/detail/id/950549378/   \n",
       "4   https://www.ozon.ru/context/detail/id/950549378/   \n",
       "\n",
       "                                          Top-3 URL  \\\n",
       "0  https://www.ozon.ru/context/detail/id/216810859/   \n",
       "1  https://www.ozon.ru/context/detail/id/861605997/   \n",
       "2  https://www.ozon.ru/context/detail/id/950549378/   \n",
       "3  https://www.ozon.ru/context/detail/id/844770867/   \n",
       "4  https://www.ozon.ru/context/detail/id/844770867/   \n",
       "\n",
       "                                           Top-4 URL  \\\n",
       "0   https://www.ozon.ru/context/detail/id/861723214/   \n",
       "1  https://www.ozon.ru/context/detail/id/1713026634/   \n",
       "2   https://www.ozon.ru/context/detail/id/844770867/   \n",
       "3   https://www.ozon.ru/context/detail/id/536896417/   \n",
       "4   https://www.ozon.ru/context/detail/id/951285479/   \n",
       "\n",
       "                                           Top-5 URL  \n",
       "0   https://www.ozon.ru/context/detail/id/268682139/  \n",
       "1  https://www.ozon.ru/context/detail/id/1422373846/  \n",
       "2   https://www.ozon.ru/context/detail/id/536896417/  \n",
       "3  https://www.ozon.ru/context/detail/id/1422369500/  \n",
       "4  https://www.ozon.ru/context/detail/id/1422369500/  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume 'results' has columns: \"SKU Query\", \"Top-1\", \"Top-2\", ..., \"Top-k\"\n",
    "k = 5\n",
    "\n",
    "final_df = (\n",
    "    source_df[['sku', 'url']]\n",
    "    .rename(columns={'sku': 'SKU Query', 'url': 'Query URL'})\n",
    "    .merge(results, on='SKU Query', how='right')\n",
    ")\n",
    "\n",
    "for i in range(1, k+1):\n",
    "    sku_col = f\"Top-{i} SKU\"\n",
    "    url_col = f\"Top-{i} URL\"\n",
    "    final_df = final_df.merge(\n",
    "        source_df[['sku', 'url']].rename(columns={'sku': sku_col, 'url': url_col}),\n",
    "        on=sku_col,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "cols_order = [\n",
    "    \"SKU Query\",\n",
    "    \"Top-1 SKU\", \"Top-2 SKU\", \"Top-3 SKU\", \"Top-4 SKU\", \"Top-5 SKU\",\n",
    "    \"Top-1 Score\", \"Top-2 Score\", \"Top-3 Score\", \"Top-4 Score\", \"Top-5 Score\",\n",
    "    \"Query URL\",\n",
    "    \"Top-1 URL\", \"Top-2 URL\", \"Top-3 URL\", \"Top-4 URL\", \"Top-5 URL\"\n",
    "]\n",
    "final_df = final_df[cols_order]\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# url_cols = [col for col in final_df.columns if re.search('url', col)]\n",
    "# final_df[url_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Карты_мира_Озон_всего=5703_top-5_Seller=ИНТЕРТРЕЙД_модель=siamese_contrastive.pt_name-weight=0.7_URL-included.csv\n"
     ]
    }
   ],
   "source": [
    "final_df_file_name = Path(results_file_path).stem + '_URL-included.csv'\n",
    "print(final_df_file_name)\n",
    "\n",
    "final_df.to_csv(Path(DATA_PATH) / final_df_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SKU Query                                              507113963\n",
       "Top-1 SKU                                             1706808786\n",
       "Top-2 SKU                                             1550235173\n",
       "Top-3 SKU                                             1709679383\n",
       "Top-4 SKU                                              553013273\n",
       "Top-5 SKU                                             1233218843\n",
       "Top-1 Score                                             0.726337\n",
       "Top-2 Score                                             0.722856\n",
       "Top-3 Score                                             0.722841\n",
       "Top-4 Score                                             0.721007\n",
       "Top-5 Score                                             0.720955\n",
       "Query URL       https://www.ozon.ru/context/detail/id/507113963/\n",
       "Top-1 URL      https://www.ozon.ru/context/detail/id/1706808786/\n",
       "Top-2 URL      https://www.ozon.ru/context/detail/id/1550235173/\n",
       "Top-3 URL      https://www.ozon.ru/context/detail/id/1709679383/\n",
       "Top-4 URL       https://www.ozon.ru/context/detail/id/553013273/\n",
       "Top-5 URL      https://www.ozon.ru/context/detail/id/1233218843/\n",
       "Name: 18, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.loc[18]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcaee945",
   "metadata": {},
   "source": [
    "# Installs & tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65f9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import dotenv\n",
    "except ImportError:\n",
    "    !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c3f67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Kaggle environment. Skipping Kaggle secrets.\n",
      "Trying to load HF_TOKEN from .env.\n",
      "Success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Log into huggingface via Kaggle Secrets or .env\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import huggingface_hub\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "    user_secrets = UserSecretsClient()\n",
    "    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not Kaggle environment. Skipping Kaggle secrets.\")\n",
    "    print(\"Trying to load HF_TOKEN from .env.\")\n",
    "    load_dotenv()\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "    print(\"Success!\")\n",
    "\n",
    "huggingface_hub.login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a0467",
   "metadata": {},
   "source": [
    "# Choose notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "be91836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "## CHOOSE MODEL PARAMETERS #################################################\n",
    "\n",
    "MODEL_NAME_POSTFIX='splitting-by-query'\n",
    "NAME_MODEL_NAME = 'cointegrated/rubert-tiny' # 'DeepPavlov/distilrubert-tiny-cased-conversational-v1'\n",
    "DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny'\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "RESULTS_DIR = 'train_results/'\n",
    "\n",
    "# BATCH_SIZE=60 # uses 14.5GiB of 1 GPU\n",
    "# NUM_WORKERS=2 # TODO: use multiple GPU, tune number of workers\n",
    "# SMOKE_TEST_BATCHES=None\n",
    "# EPOCHS=10 # epochs > 8 => overfit; NOTE: can train for longer since we take best validation checkpoint anyway\n",
    "\n",
    "BATCH_SIZE=5\n",
    "NUM_WORKERS=0\n",
    "SMOKE_TEST_BATCHES=10\n",
    "EPOCHS=2\n",
    "\n",
    "PRELOAD_MODEL_NAME = 'cc12m_rubert_tiny_ep_1.pt' # preload ruclip\n",
    "# PRELOAD_MODEL_NAME = None\n",
    "\n",
    "POS_WEIGHT = 4.0 # TODO: infer from data\n",
    "\n",
    "# USE_ALL_TRAIN_PAIRS = False\n",
    "# MAX_SAMPLES_PER_EPOCH = None\n",
    "\n",
    "USE_ALL_TRAIN_PAIRS = True\n",
    "MAX_SAMPLES_PER_EPOCH = 2_500\n",
    "# MAX_SAMPLES_PER_EPOCH = 2_500 * 12\n",
    "\n",
    "DROPOUT = 0.5\n",
    "# DROPOUT = None\n",
    "\n",
    "# BEST_CKPT_METRIC = 'f1'\n",
    "BEST_CKPT_METRIC = 'pos_acc'\n",
    "\n",
    "VALIDATION_SPLIT=.05\n",
    "TEST_SPLIT=.1\n",
    "RANDOM_SEED=42\n",
    "LR=9e-5\n",
    "MOMENTUM=0.9\n",
    "WEIGHT_DECAY=1e-2\n",
    "CONTRASTIVE_MARGIN=1.5\n",
    "CONTRASTIVE_THRESHOLD=0.3\n",
    "SHEDULER_PATIENCE=3 # in epochs\n",
    "\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3d9e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHOOSE DATA #########################################################\n",
    "\n",
    "DATA_PATH=  'data/'\n",
    "SOURCE_TABLE_NAME = 'tables_OZ_geo_5500/processed/OZ_geo_5500.csv'\n",
    "\n",
    "# --- Load source_df and pairwise_mapping_df from Parquet ---\n",
    "SOURCE_TABLE_NAME = 'tables_OZ_geo_5500/processed/OZ_geo_5500.csv'\n",
    "PAIRWISE_TABLE_NAME = 'tables_OZ_geo_5500/processed/regex-pairwise-groups/regex-pairwise-groups_num-queries=20_patterns-dict-hash=6dbf9b3ef9568e60cd959f87be7e3b26.parquet'\n",
    "IMG_DATASET_NAME = 'images_OZ_geo_5500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b5eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOGGING PARAMS ######################################################################\n",
    "\n",
    "# MLFLOW_URI = \"http://176.56.185.96:5000\"\n",
    "# MLFLOW_URI = \"http://localhost:5000\"\n",
    "MLFLOW_URI = None\n",
    "\n",
    "MLFLOW_EXPERIMENT = \"siamese/1fold\"\n",
    "\n",
    "TELEGRAM_TOKEN = None\n",
    "# TELEGRAM_TOKEN = '' # set token to get notifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b40bc83",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00017085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "from timm import create_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import transforms\n",
    "# from torchinfo import summary\n",
    "# import transformers\n",
    "# from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer,\\\n",
    "#         get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# import json\n",
    "# from itertools import product\n",
    "\n",
    "# import datasets\n",
    "# from datasets import Dataset, concatenate_datasets\n",
    "# import argparse\n",
    "import requests\n",
    "\n",
    "# from io import BytesIO\n",
    "# from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "# import more_itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import mlflow\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3494173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tg_report(text, token=None) -> None:\n",
    "    method = 'sendMessage'\n",
    "    chat_id = 324956476\n",
    "    _ = requests.post(\n",
    "            url='https://api.telegram.org/bot{0}/{1}'.format(token, method),\n",
    "            data={'chat_id': chat_id, 'text': text} \n",
    "        ).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34706f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuCLIPtiny(nn.Module):\n",
    "    def __init__(self, name_model_name):\n",
    "        super().__init__()\n",
    "        self.visual = create_model('convnext_tiny',\n",
    "                                   pretrained=False, # TODO: берём претрейн\n",
    "                                   num_classes=0,\n",
    "                                   in_chans=3)  # out 768\n",
    "\n",
    "        self.transformer = AutoModel.from_pretrained(name_model_name)\n",
    "        name_model_output_shape = self.transformer.config.hidden_size  # dynamically get hidden size\n",
    "        self.final_ln = torch.nn.Linear(name_model_output_shape, 768)  # now uses the transformer hidden size\n",
    "        self.logit_scale = torch.nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.stem[0].weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        x = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = x.last_hidden_state[:, 0, :]\n",
    "        x = self.final_ln(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(input_ids, attention_mask)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84518fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        _convert_image_to_rgb,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]), ])\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "class Tokenizers:\n",
    "    def __init__(self):\n",
    "        self.name_tokenizer = AutoTokenizer.from_pretrained(NAME_MODEL_NAME)\n",
    "        self.desc_tokenizer = AutoTokenizer.from_pretrained(DESCRIPTION_MODEL_NAME)\n",
    "\n",
    "    def tokenize_name(self, texts, max_len=77):\n",
    "        tokenized = self.name_tokenizer.batch_encode_plus(texts,\n",
    "                                                     truncation=True,\n",
    "                                                     add_special_tokens=True,\n",
    "                                                     max_length=max_len,\n",
    "                                                     padding='max_length',\n",
    "                                                     return_attention_mask=True,\n",
    "                                                     return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "    \n",
    "    def tokenize_description(self, texts, max_len=77):\n",
    "        tokenized = self.desc_tokenizer(texts,\n",
    "                                        truncation=True,\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=max_len,\n",
    "                                        padding='max_length',\n",
    "                                        return_attention_mask=True,\n",
    "                                        return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "\n",
    "class SiameseRuCLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df=None, labels=None, df_path=None, images_dir=DATA_PATH+'images/'):\n",
    "        # loads data either from path using `df_path` or directly from `df` argument\n",
    "        self.df = pd.read_csv(df_path) if df_path is not None else df\n",
    "        self.labels = labels\n",
    "        self.images_dir = images_dir\n",
    "        self.tokenizers = Tokenizers()\n",
    "        self.transform = get_transform()\n",
    "        # \n",
    "        self.max_len = 77\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        name_tokens = self.tokenizers.tokenize_name([str(row.name_first), \n",
    "                                               str(row.name_second)], max_len=self.max_len)\n",
    "        name_first = name_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        name_second = name_tokens[:, 1, :]\n",
    "        desc_tokens = self.tokenizers.tokenize_description([str(row.description_first), \n",
    "                                               str(row.description_second)])\n",
    "        desc_first = desc_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        desc_second = desc_tokens[:, 1, :]\n",
    "        im_first = cv2.imread(os.path.join(self.images_dir, row.image_name_first))\n",
    "        im_first = cv2.cvtColor(im_first, cv2.COLOR_BGR2RGB)\n",
    "        im_first = Image.fromarray(im_first)\n",
    "        im_first = self.transform(im_first)\n",
    "        im_second = cv2.imread(os.path.join(self.images_dir, row.image_name_second))\n",
    "        im_second = cv2.cvtColor(im_second, cv2.COLOR_BGR2RGB)\n",
    "        im_second = Image.fromarray(im_second)\n",
    "        im_second = self.transform(im_second)\n",
    "        label = self.labels[idx]\n",
    "        return im_first, name_first, desc_first, im_second, name_second, desc_second, label\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2d263be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "class SiameseRuCLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 device: str,\n",
    "                 name_model_name: str,\n",
    "                 description_model_name: str,\n",
    "                 preload_model_name: str = None,\n",
    "                 models_dir: str = None,\n",
    "                 dropout: float = None):\n",
    "        \"\"\"\n",
    "        Initializes the SiameseRuCLIP model.\n",
    "        Required parameters:\n",
    "          - models_dir: directory containing saved checkpoints.\n",
    "          - name_model_name: model name for text (name) branch.\n",
    "          - description_model_name: model name for description branch.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        device = torch.device(device)\n",
    "\n",
    "        # Initialize RuCLIPtiny\n",
    "        self.ruclip = RuCLIPtiny(name_model_name)\n",
    "        if preload_model_name is not None:\n",
    "            std = torch.load(\n",
    "                os.path.join(models_dir, preload_model_name),\n",
    "                weights_only=True,\n",
    "                map_location=device\n",
    "            )\n",
    "            self.ruclip.load_state_dict(std)\n",
    "            self.ruclip.eval()\n",
    "        self.ruclip = self.ruclip.to(device)\n",
    "\n",
    "        # Initialize the description transformer\n",
    "        self.description_transformer = AutoModel.from_pretrained(description_model_name)\n",
    "        self.description_transformer = self.description_transformer.to(device)\n",
    "\n",
    "        # Determine dimensionality\n",
    "        vision_dim = self.ruclip.visual.num_features\n",
    "        name_dim = self.ruclip.final_ln.out_features\n",
    "        desc_dim = self.description_transformer.config.hidden_size\n",
    "        self.hidden_dim = vision_dim + name_dim + desc_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define MLP head with optional dropout\n",
    "        layers = [\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            *( [nn.Dropout(self.dropout)] if self.dropout is not None else [] ),\n",
    "            nn.Linear(self.hidden_dim // 2, self.hidden_dim // 4),\n",
    "        ]\n",
    "        self.head = nn.Sequential(*layers).to(device)\n",
    "\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.ruclip.encode_image(image)\n",
    "\n",
    "    def encode_name(self, name):\n",
    "        return self.ruclip.encode_text(name[:, 0, :], name[:, 1, :])\n",
    "\n",
    "    def encode_description(self, desc):\n",
    "        last_hidden_states = self.description_transformer(desc[:, 0, :], desc[:, 1, :]).last_hidden_state\n",
    "        attention_mask = desc[:, 1, :]\n",
    "        return average_pool(last_hidden_states, attention_mask)\n",
    "\n",
    "    def get_final_embedding(self, im, name, desc):\n",
    "        image_emb = self.encode_image(im)\n",
    "        name_emb = self.encode_name(name)\n",
    "        desc_emb = self.encode_description(desc)\n",
    "\n",
    "        # Concatenate the embeddings and forward through the head\n",
    "        combined_emb = torch.cat([image_emb, name_emb, desc_emb], dim=1)\n",
    "        final_embedding = self.head(combined_emb)\n",
    "        return final_embedding\n",
    "\n",
    "    def forward(self, im1, name1, desc1, im2, name2, desc2):\n",
    "        out1 = self.get_final_embedding(im1, name1, desc1)\n",
    "        out2 = self.get_final_embedding(im2, name2, desc2)\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36cc0798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "# Remove pos_weight from ContrastiveLoss\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin: float = 1.5):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        d = F.pairwise_distance(output1, output2)\n",
    "        pos = (1 - label) * d.pow(2)\n",
    "        neg = label * F.relu(self.margin - d).pow(2)\n",
    "        return (pos + neg).mean()  # No weighting needed with balanced data\n",
    "\n",
    "# Use WeightedRandomSampler for training\n",
    "def create_balanced_train_loader(train_dataset, batch_size=32):\n",
    "    \"\"\"Create balanced training loader using weighted sampling.\"\"\"\n",
    "    labels = [pair['label'] for pair in train_dataset.pairs]\n",
    "    class_counts = np.bincount(labels)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = [class_weights[label] for label in labels]\n",
    "    \n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9c6281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pair(output1, output2, target, threshold):\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    # меньше границы, там где будет True — конкуренты\n",
    "    cond = euclidean_distance < threshold\n",
    "    pos_sum = 0\n",
    "    neg_sum = 0\n",
    "    pos_acc = 0\n",
    "    neg_acc = 0\n",
    "\n",
    "    for i in range(len(cond)):\n",
    "        # 1 значит не конкуренты\n",
    "        if target[i]:\n",
    "            neg_sum+=1\n",
    "            # 0 в cond значит дальше друг от друга чем threshold\n",
    "            if not cond[i]:\n",
    "                neg_acc+=1\n",
    "        elif not target[i]:\n",
    "            pos_sum+=1\n",
    "            if cond[i]:\n",
    "                pos_acc+=1\n",
    "\n",
    "    return pos_acc, pos_sum, neg_acc, neg_sum\n",
    "\n",
    "def predict(out1, out2, threshold=CONTRASTIVE_THRESHOLD):\n",
    "    # вернёт 1 если похожи\n",
    "    return F.pairwise_distance(out1, out2) < threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481940bc",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226754b",
   "metadata": {},
   "source": [
    "## Download data from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b8cde88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f079e0236f4410790b404bc048e684a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/anton/marketplace/clip-siamese/data'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download models' weights & text/image datasets\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ID = \"INDEEPA/clip-siamese\"\n",
    "LOCAL_DIR = Path(\"data/train_results\")\n",
    "LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type='dataset',\n",
    "    local_dir='data',\n",
    "    allow_patterns=[\n",
    "        \"train_results/cc12m*.pt\",\n",
    "        SOURCE_TABLE_NAME, PAIRWISE_TABLE_NAME,\n",
    "        f\"{IMG_DATASET_NAME}.zip\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# The following shell command was removed for script compatibility:\n",
    "# !unzip -n -q data/{IMG_DATASET_NAME}.zip -d data/\n",
    "\n",
    "# If you need to unzip in Python, use:\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile(f\"data/{IMG_DATASET_NAME}.zip\", 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be5962fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_df = pd.read_csv(DATA_PATH + SOURCE_TABLE_NAME)\n",
    "pairwise_mapping_df = pd.read_parquet(DATA_PATH + PAIRWISE_TABLE_NAME)\n",
    "pairwise_mapping_df.sku_query.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70179151",
   "metadata": {},
   "source": [
    "# Cluster soft negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c67ed0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggest the correct path to the embedding file based on the context and previous file saving logic\n",
    "# CHOSEN_EMBEDDING_FILE = 'embeddings/OZ_geo_5500/OZ_geo_5500_name-and-description_embeddings_num-rows=2.parquet'\n",
    "CHOSEN_EMBEDDING_FILE = 'embeddings/OZ_geo_5500/OZ_geo_5500_name-and-description_embeddings_num-rows=5562.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3586cf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded embedding file to:\n",
      "data/embeddings/OZ_geo_5500/OZ_geo_5500_name-and-description_embeddings_num-rows=5562.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>name_desc_emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1871769771</td>\n",
       "      <td>[-0.020089346915483475, -0.05487045273184776, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1679550303</td>\n",
       "      <td>[-0.00418242160230875, -0.04088427498936653, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1200553001</td>\n",
       "      <td>[-0.023978281766176224, -0.05447990819811821, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>922231521</td>\n",
       "      <td>[-0.024106157943606377, -0.053567297756671906,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>922230517</td>\n",
       "      <td>[-0.02229023538529873, -0.05309479311108589, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sku                                      name_desc_emb\n",
       "0  1871769771  [-0.020089346915483475, -0.05487045273184776, ...\n",
       "1  1679550303  [-0.00418242160230875, -0.04088427498936653, 0...\n",
       "2  1200553001  [-0.023978281766176224, -0.05447990819811821, ...\n",
       "3   922231521  [-0.024106157943606377, -0.053567297756671906,...\n",
       "4   922230517  [-0.02229023538529873, -0.05309479311108589, -..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "\n",
    "# Download the chosen embedding file from HuggingFace Hub to DATA_PATH\n",
    "from pathlib import Path\n",
    "\n",
    "downloaded_emb_file = hf_hub_download(\n",
    "    repo_id=\"INDEEPA/clip-siamese\",\n",
    "    repo_type=\"dataset\",\n",
    "    filename=CHOSEN_EMBEDDING_FILE,\n",
    "    local_dir=DATA_PATH,\n",
    ")\n",
    "\n",
    "print(f\"Downloaded embedding file to:\\n{downloaded_emb_file}\")\n",
    "emb_table = pd.read_parquet(downloaded_emb_file)\n",
    "emb_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6d4a152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster label counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>cluster</th>\n",
       "      <th>-1</th>\n",
       "      <th>396</th>\n",
       "      <th>1</th>\n",
       "      <th>98</th>\n",
       "      <th>216</th>\n",
       "      <th>389</th>\n",
       "      <th>383</th>\n",
       "      <th>159</th>\n",
       "      <th>495</th>\n",
       "      <th>269</th>\n",
       "      <th>...</th>\n",
       "      <th>429</th>\n",
       "      <th>418</th>\n",
       "      <th>12</th>\n",
       "      <th>303</th>\n",
       "      <th>138</th>\n",
       "      <th>424</th>\n",
       "      <th>373</th>\n",
       "      <th>490</th>\n",
       "      <th>428</th>\n",
       "      <th>459</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1244</td>\n",
       "      <td>64</td>\n",
       "      <td>47</td>\n",
       "      <td>42</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 498 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "cluster  -1     396   1     98    216   389   383   159   495   269  ...  \\\n",
       "count    1244    64    47    42    38    30    29    27    26    25  ...   \n",
       "\n",
       "cluster   429   418   12    303   138   424   373   490   428   459  \n",
       "count       5     5     5     5     5     5     5     5     5     5  \n",
       "\n",
       "[1 rows x 498 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Prepare the embeddings as a numpy array\n",
    "embeddings = np.stack(emb_table['name_desc_emb'].values)\n",
    "\n",
    "# Run HDBSCAN clustering using sklearn's implementation\n",
    "# Use coarser clustering: increase min_cluster_size and set min_samples for more robust, larger clusters\n",
    "clusterer = HDBSCAN(\n",
    "    min_samples=2,\n",
    "    # min_cluster_size=20,\n",
    "    # min_samples=10,\n",
    "    metric='cosine'\n",
    ")\n",
    "cluster_labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "# Add cluster labels to the emb_table and assign to cluster_emb_table\n",
    "cluster_emb_table = emb_table.copy()\n",
    "cluster_emb_table['cluster_id'] = cluster_labels\n",
    "\n",
    "# Print cluster label counts\n",
    "print(\"Cluster label counts:\")\n",
    "display(cluster_emb_table['cluster_id'].value_counts().to_frame().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1267723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster IDs with size > 10:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>cluster</th>\n",
       "      <th>-1</th>\n",
       "      <th>396</th>\n",
       "      <th>1</th>\n",
       "      <th>98</th>\n",
       "      <th>216</th>\n",
       "      <th>389</th>\n",
       "      <th>383</th>\n",
       "      <th>159</th>\n",
       "      <th>495</th>\n",
       "      <th>269</th>\n",
       "      <th>...</th>\n",
       "      <th>440</th>\n",
       "      <th>374</th>\n",
       "      <th>414</th>\n",
       "      <th>27</th>\n",
       "      <th>445</th>\n",
       "      <th>69</th>\n",
       "      <th>355</th>\n",
       "      <th>301</th>\n",
       "      <th>182</th>\n",
       "      <th>207</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1244</td>\n",
       "      <td>64</td>\n",
       "      <td>47</td>\n",
       "      <td>42</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "cluster  -1     396   1     98    216   389   383   159   495   269  ...  \\\n",
       "count    1244    64    47    42    38    30    29    27    26    25  ...   \n",
       "\n",
       "cluster   440   374   414   27    445   69    355   301   182   207  \n",
       "count      11    11    11    11    11    11    11    11    11    11  \n",
       "\n",
       "[1 rows x 99 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print cluster ids with size > N\n",
    "N = 10  # You can change N to any desired threshold\n",
    "cluster_counts = cluster_emb_table['cluster_id'].value_counts()\n",
    "large_clusters = cluster_counts[cluster_counts > N].to_frame()\n",
    "print(f\"Cluster IDs with size > {N}:\")\n",
    "display(large_clusters.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27a7575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKUs in cluster 80:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1707837031,\n",
       " 1706808656,\n",
       " 1706808406,\n",
       " 1589325642,\n",
       " 1589325623,\n",
       " 1589325615,\n",
       " 1589324360,\n",
       " 1589323257,\n",
       " 1589310326]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print SKUs for a given CLUSTER_ID\n",
    "CLUSTER_ID = 80  # Change this to the desired cluster id\n",
    "\n",
    "skus_in_cluster = cluster_emb_table.loc[cluster_emb_table['cluster_id'] == CLUSTER_ID, 'sku']\n",
    "print(f\"SKUs in cluster {CLUSTER_ID}:\")\n",
    "display(skus_in_cluster.tolist()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5dda22",
   "metadata": {},
   "source": [
    "# Make pairwise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9fc5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_query_groups\n",
    "\n",
    "def split_query_groups(\n",
    "    mapping_df: pd.DataFrame,\n",
    "    test_size: float = 0.2,\n",
    "    val_size: float = 0.05,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    For each query SKU group, splits:\n",
    "      - all query SKUs into test split (always, not by %)\n",
    "      - positives, hard negatives, soft negatives (excluding query SKU) into test/val/train\n",
    "    Returns: dict with keys 'train', 'val', 'test', each a DataFrame with columns:\n",
    "      ['sku_query', 'split', 'sku_pos', 'sku_hard_neg', 'sku_soft_neg']\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    split_rows = []\n",
    "\n",
    "    for _, row in mapping_df.iterrows():\n",
    "        q = row['sku_query']\n",
    "        pos = set(row['sku_pos']) - {q}\n",
    "        hard_neg = set(row['sku_hard_neg']) - {q}\n",
    "        soft_neg = set(row['sku_soft_neg']) - {q}\n",
    "\n",
    "        def split_list(lst, test_frac, val_frac):\n",
    "            lst = np.array(list(lst))\n",
    "            n = len(lst)\n",
    "            n_test = int(np.ceil(test_frac * n))\n",
    "            n_val = int(np.ceil(val_frac * n))\n",
    "            idx = rng.permutation(n)\n",
    "            test_idx = idx[:n_test]\n",
    "            val_idx = idx[n_test:n_test+n_val]\n",
    "            train_idx = idx[n_test+n_test+n_val:]\n",
    "            return lst[train_idx].tolist(), lst[val_idx].tolist(), lst[test_idx].tolist()\n",
    "\n",
    "        pos_train, pos_val, pos_test = split_list(pos, test_size, val_size)\n",
    "        hard_train, hard_val, hard_test = split_list(hard_neg, test_size, val_size)\n",
    "        soft_train, soft_val, soft_test = split_list(soft_neg, test_size, val_size)\n",
    "\n",
    "        # Only add the actual splits, not the empty test row\n",
    "        split_rows.append({\n",
    "            'sku_query': q,\n",
    "            'split': 'train',\n",
    "            'sku_pos': pos_train,\n",
    "            'sku_hard_neg': hard_train,\n",
    "            'sku_soft_neg': soft_train\n",
    "        })\n",
    "        split_rows.append({\n",
    "            'sku_query': q,\n",
    "            'split': 'val',\n",
    "            'sku_pos': pos_val,\n",
    "            'sku_hard_neg': hard_val,\n",
    "            'sku_soft_neg': soft_val\n",
    "        })\n",
    "        split_rows.append({\n",
    "            'sku_query': q,\n",
    "            'split': 'test',\n",
    "            'sku_pos': pos_test,\n",
    "            'sku_hard_neg': hard_test,\n",
    "            'sku_soft_neg': soft_test\n",
    "        })\n",
    "\n",
    "    split_df = pd.DataFrame(split_rows)\n",
    "    split_dict = {\n",
    "        split: split_df[split_df['split'] == split].reset_index(drop=True)\n",
    "        for split in ['train', 'val', 'test']\n",
    "    }\n",
    "    return split_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c7297a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check positives intersections\n",
    "# # TODO: merge sku having many intersections; disentangle common positives for skus having small common number of positives\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # Build a list of queries and a mapping from query to set of positives\n",
    "# queries = pairwise_mapping_df['sku_query'].tolist()\n",
    "# sku_to_pos = {\n",
    "#     row['sku_query']: set(row['sku_pos'])\n",
    "#     for _, row in pairwise_mapping_df.iterrows()\n",
    "# }\n",
    "\n",
    "# num_queries = len(queries)\n",
    "# intersection_matrix = np.zeros((num_queries, num_queries), dtype=int)\n",
    "\n",
    "# for i, q1 in enumerate(queries):\n",
    "#     pos1 = sku_to_pos[q1]\n",
    "#     for j, q2 in enumerate(queries):\n",
    "#         pos2 = sku_to_pos[q2]\n",
    "#         intersection_matrix[i, j] = len(pos1 & pos2)\n",
    "\n",
    "# # Mask diagonal and above with -1\n",
    "# mask = np.triu(np.ones_like(intersection_matrix, dtype=bool))\n",
    "# intersection_matrix[mask] = -1\n",
    "\n",
    "# # Optionally, wrap as DataFrame for readability\n",
    "# intersection_df = pd.DataFrame(\n",
    "#     intersection_matrix, \n",
    "#     index=queries, \n",
    "#     columns=queries\n",
    "# )\n",
    "# intersection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29910c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_query</th>\n",
       "      <th>split</th>\n",
       "      <th>sku_pos</th>\n",
       "      <th>sku_hard_neg</th>\n",
       "      <th>sku_soft_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1871769771</td>\n",
       "      <td>test</td>\n",
       "      <td>[467420540]</td>\n",
       "      <td>[1418084594, 1573142945, 1536520050, 1573135817]</td>\n",
       "      <td>[1899881468, 1290396077, 1597431764, 165269677...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sku_query split      sku_pos  \\\n",
       "0  1871769771  test  [467420540]   \n",
       "\n",
       "                                       sku_hard_neg  \\\n",
       "0  [1418084594, 1573142945, 1536520050, 1573135817]   \n",
       "\n",
       "                                        sku_soft_neg  \n",
       "0  [1899881468, 1290396077, 1597431764, 165269677...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_dataset = split_query_groups(\n",
    "    pairwise_mapping_df,\n",
    "    test_size=0.1,\n",
    "    val_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "splits_dataset['test'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32c58d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_pos_skus</th>\n",
       "      <th>unique_hard_neg_skus</th>\n",
       "      <th>unique_soft_neg_skus</th>\n",
       "      <th>unique_total_skus</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>547</td>\n",
       "      <td>1550</td>\n",
       "      <td>5562</td>\n",
       "      <td>5562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>121</td>\n",
       "      <td>449</td>\n",
       "      <td>4795</td>\n",
       "      <td>4898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>124</td>\n",
       "      <td>444</td>\n",
       "      <td>4802</td>\n",
       "      <td>4917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_pos_skus  unique_hard_neg_skus  unique_soft_neg_skus  \\\n",
       "split                                                                \n",
       "train              547                  1550                  5562   \n",
       "val                121                   449                  4795   \n",
       "test               124                   444                  4802   \n",
       "\n",
       "       unique_total_skus  \n",
       "split                     \n",
       "train               5562  \n",
       "val                 4898  \n",
       "test                4917  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display stats for each split: number of unique SKUs in pos/hard/soft/total as a table\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "split_stats = []\n",
    "for split_name, split_df in splits_dataset.items():\n",
    "    pos_skus = set()\n",
    "    hard_neg_skus = set()\n",
    "    soft_neg_skus = set()\n",
    "    for _, row in split_df.iterrows():\n",
    "        if isinstance(row['sku_pos'], list):\n",
    "            pos_skus.update(row['sku_pos'])\n",
    "        if isinstance(row['sku_hard_neg'], list):\n",
    "            hard_neg_skus.update(row['sku_hard_neg'])\n",
    "        if isinstance(row['sku_soft_neg'], list):\n",
    "            soft_neg_skus.update(row['sku_soft_neg'])\n",
    "    total_skus = pos_skus | hard_neg_skus | soft_neg_skus\n",
    "    split_stats.append({\n",
    "        \"split\": split_name,\n",
    "        \"unique_pos_skus\": len(pos_skus),\n",
    "        \"unique_hard_neg_skus\": len(hard_neg_skus),\n",
    "        \"unique_soft_neg_skus\": len(soft_neg_skus),\n",
    "        \"unique_total_skus\": len(total_skus)\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(split_stats).set_index(\"split\")\n",
    "display(stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2368207b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of pos/hard/soft/total per query per split:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_pos_per_query</th>\n",
       "      <th>avg_hard_neg_per_query</th>\n",
       "      <th>avg_soft_neg_per_query</th>\n",
       "      <th>avg_total_per_query</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>40.15</td>\n",
       "      <td>174.60</td>\n",
       "      <td>3673.35</td>\n",
       "      <td>3888.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>6.50</td>\n",
       "      <td>25.70</td>\n",
       "      <td>525.45</td>\n",
       "      <td>557.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>6.50</td>\n",
       "      <td>25.75</td>\n",
       "      <td>525.45</td>\n",
       "      <td>557.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       avg_pos_per_query  avg_hard_neg_per_query  avg_soft_neg_per_query  \\\n",
       "split                                                                      \n",
       "train              40.15                  174.60                 3673.35   \n",
       "val                 6.50                   25.70                  525.45   \n",
       "test                6.50                   25.75                  525.45   \n",
       "\n",
       "       avg_total_per_query  \n",
       "split                       \n",
       "train              3888.10  \n",
       "val                 557.65  \n",
       "test                557.70  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of pos/hard/soft/total per split:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_pos</th>\n",
       "      <th>total_hard_neg</th>\n",
       "      <th>total_soft_neg</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>803</td>\n",
       "      <td>3492</td>\n",
       "      <td>73467</td>\n",
       "      <td>77762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>130</td>\n",
       "      <td>514</td>\n",
       "      <td>10509</td>\n",
       "      <td>11153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>130</td>\n",
       "      <td>515</td>\n",
       "      <td>10509</td>\n",
       "      <td>11154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_pos  total_hard_neg  total_soft_neg  total\n",
       "split                                                  \n",
       "train        803            3492           73467  77762\n",
       "val          130             514           10509  11153\n",
       "test         130             515           10509  11154"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print_split_summary\n",
    "\n",
    "def print_split_summary(split_dict):\n",
    "    summary = []\n",
    "    for split_name, split_df in split_dict.items():\n",
    "        pos_counts = split_df['sku_pos'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        hard_counts = split_df['sku_hard_neg'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        soft_counts = split_df['sku_soft_neg'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        total_per_query = pos_counts + hard_counts + soft_counts\n",
    "        summary.append({\n",
    "            'split': split_name,\n",
    "            'avg_pos_per_query': pos_counts.mean(),\n",
    "            'avg_hard_neg_per_query': hard_counts.mean(),\n",
    "            'avg_soft_neg_per_query': soft_counts.mean(),\n",
    "            'avg_total_per_query': total_per_query.mean(),\n",
    "            'total_pos': pos_counts.sum(),\n",
    "            'total_hard_neg': hard_counts.sum(),\n",
    "            'total_soft_neg': soft_counts.sum(),\n",
    "            'total': pos_counts.sum() + hard_counts.sum() + soft_counts.sum(),\n",
    "            'num_queries': len(split_df)\n",
    "        })\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(summary).set_index('split')\n",
    "    print(\"Average number of pos/hard/soft/total per query per split:\")\n",
    "    display(df[['avg_pos_per_query', 'avg_hard_neg_per_query', 'avg_soft_neg_per_query', 'avg_total_per_query']])\n",
    "    print(\"\\nTotal number of pos/hard/soft/total per split:\")\n",
    "    display(df[['total_pos', 'total_hard_neg', 'total_soft_neg', 'total']])\n",
    "\n",
    "print_split_summary(splits_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3f9fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ClusterBasedPairwiseDataset(Dataset):\n",
    "    def __init__(self, split_df, source_df, cluster_emb_table=None, images_dir=None,\n",
    "                 n_pos=None, n_hard_neg=5, n_soft_neg=5, use_all_pos=True,\n",
    "                 cluster_sampling=True, n_soft_neg_per_cluster=1, lazy_loading=True):\n",
    "        \"\"\"\n",
    "        Memory-optimized dataset with lazy loading option.\n",
    "        \n",
    "        Args:\n",
    "            lazy_loading: If True, generate pairs on-the-fly instead of pre-generating all\n",
    "            n_pos: Number of positives to sample (None = use all if use_all_pos=True)\n",
    "            n_hard_neg: Number of hard negatives to sample (None = use all available)\n",
    "            n_soft_neg: Number of soft negatives to sample (None = use all available)\n",
    "        \"\"\"\n",
    "        self._current_idx = 0  # Track current index for evaluation\n",
    "        self.split_df = split_df.reset_index(drop=True)\n",
    "        self.source_df = source_df.set_index('sku')\n",
    "        self.images_dir = images_dir\n",
    "        self.cluster_emb_table = cluster_emb_table\n",
    "        self.n_pos = n_pos\n",
    "        self.n_hard_neg = n_hard_neg\n",
    "        self.n_soft_neg = n_soft_neg\n",
    "        self.use_all_pos = use_all_pos\n",
    "        self.cluster_sampling = cluster_sampling\n",
    "        self.n_soft_neg_per_cluster = n_soft_neg_per_cluster\n",
    "        self.lazy_loading = lazy_loading\n",
    "        \n",
    "        # Pre-compute cluster mappings if cluster sampling is enabled\n",
    "        if self.cluster_sampling and cluster_emb_table is not None:\n",
    "            self._build_cluster_mappings()\n",
    "        \n",
    "        if self.lazy_loading:\n",
    "            # Only generate pair indices/metadata\n",
    "            self._generate_pair_indices()\n",
    "        else:\n",
    "            # Pre-generate all pairs (original behavior)\n",
    "            self._generate_all_pairs()\n",
    "\n",
    "    def _build_cluster_mappings(self):\n",
    "        \"\"\"Build mappings from cluster to SKUs for efficient sampling.\"\"\"\n",
    "        self.sku_to_cluster = dict(zip(self.cluster_emb_table['sku'], self.cluster_emb_table['cluster_id']))\n",
    "        self.cluster_to_skus = {}\n",
    "        for _, row in self.cluster_emb_table.iterrows():\n",
    "            cluster = row['cluster_id']\n",
    "            sku = row['sku']\n",
    "            if cluster not in self.cluster_to_skus:\n",
    "                self.cluster_to_skus[cluster] = []\n",
    "            self.cluster_to_skus[cluster].append(sku)\n",
    "\n",
    "    def _generate_pair_indices(self):\n",
    "        \"\"\"Generate only the metadata needed to create pairs on-the-fly.\"\"\"\n",
    "        self.pair_indices = []\n",
    "        \n",
    "        for split_idx, row in self.split_df.iterrows():\n",
    "            q = row['sku_query']\n",
    "            pos = row['sku_pos'] if isinstance(row['sku_pos'], list) else []\n",
    "            hard_neg = row['sku_hard_neg'] if isinstance(row['sku_hard_neg'], list) else []\n",
    "            soft_neg = row['sku_soft_neg'] if isinstance(row['sku_soft_neg'], list) else []\n",
    "\n",
    "            # Store metadata for on-the-fly generation\n",
    "            self.pair_indices.append({\n",
    "                'type': 'query_data',\n",
    "                'split_idx': split_idx,\n",
    "                'query_sku': q,\n",
    "                'pos_skus': pos,\n",
    "                'hard_neg_skus': hard_neg,\n",
    "                'soft_neg_skus': soft_neg\n",
    "            })\n",
    "\n",
    "    def _sample_soft_negatives_by_cluster(self, soft_neg_skus):\n",
    "        \"\"\"Sample soft negatives from each cluster separately.\"\"\"\n",
    "        if not self.cluster_sampling or self.cluster_emb_table is None:\n",
    "            # If n_soft_neg is None, use all available\n",
    "            if self.n_soft_neg is None:\n",
    "                return soft_neg_skus\n",
    "            elif len(soft_neg_skus) <= self.n_soft_neg:\n",
    "                return soft_neg_skus\n",
    "            else:\n",
    "                return np.random.choice(soft_neg_skus, size=self.n_soft_neg, replace=False).tolist()\n",
    "        \n",
    "        # Group soft negatives by cluster\n",
    "        cluster_groups = {}\n",
    "        for sku in soft_neg_skus:\n",
    "            cluster = self.sku_to_cluster.get(sku, -1)\n",
    "            if cluster not in cluster_groups:\n",
    "                cluster_groups[cluster] = []\n",
    "            cluster_groups[cluster].append(sku)\n",
    "        \n",
    "        # Sample from each cluster\n",
    "        sampled_soft_negs = []\n",
    "        for cluster, skus in cluster_groups.items():\n",
    "            if len(skus) == 0:\n",
    "                continue\n",
    "            \n",
    "            n_to_sample = min(self.n_soft_neg_per_cluster, len(skus))\n",
    "            if n_to_sample > 0:\n",
    "                if len(skus) <= n_to_sample:\n",
    "                    sampled_soft_negs.extend(skus)\n",
    "                else:\n",
    "                    sampled_soft_negs.extend(np.random.choice(skus, size=n_to_sample, replace=False))\n",
    "        \n",
    "        # If n_soft_neg is None, return all sampled\n",
    "        if self.n_soft_neg is None:\n",
    "            return sampled_soft_negs\n",
    "            \n",
    "        # If we need more samples to reach n_soft_neg, sample randomly from all\n",
    "        remaining_needed = self.n_soft_neg - len(sampled_soft_negs)\n",
    "        if remaining_needed > 0 and soft_neg_skus:\n",
    "            remaining_skus = [sku for sku in soft_neg_skus if sku not in sampled_soft_negs]\n",
    "            if remaining_skus:\n",
    "                additional_samples = min(remaining_needed, len(remaining_skus))\n",
    "                if len(remaining_skus) <= additional_samples:\n",
    "                    sampled_soft_negs.extend(remaining_skus)\n",
    "                else:\n",
    "                    sampled_soft_negs.extend(np.random.choice(remaining_skus, size=additional_samples, replace=False))\n",
    "        \n",
    "        return sampled_soft_negs[:self.n_soft_neg] if self.n_soft_neg is not None else sampled_soft_negs\n",
    "\n",
    "    def _generate_all_pairs(self):\n",
    "        \"\"\"Pre-generate all pairs for the dataset (memory-intensive but faster access).\"\"\"\n",
    "        self.pairs = []\n",
    "        \n",
    "        for _, row in self.split_df.iterrows():\n",
    "            q = row['sku_query']\n",
    "            pos = row['sku_pos'] if isinstance(row['sku_pos'], list) else []\n",
    "            hard_neg = row['sku_hard_neg'] if isinstance(row['sku_hard_neg'], list) else []\n",
    "            soft_neg = row['sku_soft_neg'] if isinstance(row['sku_soft_neg'], list) else []\n",
    "\n",
    "            # Sample positives\n",
    "            if self.use_all_pos and self.n_pos is None:\n",
    "                pos_sample = pos\n",
    "            else:\n",
    "                n_pos_actual = self.n_pos if self.n_pos is not None else len(pos)\n",
    "                if len(pos) <= n_pos_actual:\n",
    "                    pos_sample = pos\n",
    "                else:\n",
    "                    pos_sample = np.random.choice(pos, size=n_pos_actual, replace=False).tolist()\n",
    "\n",
    "            # Sample hard negatives - handle None case\n",
    "            if self.n_hard_neg is None:\n",
    "                # Use all available hard negatives\n",
    "                hard_sample = hard_neg\n",
    "            elif len(hard_neg) <= self.n_hard_neg:\n",
    "                hard_sample = hard_neg\n",
    "            else:\n",
    "                hard_sample = np.random.choice(hard_neg, size=self.n_hard_neg, replace=False).tolist()\n",
    "\n",
    "            # Sample soft negatives with cluster-based sampling - handle None case\n",
    "            soft_sample = self._sample_soft_negatives_by_cluster(soft_neg)\n",
    "\n",
    "            # Create pairs\n",
    "            for pos_sku in pos_sample:\n",
    "                self.pairs.append({\n",
    "                    'sku_first': q,\n",
    "                    'sku_second': pos_sku,\n",
    "                    'label': 0\n",
    "                })\n",
    "\n",
    "            for hard_sku in hard_sample:\n",
    "                self.pairs.append({\n",
    "                    'sku_first': q,\n",
    "                    'sku_second': hard_sku,\n",
    "                    'label': 1\n",
    "                })\n",
    "\n",
    "            for soft_sku in soft_sample:\n",
    "                self.pairs.append({\n",
    "                    'sku_first': q,\n",
    "                    'sku_second': soft_sku,\n",
    "                    'label': 1\n",
    "                })\n",
    "\n",
    "        # Convert to DataFrame and join with source data\n",
    "        if self.pairs:\n",
    "            pairs_df = pd.DataFrame(self.pairs)\n",
    "            \n",
    "            # Join with source_df to get all required columns\n",
    "            first_data = self.source_df.loc[pairs_df['sku_first']].reset_index()\n",
    "            first_data.columns = [f\"{col}_first\" if col != 'sku' else 'sku_first' for col in first_data.columns]\n",
    "            \n",
    "            second_data = self.source_df.loc[pairs_df['sku_second']].reset_index()\n",
    "            second_data.columns = [f\"{col}_second\" if col != 'sku' else 'sku_second' for col in second_data.columns]\n",
    "            \n",
    "            self.pairs_df = pd.concat([\n",
    "                first_data.reset_index(drop=True),\n",
    "                second_data.reset_index(drop=True),\n",
    "                pairs_df[['label']].reset_index(drop=True)\n",
    "            ], axis=1)\n",
    "            \n",
    "            self.siamese_dataset = SiameseRuCLIPDataset(\n",
    "                self.pairs_df.drop(columns=['label']),\n",
    "                self.pairs_df['label'].values,\n",
    "                images_dir=self.images_dir\n",
    "            )\n",
    "        else:\n",
    "            self.pairs_df = pd.DataFrame()\n",
    "            self.siamese_dataset = None\n",
    "\n",
    "    def _generate_pair_on_demand(self, idx):\n",
    "        \"\"\"Generate a single pair on-demand (for lazy loading).\"\"\"\n",
    "        # This is a simplified version - you'd need to implement the logic\n",
    "        # to map idx to a specific (query, positive/negative) combination\n",
    "        # and generate the pair data on-the-fly\n",
    "        # This is more complex to implement but saves memory\n",
    "        raise NotImplementedError(\"Lazy loading not fully implemented yet\")\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.lazy_loading:\n",
    "            # Calculate total pairs without generating them\n",
    "            total_pairs = 0\n",
    "            for _, row in self.split_df.iterrows():\n",
    "                pos_count = len(row['sku_pos']) if isinstance(row['sku_pos'], list) else 0\n",
    "                hard_count = len(row['sku_hard_neg']) if isinstance(row['sku_hard_neg'], list) else 0\n",
    "                soft_count = len(row['sku_soft_neg']) if isinstance(row['sku_soft_neg'], list) else 0\n",
    "                \n",
    "                # Apply limits only if they're not None\n",
    "                if self.n_hard_neg is not None:\n",
    "                    hard_count = min(hard_count, self.n_hard_neg)\n",
    "                if self.n_soft_neg is not None:\n",
    "                    soft_count = min(soft_count, self.n_soft_neg)\n",
    "                    \n",
    "                total_pairs += pos_count + hard_count + soft_count\n",
    "            return total_pairs\n",
    "        else:\n",
    "            return len(self.pairs) if self.pairs else 0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self._current_idx = idx  # Store current index\n",
    "        if self.lazy_loading:\n",
    "            return self._generate_pair_on_demand(idx)\n",
    "        else:\n",
    "            if self.siamese_dataset is None:\n",
    "                raise IndexError(\"No pairs available\")\n",
    "            return self.siamese_dataset[idx]\n",
    "    def get_pair_info(self, idx):\n",
    "        \"\"\"Get query and target SKU information for a given index.\"\"\"\n",
    "        pair = self.pairs[idx]\n",
    "        return {\n",
    "            'query_sku': pair['sku_first'],\n",
    "            'target_sku': pair['sku_second'],\n",
    "            'label': pair['label']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17b84c",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "485f83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_comprehensive_evaluation_limited(model, limited_batches, device, k_values=[1, 3, 5, 10]):\n",
    "    \"\"\"\n",
    "    Fast ranking evaluation on limited data for smoke testing.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        limited_batches: List of limited batches for evaluation\n",
    "        device: Device to run evaluation on\n",
    "        k_values: List of k values for precision@k and recall@k\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    model.eval()\n",
    "    all_data = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(limited_batches):\n",
    "            # Handle batch unpacking (with or without SKUs)\n",
    "            if len(batch) == 9:  # Enhanced dataset with SKUs\n",
    "                im1, n1, d1, im2, n2, d2, labels, query_skus, target_skus = batch\n",
    "                im1, n1, d1, im2, n2, d2, labels = [t.to(device) for t in [im1, n1, d1, im2, n2, d2, labels]]\n",
    "                query_skus = [str(sku) for sku in query_skus]\n",
    "                target_skus = [str(sku) for sku in target_skus]\n",
    "            else:  # Original dataset without SKUs\n",
    "                im1, n1, d1, im2, n2, d2, labels = [t.to(device) for t in batch]\n",
    "                batch_size = labels.size(0)\n",
    "                query_skus = [f\"query_{batch_idx}_{i}\" for i in range(batch_size)]\n",
    "                target_skus = [f\"target_{batch_idx}_{i}\" for i in range(batch_size)]\n",
    "\n",
    "            out1, out2 = model(im1, n1, d1, im2, n2, d2)\n",
    "            distances = F.pairwise_distance(out1, out2)\n",
    "            similarities = 1 / (1 + distances)\n",
    "            \n",
    "            # Convert to CPU for processing\n",
    "            distances_cpu = distances.cpu().numpy()\n",
    "            similarities_cpu = similarities.cpu().numpy()\n",
    "            labels_cpu = labels.cpu().numpy()\n",
    "            \n",
    "            # Store batch results\n",
    "            batch_size = len(labels_cpu)\n",
    "            for i in range(batch_size):\n",
    "                all_data.append({\n",
    "                    'query_sku': query_skus[i],\n",
    "                    'target_sku': target_skus[i],\n",
    "                    'distance': distances_cpu[i],\n",
    "                    'similarity': similarities_cpu[i],\n",
    "                    'label': labels_cpu[i]\n",
    "                })\n",
    "    \n",
    "    if not all_data:\n",
    "        # Return empty metrics if no data\n",
    "        return {\n",
    "            'mean_average_precision': 0.0,\n",
    "            'precision_at_k': {k: 0.0 for k in k_values},\n",
    "            'recall_at_k': {k: 0.0 for k in k_values},\n",
    "            'f1_score': 0.0,\n",
    "            'balanced_accuracy': 0.0,\n",
    "            'specificity': 0.0,\n",
    "            'optimal_threshold': 0.5\n",
    "        }\n",
    "    \n",
    "    # Convert to DataFrame for easier processing\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Compute global metrics\n",
    "    all_distances = df['distance'].values\n",
    "    all_labels = df['label'].values\n",
    "    all_similarities = df['similarity'].values\n",
    "    \n",
    "    # Convert to binary classification format (0 = positive/similar, 1 = negative/dissimilar)\n",
    "    y_true = (all_labels == 0).astype(int)\n",
    "    y_scores = all_similarities\n",
    "    \n",
    "    # Compute global metrics\n",
    "    if len(np.unique(y_true)) > 1:  # Check if we have both classes\n",
    "        try:\n",
    "            precision, recall, pr_thresholds = precision_recall_curve(y_true, y_scores)\n",
    "            ap_score = average_precision_score(y_true, y_scores)\n",
    "            \n",
    "            # Find optimal threshold based on F1 score\n",
    "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "            best_f1_idx = np.argmax(f1_scores)\n",
    "            \n",
    "            if len(pr_thresholds) > best_f1_idx:\n",
    "                best_threshold_sim = pr_thresholds[best_f1_idx]\n",
    "                best_threshold_dist = 1 / (1 + best_threshold_sim) - 1 if best_threshold_sim > 0 else 0.5\n",
    "            else:\n",
    "                best_threshold_dist = 0.5\n",
    "                \n",
    "            best_f1 = f1_scores[best_f1_idx]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error computing global metrics: {e}\")\n",
    "            ap_score = 0.0\n",
    "            best_f1 = 0.0\n",
    "            best_threshold_dist = 0.5\n",
    "    else:\n",
    "        ap_score = 0.0\n",
    "        best_f1 = 0.0\n",
    "        best_threshold_dist = 0.5\n",
    "    \n",
    "    # Compute confusion matrix at optimal threshold\n",
    "    y_pred = (all_distances < best_threshold_dist).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix components\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision_global = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall_global = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    balanced_accuracy = (recall_global + specificity) / 2\n",
    "    \n",
    "    # Compute ranking metrics using fast computation\n",
    "    precision_at_k, recall_at_k, mean_avg_precision = fast_ranking_metrics_limited(df, k_values)\n",
    "    \n",
    "    # Combine all metrics\n",
    "    metrics = {\n",
    "        # Global metrics\n",
    "        'average_precision': ap_score,\n",
    "        'precision': precision_global,\n",
    "        'recall': recall_global,\n",
    "        'f1_score': best_f1,\n",
    "        'specificity': specificity,\n",
    "        'balanced_accuracy': balanced_accuracy,\n",
    "        'optimal_threshold': best_threshold_dist,\n",
    "        'confusion_matrix': {'TP': int(tp), 'FP': int(fp), 'TN': int(tn), 'FN': int(fn)},\n",
    "        \n",
    "        # Ranking metrics\n",
    "        'mean_average_precision': mean_avg_precision,\n",
    "        'precision_at_k': precision_at_k,\n",
    "        'recall_at_k': recall_at_k,\n",
    "        'k_values': k_values\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def fast_ranking_metrics_limited(df, k_values):\n",
    "    \"\"\"\n",
    "    Compute ranking metrics efficiently from a DataFrame with limited data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['query_sku', 'similarity', 'label']\n",
    "        k_values: List of k values for precision@k and recall@k\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (precision_at_k_dict, recall_at_k_dict, mean_average_precision)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Initialize containers\n",
    "    precision_at_k = {k: [] for k in k_values}\n",
    "    recall_at_k = {k: [] for k in k_values}\n",
    "    average_precisions = []\n",
    "    \n",
    "    # Group by query\n",
    "    for query_sku in df['query_sku'].unique():\n",
    "        query_data = df[df['query_sku'] == query_sku].sort_values('similarity', ascending=False)\n",
    "        \n",
    "        # Get labels in ranked order (0 = positive, 1 = negative)\n",
    "        is_positive = (query_data['label'] == 0).values\n",
    "        total_positives = is_positive.sum()\n",
    "        \n",
    "        if total_positives == 0:\n",
    "            continue\n",
    "        \n",
    "        # Compute metrics for all k values at once\n",
    "        for k in k_values:\n",
    "            if len(is_positive) >= k:\n",
    "                top_k_positives = is_positive[:k].sum()\n",
    "                precision_at_k[k].append(top_k_positives / k)\n",
    "                recall_at_k[k].append(top_k_positives / total_positives)\n",
    "        \n",
    "        # Compute Average Precision efficiently\n",
    "        precision_values = []\n",
    "        num_positives = 0\n",
    "        for i, is_pos in enumerate(is_positive):\n",
    "            if is_pos:\n",
    "                num_positives += 1\n",
    "                precision_values.append(num_positives / (i + 1))\n",
    "        \n",
    "        if precision_values:\n",
    "            average_precisions.append(np.mean(precision_values))\n",
    "    \n",
    "    # Average across queries\n",
    "    avg_precision_at_k = {k: np.mean(v) if v else 0.0 for k, v in precision_at_k.items()}\n",
    "    avg_recall_at_k = {k: np.mean(v) if v else 0.0 for k, v in recall_at_k.items()}\n",
    "    mean_avg_precision = np.mean(average_precisions) if average_precisions else 0.0\n",
    "    \n",
    "    return avg_precision_at_k, avg_recall_at_k, mean_avg_precision\n",
    "\n",
    "\n",
    "def validation_with_embedding_collection(model, criterion, data_loader, epoch, \n",
    "                                        device='cpu', split_name='validation',\n",
    "                                        metric='f1', limit_batches=None, \n",
    "                                        collect_for_ranking=False, k_values=[1, 3, 5, 10]):\n",
    "    \"\"\"\n",
    "    Validation function that collects embeddings during loss computation for ranking metrics.\n",
    "    This REUSES embeddings computed for loss, avoiding duplicate forward passes.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        criterion: Loss function\n",
    "        data_loader: DataLoader for validation data\n",
    "        epoch: Current epoch number\n",
    "        device: Device to run on\n",
    "        split_name: Name of the split being evaluated\n",
    "        metric: Optimization metric ('f1', 'pos_acc', 'map')\n",
    "        limit_batches: Limit number of batches (for smoke testing)\n",
    "        collect_for_ranking: Whether to collect embeddings for ranking metrics\n",
    "        k_values: List of k values for ranking metrics\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (pos_acc, neg_acc, avg_acc, f1, avg_loss, best_thr, eval_metrics)\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    import torch.nn.functional as F\n",
    "    from sklearn.metrics import f1_score\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    assert metric in ('f1', 'pos_acc', 'map'), \"metric must be 'f1', 'pos_acc', or 'map'\"\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_distances, all_labels = [], []\n",
    "    \n",
    "    # For ranking metrics collection (when collect_for_ranking=True)\n",
    "    ranking_data = [] if collect_for_ranking else None\n",
    "    \n",
    "    # Calculate total batches\n",
    "    total_batches = limit_batches if limit_batches is not None else len(data_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Create data_iter BEFORE the loop\n",
    "        data_iter = enumerate(tqdm(data_loader, \n",
    "                                 desc=f\"{split_name.capitalize()}\", \n",
    "                                 unit=\"batch\", \n",
    "                                 total=total_batches))\n",
    "        \n",
    "        for batch_idx, batch in data_iter:  # Now data_iter is defined\n",
    "            if limit_batches is not None and batch_idx >= limit_batches:\n",
    "                break\n",
    "                \n",
    "            # Handle batch unpacking\n",
    "            if len(batch) == 9:  # Enhanced dataset with SKUs\n",
    "                im1, n1, d1, im2, n2, d2, lbl, query_skus, target_skus = batch\n",
    "                im1, n1, d1, im2, n2, d2, lbl = [t.to(device) for t in [im1, n1, d1, im2, n2, d2, lbl]]\n",
    "            else:  # Original dataset without SKUs\n",
    "                im1, n1, d1, im2, n2, d2, lbl = [t.to(device) for t in batch]\n",
    "                if collect_for_ranking:\n",
    "                    # 🔥 FIX: Use consistent query grouping instead of per-sample queries\n",
    "                    batch_size = lbl.size(0)\n",
    "                    # Group all samples in this batch under a single query for smoke testing\n",
    "                    query_skus = [f\"smoke_test_query\"] * batch_size  # ← All samples share same query\n",
    "                    target_skus = [f\"target_{batch_idx}_{i}\" for i in range(batch_size)]\n",
    "            \n",
    "            # SINGLE FORWARD PASS - compute embeddings once\n",
    "            out1, out2 = model(im1, n1, d1, im2, n2, d2)\n",
    "            \n",
    "            # Compute loss using the same embeddings\n",
    "            loss = criterion(out1, out2, lbl)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # REUSE the same embeddings for distance computation\n",
    "            distances = F.pairwise_distance(out1, out2)\n",
    "            \n",
    "            # Store for threshold optimization\n",
    "            all_distances.append(distances.detach().cpu())\n",
    "            all_labels.append(lbl.detach().cpu())\n",
    "            \n",
    "            # Collect data for ranking metrics if requested\n",
    "            if collect_for_ranking and ranking_data is not None:\n",
    "                similarities = 1 / (1 + distances)\n",
    "                distances_cpu = distances.cpu().numpy()\n",
    "                similarities_cpu = similarities.cpu().numpy()\n",
    "                labels_cpu = lbl.cpu().numpy()\n",
    "                \n",
    "                batch_size = len(labels_cpu)\n",
    "                for i in range(batch_size):\n",
    "                    ranking_data.append({\n",
    "                        'query_sku': query_skus[i] if len(batch) == 9 else f\"query_{batch_idx}_{i}\",\n",
    "                        'target_sku': target_skus[i] if len(batch) == 9 else f\"target_{batch_idx}_{i}\",\n",
    "                        'distance': distances_cpu[i],\n",
    "                        'similarity': similarities_cpu[i],\n",
    "                        'label': labels_cpu[i]\n",
    "                    })\n",
    "            \n",
    "            # Explicit cleanup\n",
    "            del im1, n1, d1, im2, n2, d2, lbl, out1, out2, loss, distances\n",
    "            if len(batch) == 9:\n",
    "                del query_skus, target_skus\n",
    "    \n",
    "    # Calculate metrics using actual number of processed batches\n",
    "    actual_batches = min(batch_idx + 1, limit_batches or len(data_loader))\n",
    "    avg_loss = total_loss / actual_batches\n",
    "    \n",
    "    # Concatenate all distances and labels\n",
    "    distances = torch.cat(all_distances)\n",
    "    labels = torch.cat(all_labels)\n",
    "    \n",
    "    # Clear the lists immediately to free memory\n",
    "    del all_distances, all_labels\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Threshold sweep for optimization\n",
    "    margin = 1.5\n",
    "    steps = 200\n",
    "    grid = np.linspace(0.0, margin, steps)\n",
    "    best_val, best_thr = -1.0, 0.0\n",
    "    y_true = (labels.numpy() == 0).astype(int)\n",
    "    \n",
    "    for t in grid:\n",
    "        y_pred = (distances.numpy() < t).astype(int)\n",
    "        if metric == 'f1':\n",
    "            val = f1_score(y_true, y_pred, zero_division=0)\n",
    "        else:  # metric == 'pos_acc'\n",
    "            pos_mask = (y_true == 1)\n",
    "            val = (y_pred[pos_mask] == 1).mean() if pos_mask.sum() > 0 else 0.0\n",
    "        if val > best_val:\n",
    "            best_val, best_thr = val, t\n",
    "    \n",
    "    threshold = best_thr\n",
    "    \n",
    "    # Final metrics at chosen threshold\n",
    "    preds = (distances < threshold).long()\n",
    "    pos_mask = (labels == 0)\n",
    "    neg_mask = (labels == 1)\n",
    "    \n",
    "    pos_acc = (preds[pos_mask] == 1).float().mean().item() if pos_mask.any() else 0.0\n",
    "    neg_acc = (preds[neg_mask] == 0).float().mean().item() if neg_mask.any() else 0.0\n",
    "    avg_acc = (pos_acc + neg_acc) / 2.0\n",
    "    f1 = f1_score((labels.numpy() == 0).astype(int), preds.numpy(), zero_division=0)\n",
    "    \n",
    "    # Compute ranking metrics if requested\n",
    "    if collect_for_ranking and ranking_data:\n",
    "        import pandas as pd\n",
    "        ranking_df = pd.DataFrame(ranking_data)\n",
    "        precision_at_k, recall_at_k, mean_avg_precision = fast_ranking_metrics_limited(ranking_df, k_values)\n",
    "        \n",
    "        eval_metrics = {\n",
    "            'mean_average_precision': mean_avg_precision,\n",
    "            'precision_at_k': precision_at_k,\n",
    "            'recall_at_k': recall_at_k,\n",
    "            'f1_score': f1,\n",
    "            'balanced_accuracy': avg_acc,\n",
    "            'specificity': neg_acc\n",
    "        }\n",
    "    else:\n",
    "        eval_metrics = {\n",
    "            'mean_average_precision': 0.0,\n",
    "            'precision_at_k': {k: 0.0 for k in k_values},\n",
    "            'recall_at_k': {k: 0.0 for k in k_values},\n",
    "            'f1_score': f1,\n",
    "            'balanced_accuracy': avg_acc,\n",
    "            'specificity': neg_acc\n",
    "        }\n",
    "    \n",
    "    # Final cleanup\n",
    "    del distances, labels, preds, pos_mask, neg_mask\n",
    "    if ranking_data:\n",
    "        del ranking_data\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Log to console\n",
    "    report = (f\"[{split_name}] Epoch {epoch} – \"\n",
    "              f\"loss: {avg_loss:.4f}, \"\n",
    "              f\"P Acc: {pos_acc:.3f}, \"\n",
    "              f\"N Acc: {neg_acc:.3f}, \"\n",
    "              f\"Avg Acc: {avg_acc:.3f}, \"\n",
    "              f\"F1: {f1:.3f}, \"\n",
    "              f\"thr*: {threshold:.3f} \"\n",
    "              f\"(optimised: {metric})\")\n",
    "    print(report)\n",
    "    \n",
    "    return pos_acc, neg_acc, avg_acc, f1, avg_loss, threshold, eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6695c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_smart_ranking(model, optimizer, criterion, num_epochs, \n",
    "                           train_loader, valid_loader=None, device='cpu',\n",
    "                           print_epoch=False, models_dir=None, metric='f1',\n",
    "                           limit_batches=None, \n",
    "                           compute_ranking_every_n_epochs=5,  # Only compute ranking periodically\n",
    "                           k_values=[1, 3, 5, 10]):\n",
    "    \"\"\"\n",
    "    Memory-optimized training with smart ranking metrics computation that REUSES embeddings.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        optimizer: Optimizer for training\n",
    "        criterion: Loss function\n",
    "        num_epochs: Number of epochs to train\n",
    "        train_loader: Training data loader\n",
    "        valid_loader: Validation data loader\n",
    "        device: Device to use ('cpu' or 'cuda')\n",
    "        print_epoch: Whether to print epoch information\n",
    "        models_dir: Directory to save model checkpoints\n",
    "        metric: Metric to optimize ('f1', 'pos_acc', 'map')\n",
    "        limit_batches: Limit batches for smoke testing\n",
    "        compute_ranking_every_n_epochs: How often to compute ranking metrics (0 = never, 1 = every epoch)\n",
    "        k_values: List of k values for precision@k and recall@k\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_losses, val_losses, best_valid_metric, best_weights, thr_history, ranking_history)\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    import psutil\n",
    "    from tqdm.auto import tqdm\n",
    "    from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "    from pathlib import Path\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    # Ensure epochs_num is an integer and properly limited\n",
    "    num_epochs = int(num_epochs)\n",
    "    assert num_epochs > 0, f\"epochs_num must be positive, got {num_epochs}\"\n",
    "    \n",
    "    print(f\"🔄 Starting training for {num_epochs} epochs\")\n",
    "    \n",
    "    assert metric in ('f1', 'pos_acc', 'map'), \"metric must be 'f1', 'pos_acc', or 'map'\"\n",
    "\n",
    "    def log_memory_usage(stage):\n",
    "        process = psutil.Process()\n",
    "        memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "        print(f\"Memory usage at {stage}: {memory_mb:.1f} MB\")\n",
    "\n",
    "    model.to(device)\n",
    "    train_losses, val_losses, thr_history = [], [], []\n",
    "    ranking_history = []\n",
    "    best_valid_metric, best_threshold = float('-inf'), None\n",
    "    best_weights = None\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"max\",\n",
    "        factor=0.1,\n",
    "        patience=3,\n",
    "        threshold=1e-4,\n",
    "        threshold_mode='rel'\n",
    "    )\n",
    "\n",
    "    if models_dir:\n",
    "        Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    log_memory_usage(\"training start\")\n",
    "    \n",
    "    # Create epoch progress bar with explicit range\n",
    "    epoch_range = list(range(1, num_epochs + 1))  # Convert to list to be explicit\n",
    "    epoch_pbar = tqdm(epoch_range, desc=\"Epochs\", unit=\"epoch\")\n",
    "    \n",
    "    epoch_counter = 0  # Add explicit counter for debugging\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        epoch_counter += 1\n",
    "        print(f\"🔸 Processing epoch {epoch}/{num_epochs} (counter: {epoch_counter})\")\n",
    "        \n",
    "        # Add safety check\n",
    "        if epoch_counter > num_epochs:\n",
    "            print(f\"⚠️ Safety break: epoch_counter ({epoch_counter}) > epochs_num ({num_epochs})\")\n",
    "            break\n",
    "        log_memory_usage(f\"epoch {epoch} start\")\n",
    "        \n",
    "        # ==================== TRAINING PHASE ====================\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        \n",
    "        # Create batch progress bar for training\n",
    "        train_total = limit_batches if limit_batches is not None else len(train_loader)\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch} - Training\", \n",
    "                         leave=False, unit=\"batch\", total=train_total)\n",
    "        \n",
    "        batch_count = 0\n",
    "        for batch_idx, batch in enumerate(train_pbar):\n",
    "            if limit_batches is not None and batch_idx >= limit_batches:\n",
    "                break\n",
    "                \n",
    "            # Handle batch unpacking (with or without SKUs)\n",
    "            if len(batch) == 9:  # Enhanced dataset with SKUs\n",
    "                im1, n1, d1, im2, n2, d2, lbl, query_skus, target_skus = batch\n",
    "                im1, n1, d1, im2, n2, d2, lbl = [t.to(device) for t in [im1, n1, d1, im2, n2, d2, lbl]]\n",
    "            else:  # Original dataset without SKUs\n",
    "                im1, n1, d1, im2, n2, d2, lbl = [t.to(device) for t in batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out1, out2 = model(im1, n1, d1, im2, n2, d2)\n",
    "            loss = criterion(out1, out2, lbl)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Explicit cleanup of batch variables\n",
    "            del im1, n1, d1, im2, n2, d2, lbl, out1, out2, loss\n",
    "            if len(batch) == 9:\n",
    "                del query_skus, target_skus\n",
    "            \n",
    "            train_pbar.set_postfix({'loss': f'{total_train_loss/batch_count:.4f}'})\n",
    "\n",
    "        train_pbar.close()\n",
    "        \n",
    "        # Calculate average loss using actual number of processed batches\n",
    "        avg_train_loss = total_train_loss / batch_count if batch_count > 0 else 0.0\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Memory cleanup after training phase\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        log_memory_usage(f\"epoch {epoch} after training\")\n",
    "\n",
    "        # ==================== VALIDATION PHASE ====================\n",
    "        if valid_loader is not None:\n",
    "            # Decide whether to compute ranking metrics\n",
    "            compute_ranking = (\n",
    "                compute_ranking_every_n_epochs > 0 and  # Ranking computation enabled\n",
    "                (epoch % compute_ranking_every_n_epochs == 0 or epoch == num_epochs)  # Periodic or final\n",
    "            )\n",
    "            \n",
    "            print(f\"Computing validation metrics (ranking: {compute_ranking})...\")\n",
    "            \n",
    "            # 🔥 KEY FIX: Use validation_with_embedding_collection instead of validation_with_optional_ranking\n",
    "            # This REUSES embeddings computed for loss computation!\n",
    "            pos_acc, neg_acc, avg_acc, f1, avg_val_loss, best_thr, eval_metrics = validation_with_embedding_collection(\n",
    "                model=model,\n",
    "                criterion=criterion,\n",
    "                data_loader=valid_loader,\n",
    "                epoch=epoch,\n",
    "                device=device,\n",
    "                split_name='validation',\n",
    "                metric=metric,\n",
    "                limit_batches=limit_batches,\n",
    "                collect_for_ranking=compute_ranking,  # ← Key parameter that enables embedding reuse!\n",
    "                k_values=k_values\n",
    "            )\n",
    "            \n",
    "            ranking_history.append(eval_metrics)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            thr_history.append(best_thr)\n",
    "\n",
    "            # Choose the metric value for optimization\n",
    "            if metric == 'f1':\n",
    "                current_metric = f1\n",
    "            elif metric == 'pos_acc':\n",
    "                current_metric = pos_acc\n",
    "            elif metric == 'map':\n",
    "                current_metric = eval_metrics.get('mean_average_precision', 0.0)\n",
    "            else:\n",
    "                current_metric = f1\n",
    "\n",
    "            scheduler.step(current_metric)\n",
    "\n",
    "            # Enhanced logging with ranking metrics when available\n",
    "            if compute_ranking and eval_metrics.get('mean_average_precision', 0) > 0:\n",
    "                print(f\"Epoch {epoch}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, \"\n",
    "                      f\"Val Loss: {avg_val_loss:.4f}, Val {metric}: {current_metric:.4f}\")\n",
    "                print(f\"  Ranking Metrics - MAP: {eval_metrics.get('mean_average_precision', 0):.4f}\")\n",
    "                \n",
    "                # Print P@k and R@k for selected k values\n",
    "                for k in [1, 3, 5]:  # Show only key metrics to avoid clutter\n",
    "                    if k in k_values and k in eval_metrics.get('precision_at_k', {}):\n",
    "                        p_k = eval_metrics['precision_at_k'].get(k, 0)\n",
    "                        r_k = eval_metrics['recall_at_k'].get(k, 0)\n",
    "                        print(f\"  P@{k}: {p_k:.3f}, R@{k}: {r_k:.3f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, \"\n",
    "                      f\"Val Loss: {avg_val_loss:.4f}, Val {metric}: {current_metric:.4f}, \"\n",
    "                      f\"Pos Acc: {pos_acc:.3f}, Neg Acc: {neg_acc:.3f}, F1: {f1:.3f}, \"\n",
    "                      f\"Threshold: {best_thr:.3f}\")\n",
    "\n",
    "            # Save checkpoint every epoch if requested\n",
    "            if models_dir:\n",
    "                checkpoint_path = Path(models_dir) / f\"checkpoint_epoch_{epoch}.pt\"\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_loss': avg_train_loss,\n",
    "                    'val_loss': avg_val_loss,\n",
    "                    'metric_value': current_metric,\n",
    "                    'threshold': best_thr\n",
    "                }, checkpoint_path)\n",
    "\n",
    "            # Update best model if improved\n",
    "            if current_metric > best_valid_metric:\n",
    "                best_valid_metric = current_metric\n",
    "                best_threshold = best_thr\n",
    "                best_weights = deepcopy(model.state_dict())\n",
    "                if models_dir:\n",
    "                    best_model_path = Path(models_dir) / \"best_model.pt\"\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': best_weights,\n",
    "                        'metric_value': best_valid_metric,\n",
    "                        'threshold': best_threshold,\n",
    "                        'metric_name': metric\n",
    "                    }, best_model_path)\n",
    "\n",
    "            # Memory cleanup after validation\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            log_memory_usage(f\"epoch {epoch} end\")\n",
    "\n",
    "        # Update epoch progress bar\n",
    "        if valid_loader is not None:\n",
    "            epoch_pbar.set_postfix({\n",
    "                'train_loss': f'{avg_train_loss:.4f}',\n",
    "                'val_loss': f'{avg_val_loss:.4f}',\n",
    "                f'val_{metric}': f'{current_metric:.4f}'\n",
    "            })\n",
    "\n",
    "    epoch_pbar.close()\n",
    "    \n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Best validation {metric}: {best_valid_metric:.4f} (threshold={best_threshold:.4f})\")\n",
    "    \n",
    "    # Final memory cleanup\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    log_memory_usage(\"training complete\")\n",
    "    \n",
    "    print(f\"✅ Training completed! Processed {epoch_counter} epochs out of {num_epochs} requested\")\n",
    "    \n",
    "    return train_losses, val_losses, best_valid_metric, best_weights, thr_history, ranking_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7201fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress hf warnings\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96a54d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score                 # ← new\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def best_threshold(distances: torch.Tensor,\n",
    "                   labels:    torch.Tensor,\n",
    "                   steps:     int = 200,\n",
    "                   margin:    float = 1.5):\n",
    "    \"\"\"\n",
    "    Sweep `steps` evenly-spaced thresholds between 0 and `margin`\n",
    "    and return the one that maximises duplicate-class F1.\n",
    "    Labels: 0 = duplicate (positive), 1 = different (negative).\n",
    "    \"\"\"\n",
    "    d   = distances.detach().cpu().numpy()\n",
    "    y   = labels.detach().cpu().numpy()\n",
    "    thr = np.linspace(0.0, margin, steps)\n",
    "\n",
    "    best_f1, best_thr = -1.0, 0.0\n",
    "    for t in thr:\n",
    "        y_pred = (d < t).astype(int)          # 1 = duplicate prediction\n",
    "        f1     = f1_score(1 - y, y_pred)      # make 1 = positive for sklearn\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, t\n",
    "    return best_thr, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1b6c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_distribution(train_dataset, val_dataset, test_dataset):\n",
    "    \"\"\"Print the distribution of positives, hard negatives, and soft negatives across all splits.\"\"\"\n",
    "    import pandas as pd\n",
    "    from IPython.display import display\n",
    "\n",
    "    def get_split_stats(dataset, split_name):\n",
    "        total_pairs = len(dataset)\n",
    "        \n",
    "        # Count actual pairs by label\n",
    "        pos_count = sum(1 for pair in dataset.pairs if pair['label'] == 0)\n",
    "        neg_count = sum(1 for pair in dataset.pairs if pair['label'] == 1)\n",
    "        \n",
    "        # Split negatives into hard and soft by counting in pairs\n",
    "        hard_neg_count = 0\n",
    "        soft_neg_count = 0\n",
    "        for pair in dataset.pairs:\n",
    "            if pair['label'] == 1:  # it's a negative pair\n",
    "                sku_second = pair['sku_second']\n",
    "                # Check if this negative is in hard negatives of the query\n",
    "                query_row = dataset.split_df[dataset.split_df['sku_query'] == pair['sku_first']].iloc[0]\n",
    "                if sku_second in query_row['sku_hard_neg']:\n",
    "                    hard_neg_count += 1\n",
    "                else:\n",
    "                    soft_neg_count += 1\n",
    "        \n",
    "        return {\n",
    "            'Split': split_name,\n",
    "            'Total Pairs': f\"{total_pairs:,}\",\n",
    "            'Positives': f\"{pos_count:,} ({(pos_count/total_pairs*100):.1f}%)\",\n",
    "            'Hard Negatives': f\"{hard_neg_count:,} ({(hard_neg_count/total_pairs*100):.1f}%)\",\n",
    "            'Soft Negatives': f\"{soft_neg_count:,} ({(soft_neg_count/total_pairs*100):.1f}%)\"\n",
    "        }\n",
    "\n",
    "    # Create distribution table\n",
    "    stats = []\n",
    "    stats.append(get_split_stats(train_dataset, \"Training\"))\n",
    "    stats.append(get_split_stats(val_dataset, \"Validation\"))\n",
    "    stats.append(get_split_stats(test_dataset, \"Testing\"))\n",
    "    \n",
    "    # Create and display DataFrame\n",
    "    df = pd.DataFrame(stats)\n",
    "    df = df.set_index('Split')\n",
    "    \n",
    "    print(\"\\nDataset Distribution Statistics:\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea10a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import random\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "def _run(train_df, val_df, test_df, source_df, cluster_emb_table, images_dir,\n",
    "         # Model params \n",
    "         model_name='DeepPavlov/distilrubert-tiny-cased-conversational-v1',\n",
    "         description_model_name='cointegrated/rubert-tiny',\n",
    "         batch_size=1, num_epochs=10, learning_rate=1e-4,\n",
    "         # Dataset sampling params\n",
    "         n_pos=None,\n",
    "         n_hard_neg=5,\n",
    "         n_soft_neg=5,\n",
    "         use_all_pos=True,\n",
    "         cluster_sampling=True,\n",
    "         n_soft_neg_per_cluster=1,\n",
    "         # Other params\n",
    "         mlflow_tracking_uri=None, \n",
    "         experiment_name=None, \n",
    "         SMOKE_TEST_BATCHES=None,\n",
    "         optimize_for_ranking=False, k_values=[1, 3, 5, 10]\n",
    "         ):\n",
    "    \"\"\"\n",
    "    Memory-optimized training pipeline with comprehensive cleanup.\n",
    "    \"\"\"\n",
    "    \n",
    "    def log_memory_usage(stage):\n",
    "        process = psutil.Process()\n",
    "        memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "        print(f\"Memory usage at {stage}: {memory_mb:.1f} MB\")\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    if SMOKE_TEST_BATCHES is not None:\n",
    "        print(f\"🔥 SMOKE TEST MODE: Limited to {SMOKE_TEST_BATCHES} batches per phase\")\n",
    "    \n",
    "    log_memory_usage(\"start\")\n",
    "    \n",
    "    # ==================== DATASET CREATION ====================\n",
    "    print(\"Creating datasets...\")\n",
    "\n",
    "    train_dataset = ClusterBasedPairwiseDataset(\n",
    "        split_df=train_df,\n",
    "        source_df=source_df,\n",
    "        cluster_emb_table=cluster_emb_table,\n",
    "        images_dir=images_dir,\n",
    "        n_pos=n_pos,\n",
    "        n_hard_neg=n_hard_neg,\n",
    "        n_soft_neg=n_soft_neg,\n",
    "        use_all_pos=use_all_pos,\n",
    "        cluster_sampling=cluster_sampling,\n",
    "        n_soft_neg_per_cluster=n_soft_neg_per_cluster,\n",
    "        lazy_loading=False\n",
    "    )\n",
    "\n",
    "    val_dataset = ClusterBasedPairwiseDataset(\n",
    "        split_df=val_df,\n",
    "        source_df=source_df,\n",
    "        cluster_emb_table=cluster_emb_table,\n",
    "        images_dir=images_dir,\n",
    "        n_pos=n_pos,\n",
    "        n_hard_neg=n_hard_neg,\n",
    "        n_soft_neg=n_soft_neg,\n",
    "        use_all_pos=use_all_pos,\n",
    "        cluster_sampling=cluster_sampling,\n",
    "        n_soft_neg_per_cluster=n_soft_neg_per_cluster,\n",
    "        lazy_loading=False\n",
    "    )\n",
    "\n",
    "    test_dataset = ClusterBasedPairwiseDataset(\n",
    "        split_df=test_df,\n",
    "        source_df=source_df,\n",
    "        cluster_emb_table=cluster_emb_table,\n",
    "        images_dir=images_dir,\n",
    "        n_pos=n_pos,\n",
    "        n_hard_neg=n_hard_neg,\n",
    "        n_soft_neg=n_soft_neg,\n",
    "        use_all_pos=use_all_pos,\n",
    "        cluster_sampling=cluster_sampling,\n",
    "        n_soft_neg_per_cluster=n_soft_neg_per_cluster,\n",
    "        lazy_loading=False\n",
    "    )\n",
    "\n",
    "    # Print distribution statistics\n",
    "    print_dataset_distribution(train_dataset, val_dataset, test_dataset)\n",
    "    \n",
    "    # ==================== DATALOADER SETUP ====================\n",
    "    print(\"Creating dataloaders...\")\n",
    "    # Create balanced training loader\n",
    "    train_loader = create_balanced_train_loader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    print(f\"DataLoader info:\")\n",
    "    print(f\"  Training batches: {len(train_loader)} (batch_size={batch_size})\")\n",
    "    print(f\"  Validation batches: {len(val_loader)} (batch_size={batch_size})\")\n",
    "    print(f\"  Testing batches: {len(test_loader)} (batch_size={batch_size})\")\n",
    "    \n",
    "    # ==================== MODEL & OPTIMIZER ====================\n",
    "    print(\"Initializing model and optimizer...\")\n",
    "    \n",
    "    model = SiameseRuCLIP(\n",
    "        device=device,\n",
    "        name_model_name=model_name,\n",
    "        description_model_name=description_model_name\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = ContrastiveLoss(margin=1.5)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if mlflow_tracking_uri:\n",
    "        mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        mlflow.log_params({\n",
    "            'batch_size': batch_size,\n",
    "            'num_epochs': num_epochs,\n",
    "            'learning_rate': learning_rate,\n",
    "            'model_name': model_name,\n",
    "            'description_model_name': description_model_name,\n",
    "            'smoke_test_batches': SMOKE_TEST_BATCHES\n",
    "        })\n",
    "\n",
    "    log_memory_usage(\"after model creation\")\n",
    "    \n",
    "    # ==================== TRAINING ====================\n",
    "    print(\"Starting training...\")\n",
    "    # Print batch limitation message once after training\n",
    "    if SMOKE_TEST_BATCHES is not None:\n",
    "        print(f'🚨 Training was limited to {SMOKE_TEST_BATCHES} batches per epoch')\n",
    "\n",
    "    # Choose optimization metric\n",
    "    if optimize_for_ranking:\n",
    "        optimization_metric = 'map'  # Optimize for Mean Average Precision\n",
    "    else:\n",
    "        optimization_metric = 'f1'\n",
    "    \n",
    "    # Train with ranking metrics every 5 epochs\n",
    "    train_losses, val_losses, best_metric, best_weights, thr_history, ranking_history = train_with_smart_ranking(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        num_epochs=num_epochs,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=val_loader,\n",
    "        device=device,\n",
    "        print_epoch=True,\n",
    "        metric='f1',  # or 'map' for ranking optimization\n",
    "        limit_batches=SMOKE_TEST_BATCHES,\n",
    "        compute_ranking_every_n_epochs=1,  # Compute ranking every 2 epochs\n",
    "        k_values=[1, 3, 5, 10, 20]\n",
    "    )\n",
    "    \n",
    "    # ==================== SMART TEST EVALUATION ====================\n",
    "    print(\"\\nRunning final test evaluation with fast ranking metrics...\")\n",
    "    if best_weights is not None:\n",
    "        model.load_state_dict(best_weights)\n",
    "\n",
    "    # 🔥 FIXED: Use the same approach as training/validation\n",
    "    if SMOKE_TEST_BATCHES is not None:\n",
    "        print(f\"🔥 Test evaluation limited to {SMOKE_TEST_BATCHES} batches\")\n",
    "        # Use validation_with_embedding_collection for consistency\n",
    "        pos_acc, neg_acc, avg_acc, test_f1, test_loss, threshold, test_metrics = validation_with_embedding_collection(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            data_loader=test_loader,\n",
    "            epoch='test',\n",
    "            device=device,\n",
    "            split_name='test',\n",
    "            metric='f1',\n",
    "            limit_batches=SMOKE_TEST_BATCHES,  # ← This properly limits batches\n",
    "            collect_for_ranking=True,  # Get ranking metrics\n",
    "            k_values=k_values\n",
    "        )\n",
    "    else:\n",
    "        # For full evaluation, use the comprehensive function\n",
    "        test_metrics = fast_comprehensive_evaluation(model, test_loader, device, k_values=k_values)\n",
    "        # Extract individual metrics from test_metrics for consistency\n",
    "        pos_acc = test_metrics.get('precision', 0)\n",
    "        neg_acc = test_metrics.get('specificity', 0) \n",
    "        avg_acc = test_metrics.get('balanced_accuracy', 0)\n",
    "        test_f1 = test_metrics.get('f1_score', 0)\n",
    "        test_loss = 0.0  # Not computed in comprehensive evaluation\n",
    "        threshold = test_metrics.get('optimal_threshold', 0.5)\n",
    "\n",
    "    # Print test results\n",
    "    print(\"\\nFinal Test Results:\")\n",
    "    print(f\"  Test F1: {test_f1:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"  Test Positive Accuracy: {pos_acc:.4f}\")\n",
    "    print(f\"  Test Negative Accuracy: {neg_acc:.4f}\")\n",
    "    print(f\"  Test Average Accuracy: {avg_acc:.4f}\")\n",
    "    print(f\"  Best Threshold: {threshold:.4f}\")\n",
    "    \n",
    "    # Print ranking metrics if available\n",
    "    if test_metrics.get('mean_average_precision', 0) > 0:\n",
    "        print(f\"  Mean Average Precision: {test_metrics['mean_average_precision']:.4f}\")\n",
    "        for k in [1, 3, 5]:\n",
    "            if k in test_metrics.get('precision_at_k', {}):\n",
    "                p_k = test_metrics['precision_at_k'][k]\n",
    "                r_k = test_metrics['recall_at_k'][k]\n",
    "                print(f\"  P@{k}: {p_k:.3f}, R@{k}: {r_k:.3f}\")\n",
    "\n",
    "    # Final cleanup\n",
    "    del test_loader, model, optimizer, criterion\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    log_memory_usage(\"final\")\n",
    "    \n",
    "    # Log final metrics to MLflow if enabled\n",
    "    if mlflow_tracking_uri:\n",
    "        mlflow.log_metrics({\n",
    "            'test_loss': test_loss,\n",
    "            'test_f1': test_f1,\n",
    "            'test_pos_acc': pos_acc,\n",
    "            'test_neg_acc': neg_acc,\n",
    "            'test_avg_acc': avg_acc,\n",
    "            'test_threshold': threshold,\n",
    "            'test_map': test_metrics.get('mean_average_precision', 0.0)\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'best_valid_metric': best_metric,\n",
    "        'test_f1': test_f1,\n",
    "        'test_loss': test_loss,\n",
    "        'test_pos_acc': pos_acc,\n",
    "        'test_neg_acc': neg_acc,\n",
    "        'test_avg_acc': avg_acc,\n",
    "        'test_threshold': threshold,\n",
    "        'test_metrics': test_metrics,\n",
    "        'ranking_history': ranking_history,\n",
    "        'k_values': k_values\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d32ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "🔥 SMOKE TEST MODE: Limited to 10 batches per phase\n",
      "Memory usage at start: 5940.1 MB\n",
      "Creating datasets...\n",
      "\n",
      "Dataset Distribution Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Pairs</th>\n",
       "      <th>Positives</th>\n",
       "      <th>Hard Negatives</th>\n",
       "      <th>Soft Negatives</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Split</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training</th>\n",
       "      <td>13,903</td>\n",
       "      <td>803 (5.8%)</td>\n",
       "      <td>3,492 (25.1%)</td>\n",
       "      <td>9,608 (69.1%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>6,017</td>\n",
       "      <td>130 (2.2%)</td>\n",
       "      <td>514 (8.5%)</td>\n",
       "      <td>5,373 (89.3%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Testing</th>\n",
       "      <td>5,966</td>\n",
       "      <td>130 (2.2%)</td>\n",
       "      <td>515 (8.6%)</td>\n",
       "      <td>5,321 (89.2%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Total Pairs   Positives Hard Negatives Soft Negatives\n",
       "Split                                                           \n",
       "Training        13,903  803 (5.8%)  3,492 (25.1%)  9,608 (69.1%)\n",
       "Validation       6,017  130 (2.2%)     514 (8.5%)  5,373 (89.3%)\n",
       "Testing          5,966  130 (2.2%)     515 (8.6%)  5,321 (89.2%)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataloaders...\n",
      "DataLoader info:\n",
      "  Training batches: 2781 (batch_size=5)\n",
      "  Validation batches: 1204 (batch_size=5)\n",
      "  Testing batches: 1194 (batch_size=5)\n",
      "Initializing model and optimizer...\n",
      "Memory usage at after model creation: 5941.6 MB\n",
      "Starting training...\n",
      "🚨 Training was limited to 10 batches per epoch\n",
      "🔄 Starting training for 2 epochs\n",
      "Memory usage at training start: 5941.6 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4500b52feeb4456db255f32a7047b584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/2 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔸 Processing epoch 1/2 (counter: 1)\n",
      "Memory usage at epoch 1 start: 5941.6 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a648d251e29413bbbb41a8390aa6c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 - Training:   0%|          | 0/10 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage at epoch 1 after training: 6030.3 MB\n",
      "Computing validation metrics (ranking: True)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6e4c1533d94bf8b2788ab5190e6308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/10 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[validation] Epoch 1 – loss: nan, P Acc: 0.000, N Acc: 1.000, Avg Acc: 0.500, F1: 0.000, thr*: 0.000 (optimised: f1)\n",
      "Epoch 1/2 - Train Loss: nan, Val Loss: nan, Val f1: 0.0000\n",
      "  Ranking Metrics - MAP: 1.0000\n",
      "  P@1: 1.000, R@1: 1.000\n",
      "  P@3: 0.000, R@3: 0.000\n",
      "  P@5: 0.000, R@5: 0.000\n",
      "Memory usage at epoch 1 end: 6030.3 MB\n",
      "🔸 Processing epoch 2/2 (counter: 2)\n",
      "Memory usage at epoch 2 start: 6030.3 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd777f1b17648918e7aa012a4c5a060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 - Training:   0%|          | 0/10 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract the splits\n",
    "train_df = splits_dataset['train']\n",
    "val_df = splits_dataset['val']\n",
    "test_df = splits_dataset['test']\n",
    "\n",
    "images_dir = DATA_PATH + IMG_DATASET_NAME\n",
    "results = _run(\n",
    "    # Required data params\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    test_df=test_df,\n",
    "    source_df=source_df,\n",
    "    cluster_emb_table=cluster_emb_table,\n",
    "    images_dir=images_dir,\n",
    "    \n",
    "    # Model params\n",
    "    model_name=NAME_MODEL_NAME,\n",
    "    description_model_name=DESCRIPTION_MODEL_NAME,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=EPOCHS,\n",
    "    \n",
    "    # # Dataset sampling params - example balanced configuration\n",
    "    # n_pos=100,                    # Sample 10 positives\n",
    "    # n_hard_neg=100,              # 10 hard negatives\n",
    "    # n_soft_neg=100,              # 10 soft negatives\n",
    "    # use_all_pos=False,          # Use fixed number instead of all\n",
    "    # cluster_sampling=True,       # Sample from clusters\n",
    "    # n_soft_neg_per_cluster=1,   # Take 1 from each cluster\n",
    "\n",
    "    # Let training use all available data, limit val/test\n",
    "    n_pos=None,                 # Use all positives available\n",
    "    n_hard_neg=None,           # Use all hard negatives available  \n",
    "    n_soft_neg=None,           # Use all soft negatives available\n",
    "    use_all_pos=True,          # Use all positives\n",
    "    cluster_sampling=True,\n",
    "    n_soft_neg_per_cluster=1,\n",
    "    \n",
    "    # Other params\n",
    "    SMOKE_TEST_BATCHES=SMOKE_TEST_BATCHES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a6fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_TO_WANDB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# TABLE_DATASET_FILE = 'new_labeled.csv'\n",
    "# IMG_DATASET_NAME = 'images_7k'\n",
    "\n",
    "TABLE_DATASET_FILE = 'WB_OZ_100.csv'\n",
    "IMG_DATASET_NAME = 'images_WB_OZ_100'\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"siamese\"\n",
    "\n",
    "model_configs = [\n",
    "    dict(\n",
    "        MODEL_CKPT = 'siamese_fitted_10epochs_bert_turbo.pt',\n",
    "        NAME_MODEL_NAME = 'DeepPavlov/distilrubert-tiny-cased-conversational-v1',\n",
    "        DESCRIPTION_MODEL_NAME = 'sergeyzh/rubert-tiny-turbo',\n",
    "    ),\n",
    "    \n",
    "    dict(\n",
    "        MODEL_CKPT = 'siamese_fitted_10epochs_bert_tiny.pt',\n",
    "        NAME_MODEL_NAME = 'DeepPavlov/distilrubert-tiny-cased-conversational-v1',\n",
    "        DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny',\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log into services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import dotenv\n",
    "except ImportError:\n",
    "    !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Use tokens from .env\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import huggingface_hub\n",
    "import wandb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "huggingface_hub.login(token=HF_TOKEN)\n",
    "\n",
    "if LOG_TO_WANDB:\n",
    "    WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "    wandb.login(key=WANDB_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "from timm import create_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "# from torchinfo import summary\n",
    "import transformers\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer,\\\n",
    "        get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import json\n",
    "from itertools import product\n",
    "\n",
    "# import datasets\n",
    "# from datasets import Dataset, concatenate_datasets\n",
    "import argparse\n",
    "import requests\n",
    "\n",
    "from io import BytesIO\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import more_itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RuCLIPtiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuCLIPtiny(nn.Module):\n",
    "    def __init__(self, name_model_name: str):\n",
    "        \"\"\"\n",
    "        Initializes the RuCLIPtiny module using the provided name model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.visual = create_model('convnext_tiny',\n",
    "                                   pretrained=False,  # set True if you want pretrained weights\n",
    "                                   num_classes=0,\n",
    "                                   in_chans=3)       # output: e.g. 768-dim features\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(name_model_name)\n",
    "        name_model_output_size = self.transformer.config.hidden_size  # inferred dynamically\n",
    "        self.final_ln = nn.Linear(name_model_output_size, 768)         # project to 768 dims\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * torch.log(torch.tensor(1/0.07)))\n",
    "    \n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.stem[0].weight.dtype\n",
    "\n",
    "    def encode_image(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # use the CLS token (first token)\n",
    "        x = x.last_hidden_state[:, 0, :]\n",
    "        x = self.final_ln(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, image: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(input_ids, attention_mask)\n",
    "        # Normalize features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        return logits_per_image, logits_per_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        lambda image: image.convert(\"RGB\"),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "class Tokenizers:\n",
    "    def __init__(self, name_model_name: str, description_model_name: str):\n",
    "        self.name_tokenizer = AutoTokenizer.from_pretrained(name_model_name)\n",
    "        self.desc_tokenizer = AutoTokenizer.from_pretrained(description_model_name)\n",
    "\n",
    "    def tokenize_name(self, texts, max_len=77):\n",
    "        tokenized = self.name_tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "\n",
    "    def tokenize_description(self, texts, max_len=77):\n",
    "        tokenized = self.desc_tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseRuCLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir: str, name_model_name: str, description_model_name: str, df=None, labels=None, df_path=None):\n",
    "        \"\"\"\n",
    "        Dataset requires the concrete models' names for tokenization.\n",
    "        \"\"\"\n",
    "        assert os.path.isdir(images_dir), f\"Image dir does not exist: '{self.images_dir}'\"\n",
    "\n",
    "        self.df = pd.read_csv(df_path) if df_path is not None else df\n",
    "\n",
    "        assert not df.image_name_first.isna().any()\n",
    "        assert not df.image_name_second.isna().any()\n",
    "\n",
    "        self.labels = labels\n",
    "        self.images_dir = images_dir\n",
    "        self.tokenizers = Tokenizers(name_model_name, description_model_name)\n",
    "        self.transform = get_transform()\n",
    "        self.max_len = 77\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Tokenize names\n",
    "        name_tokens = self.tokenizers.tokenize_name([str(row.name_first), str(row.name_second)], max_len=self.max_len)\n",
    "        name_first = name_tokens[:, 0, :]  # [input_ids, attention_mask]\n",
    "        name_second = name_tokens[:, 1, :]\n",
    "        # Tokenize descriptions\n",
    "        desc_tokens = self.tokenizers.tokenize_description([str(row.description_first), str(row.description_second)])\n",
    "        desc_first = desc_tokens[:, 0, :]\n",
    "        desc_second = desc_tokens[:, 1, :]\n",
    "        # Process images\n",
    "        im_first_path = os.path.join(self.images_dir, row.image_name_first)\n",
    "        im_first = cv2.imread(im_first_path)\n",
    "        im_first = cv2.cvtColor(im_first, cv2.COLOR_BGR2RGB)\n",
    "        im_first = Image.fromarray(im_first)\n",
    "        im_first = self.transform(im_first)\n",
    "        im_second_path = os.path.join(self.images_dir, row.image_name_first)\n",
    "        im_second = cv2.imread(os.path.join(im_second_path))\n",
    "        im_second = cv2.cvtColor(im_second, cv2.COLOR_BGR2RGB)\n",
    "        im_second = Image.fromarray(im_second)\n",
    "        im_second = self.transform(im_second)\n",
    "        label = self.labels[idx]\n",
    "        return im_first, name_first, desc_first, im_second, name_second, desc_second, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SiameseRuCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "class SiameseRuCLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 device: str,\n",
    "                 name_model_name: str,\n",
    "                 description_model_name: str,\n",
    "                 models_dir: str = None,\n",
    "                 preload_ruclip: bool = False,\n",
    "                 preload_model_name: str = None):\n",
    "        \"\"\"\n",
    "        Initializes the SiameseRuCLIP model.\n",
    "        Required parameters:\n",
    "          - models_dir: directory containing saved checkpoints.\n",
    "          - name_model_name: model name for text (name) branch.\n",
    "          - description_model_name: model name for description branch.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Convert the device string to torch.device.\n",
    "        device = torch.device(device)\n",
    "\n",
    "        # Initialize RuCLIPtiny with the provided name model.\n",
    "        self.ruclip = RuCLIPtiny(name_model_name)\n",
    "        if preload_ruclip:\n",
    "            std = torch.load(\n",
    "                os.path.join(models_dir, preload_model_name),\n",
    "                weights_only=True,\n",
    "                map_location=device\n",
    "            )\n",
    "            self.ruclip.load_state_dict(std)\n",
    "            self.ruclip.eval()\n",
    "        # Explicitly move ruclip to the device.\n",
    "        self.ruclip = self.ruclip.to(device)\n",
    "        \n",
    "        # Initialize description transformer with the provided description model and move it.\n",
    "        self.description_transformer = AutoModel.from_pretrained(description_model_name)\n",
    "        self.description_transformer = self.description_transformer.to(device)\n",
    "        \n",
    "        # Infer dimensions automatically from inner modules.\n",
    "        vision_dim = self.ruclip.visual.num_features           # e.g., 768 from ConvNeXt tiny\n",
    "        name_dim = self.ruclip.final_ln.out_features             # e.g., 768 after projection\n",
    "        desc_dim = self.description_transformer.config.hidden_size  # e.g., 312 for cointegrated/rubert-tiny\n",
    "        per_product_dim = vision_dim + name_dim + desc_dim        # e.g., 768+768+312 = 1848\n",
    "        head_input_dim = 2 * per_product_dim                     # for a pair of products\n",
    "        \n",
    "        self.hidden_dim = per_product_dim\n",
    "        \n",
    "        # Build the MLP head and move it explicitly.\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(head_input_dim, head_input_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(head_input_dim // 2, head_input_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(head_input_dim // 4, 2)\n",
    "        ).to(device)\n",
    "        \n",
    "    def encode_description(self, desc):\n",
    "        # desc: [input_ids, attention_mask]\n",
    "        out = self.description_transformer(desc[:, 0, :], desc[:, 1, :])\n",
    "        last_hidden = out.last_hidden_state\n",
    "        attention_mask = desc[:, 1, :]\n",
    "        # Average pooling over token representations.\n",
    "        return (last_hidden * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    def forward(self, im1, name1, desc1, im2, name2, desc2):\n",
    "        image_emb1 = self.ruclip.encode_image(im1)\n",
    "        image_emb2 = self.ruclip.encode_image(im2)\n",
    "        name_emb1 = self.ruclip.encode_text(name1[:, 0, :], name1[:, 1, :])\n",
    "        name_emb2 = self.ruclip.encode_text(name2[:, 0, :], name2[:, 1, :])\n",
    "        desc_emb1 = self.encode_description(desc1)\n",
    "        desc_emb2 = self.encode_description(desc2)\n",
    "        first_emb = torch.cat([image_emb1, name_emb1, desc_emb1], dim=1)\n",
    "        second_emb = torch.cat([image_emb2, name_emb2, desc_emb2], dim=1)\n",
    "        x = torch.cat([first_emb, second_emb], dim=1)\n",
    "        out = self.head(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7e840f948a46bb90e80ce2b51d27fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1758.00s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    }
   ],
   "source": [
    "# Download models' weights & text/image datasets\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ID = \"INDEEPA/clip-siamese\"\n",
    "LOCAL_DIR = Path(\"data/train_results\")\n",
    "LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type='dataset',\n",
    "    local_dir='data',\n",
    "    allow_patterns=[\n",
    "        \"train_results/siamese_fitted*.pt\",\n",
    "        TABLE_DATASET_FILE,\n",
    "        f\"{IMG_DATASET_NAME}.zip\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "!unzip -n -q data/{IMG_DATASET_NAME}.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "labeled = pd.read_csv(DATA_PATH + TABLE_DATASET_FILE)\n",
    "images_dir = DATA_PATH + IMG_DATASET_NAME\n",
    "\n",
    "y = labeled.label.values\n",
    "X = labeled.drop(columns='label').copy()\n",
    "\n",
    "def load_data(model_config):\n",
    "    test_ds = SiameseRuCLIPDataset(\n",
    "        images_dir,\n",
    "        model_config['NAME_MODEL_NAME'], \n",
    "        model_config['DESCRIPTION_MODEL_NAME'], \n",
    "        X, y\n",
    "    )\n",
    "    test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "    return test_dl\n",
    "\n",
    "# model_config = model_configs[1]\n",
    "# test_dl = load_data(model_config)\n",
    "# im1, name1, desc1, im2, name2, desc2, label = next(iter(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def load_model(model_config):\n",
    "    ckpt_name = model_config['MODEL_CKPT']\n",
    "    model_ckpt_path = Path(DATA_PATH) / 'train_results' / ckpt_name\n",
    "    std = torch.load(model_ckpt_path, map_location=DEVICE)\n",
    "\n",
    "    # Initialize the model using the configuration.\n",
    "    model = SiameseRuCLIP(\n",
    "        name_model_name=model_config[\"NAME_MODEL_NAME\"],\n",
    "        description_model_name=model_config[\"DESCRIPTION_MODEL_NAME\"],\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(std)\n",
    "    return model\n",
    "\n",
    "# model = load_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation score\n",
    "\n",
    "def validation(model, valid_loader, score, device='cpu', limit_batches=None) -> float:\n",
    "    correct_val = 0\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "        for batch_idx, data in tqdm(enumerate(valid_loader)):\n",
    "            if limit_batches is not None and batch_idx > limit_batches-1:\n",
    "                break\n",
    "\n",
    "            im1, name1, desc1, im2, name2, desc2, label = data \n",
    "            # Move all input tensors to the specified device\n",
    "            im1, name1, desc1, im2, name2, desc2, label = im1.to(device), name1.to(device), desc1.to(device), im2.to(device), name2.to(device), desc2.to(device), label.to(device)  \n",
    "            out = model(im1, name1, desc1, im2, name2, desc2) \n",
    "            _, predicted = torch.max(out.data, -1)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            # Move label to CPU and convert to NumPy array\n",
    "            label = label.cpu().numpy()  # Added this line\n",
    "            correct_val += score(label, predicted)\n",
    "            # break\n",
    "    return correct_val / len(valid_loader)\n",
    "\n",
    "# test_score = validation(model, test_dl, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "def log_to_wandb(model_config, test_score):\n",
    "    wandb.init(\n",
    "        project=\"product-matching\",\n",
    "        entity=\"overfit1010\",\n",
    "        name=f\"test-{model_config['MODEL_CKPT']}\",\n",
    "        config={\n",
    "            \"table_dataset_file\": TABLE_DATASET_FILE,\n",
    "            \"img_dataset_name\": IMG_DATASET_NAME,\n",
    "            \"model_ckpt\": model_config['MODEL_CKPT'],\n",
    "            \"name_model_name\": model_config['NAME_MODEL_NAME'],\n",
    "            \"description_model_name\": model_config['DESCRIPTION_MODEL_NAME'],\n",
    "            \"model_type\": MODEL_TYPE,\n",
    "            \"run_type\": \"test\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Optional: log summary metric\n",
    "    wandb.summary[\"test.f1_score\"] = test_score\n",
    "    wandb.finish()\n",
    "\n",
    "# log_to_wandb(model_config, test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3e34b83253427d8c2ba73f747126c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_ckpt': 'siamese_fitted_10epochs_bert_turbo.pt', 'f1_score': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf936d6cb9634cf2a4362bdb04399d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_ckpt': 'siamese_fitted_10epochs_bert_tiny.pt', 'f1_score': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_ckpt</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>siamese_fitted_10epochs_bert_turbo.pt</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siamese_fitted_10epochs_bert_tiny.pt</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model_ckpt  f1_score\n",
       "0  siamese_fitted_10epochs_bert_turbo.pt       0.0\n",
       "1   siamese_fitted_10epochs_bert_tiny.pt       0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for model_config in model_configs:\n",
    "    model = load_model(model_config)\n",
    "    test_dl = load_data(model_config)\n",
    "    test_score = validation(\n",
    "        model,\n",
    "        test_dl,\n",
    "        f1_score,\n",
    "        # limit_batches=1 # smoke run\n",
    "    )\n",
    "\n",
    "    result = {\n",
    "        'model_ckpt': model_config['MODEL_CKPT'],\n",
    "        'f1_score': test_score\n",
    "    }\n",
    "\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "\n",
    "    if LOG_TO_WANDB:\n",
    "        log_to_wandb(model_config, test_score)\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log into services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import dotenv\n",
    "except ImportError:\n",
    "    !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/anton/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtony-pitchblack\u001b[0m (\u001b[33moverfit1010\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use tokens from .env\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import huggingface_hub\n",
    "import wandb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "huggingface_hub.login(token=HF_TOKEN)\n",
    "\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "wandb.login(key=WANDB_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "from timm import create_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "# from torchinfo import summary\n",
    "import transformers\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer,\\\n",
    "        get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import json\n",
    "from itertools import product\n",
    "\n",
    "# import datasets\n",
    "# from datasets import Dataset, concatenate_datasets\n",
    "import argparse\n",
    "import requests\n",
    "\n",
    "from io import BytesIO\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import more_itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_DATASET_FILE = 'new_labeled.csv'\n",
    "IMG_DATASET_NAME = 'images_7k'\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = [\n",
    "    dict(\n",
    "        MODEL_CKPT = 'siamese_fitted_10epochs_bert_turbo.pt',\n",
    "        NAME_MODEL_NAME = 'DeepPavlov/distilrubert-tiny-cased-conversational-v1',\n",
    "        DESCRIPTION_MODEL_NAME = 'sergeyzh/rubert-tiny-turbo',\n",
    "    ),\n",
    "    \n",
    "    dict(\n",
    "        MODEL_CKPT = 'siamese_fitted_10epochs_bert_tiny.pt',\n",
    "        NAME_MODEL_NAME = 'DeepPavlov/distilrubert-tiny-cased-conversational-v1',\n",
    "        DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny',\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RuCLIPtiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuCLIPtiny(nn.Module):\n",
    "    def __init__(self, name_model_name: str):\n",
    "        \"\"\"\n",
    "        Initializes the RuCLIPtiny module using the provided name model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.visual = create_model('convnext_tiny',\n",
    "                                   pretrained=False,  # set True if you want pretrained weights\n",
    "                                   num_classes=0,\n",
    "                                   in_chans=3)       # output: e.g. 768-dim features\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(name_model_name)\n",
    "        name_model_output_size = self.transformer.config.hidden_size  # inferred dynamically\n",
    "        self.final_ln = nn.Linear(name_model_output_size, 768)         # project to 768 dims\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * torch.log(torch.tensor(1/0.07)))\n",
    "    \n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.stem[0].weight.dtype\n",
    "\n",
    "    def encode_image(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # use the CLS token (first token)\n",
    "        x = x.last_hidden_state[:, 0, :]\n",
    "        x = self.final_ln(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, image: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(input_ids, attention_mask)\n",
    "        # Normalize features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        return logits_per_image, logits_per_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        lambda image: image.convert(\"RGB\"),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "class Tokenizers:\n",
    "    def __init__(self, name_model_name: str, description_model_name: str):\n",
    "        self.name_tokenizer = AutoTokenizer.from_pretrained(name_model_name)\n",
    "        self.desc_tokenizer = AutoTokenizer.from_pretrained(description_model_name)\n",
    "\n",
    "    def tokenize_name(self, texts, max_len=77):\n",
    "        tokenized = self.name_tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "\n",
    "    def tokenize_description(self, texts, max_len=77):\n",
    "        tokenized = self.desc_tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseRuCLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir: str, name_model_name: str, description_model_name: str, df=None, labels=None, df_path=None):\n",
    "        \"\"\"\n",
    "        Dataset requires the concrete models' names for tokenization.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(df_path) if df_path is not None else df\n",
    "        self.labels = labels\n",
    "        self.images_dir = images_dir\n",
    "        self.tokenizers = Tokenizers(name_model_name, description_model_name)\n",
    "        self.transform = get_transform()\n",
    "        self.max_len = 77\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Tokenize names\n",
    "        name_tokens = self.tokenizers.tokenize_name([str(row.name_first), str(row.name_second)], max_len=self.max_len)\n",
    "        name_first = name_tokens[:, 0, :]  # [input_ids, attention_mask]\n",
    "        name_second = name_tokens[:, 1, :]\n",
    "        # Tokenize descriptions\n",
    "        desc_tokens = self.tokenizers.tokenize_description([str(row.description_first), str(row.description_second)])\n",
    "        desc_first = desc_tokens[:, 0, :]\n",
    "        desc_second = desc_tokens[:, 1, :]\n",
    "        # Process images\n",
    "        im_first = cv2.imread(os.path.join(self.images_dir, row.image_name_first))\n",
    "        im_first = cv2.cvtColor(im_first, cv2.COLOR_BGR2RGB)\n",
    "        im_first = Image.fromarray(im_first)\n",
    "        im_first = self.transform(im_first)\n",
    "        im_second = cv2.imread(os.path.join(self.images_dir, row.image_name_second))\n",
    "        im_second = cv2.cvtColor(im_second, cv2.COLOR_BGR2RGB)\n",
    "        im_second = Image.fromarray(im_second)\n",
    "        im_second = self.transform(im_second)\n",
    "        label = self.labels[idx]\n",
    "        return im_first, name_first, desc_first, im_second, name_second, desc_second, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SiameseRuCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        last_hidden = last_hidden_states.masked_fill(\n",
    "            ~attention_mask[..., None].bool(), 0.0\n",
    "        )\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "class SiameseRuCLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 device: str,\n",
    "                 name_model_name: str,\n",
    "                 description_model_name: str,\n",
    "                 models_dir: str = None,\n",
    "                 preload_ruclip: bool = None,\n",
    "                 preload_model_name: str = None):\n",
    "        \"\"\"\n",
    "        Initializes the SiameseRuCLIP model.\n",
    "        Required parameters:\n",
    "          - models_dir: directory containing saved checkpoints.\n",
    "          - name_model_name: model name for text (name) branch.\n",
    "          - description_model_name: model name for description branch.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Initialize RuCLIPtiny with the provided name model.\n",
    "        self.ruclip = RuCLIPtiny(name_model_name)\n",
    "        if preload_ruclip:\n",
    "            std = torch.load(os.path.join(models_dir, preload_model_name),\n",
    "                             weights_only=True,\n",
    "                             map_location=device)\n",
    "            self.ruclip.load_state_dict(std)\n",
    "            self.ruclip = self.ruclip.to(device)\n",
    "            self.ruclip.eval()\n",
    "        # Initialize description transformer with the provided description model.\n",
    "        self.description_transformer = AutoModel.from_pretrained(description_model_name)\n",
    "        \n",
    "        # Infer dimensions automatically from inner modules.\n",
    "        vision_dim = self.ruclip.visual.num_features            # e.g. 768 from ConvNeXt tiny\n",
    "        name_dim = self.ruclip.final_ln.out_features              # e.g. 768 after projection\n",
    "        desc_dim = self.description_transformer.config.hidden_size  # e.g. 312 for cointegrated/rubert-tiny\n",
    "        per_product_dim = vision_dim + name_dim + desc_dim         # total per–product embedding, e.g., 768+768+312 = 1848\n",
    "        head_input_dim = 2 * per_product_dim                      # two products concatenated\n",
    "        \n",
    "        self.hidden_dim = per_product_dim\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(head_input_dim, head_input_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(head_input_dim // 2, head_input_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(head_input_dim // 4, 2)\n",
    "        )\n",
    "    \n",
    "    def encode_description(self, desc):\n",
    "        # desc: [input_ids, attention_mask]\n",
    "        out = self.description_transformer(desc[:, 0, :], desc[:, 1, :])\n",
    "        last_hidden = out.last_hidden_state\n",
    "        attention_mask = desc[:, 1, :]\n",
    "        # Average pooling over token representations.\n",
    "        return (last_hidden * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    def forward(self, im1, name1, desc1, im2, name2, desc2):\n",
    "        image_emb1 = self.ruclip.encode_image(im1)\n",
    "        image_emb2 = self.ruclip.encode_image(im2)\n",
    "        name_emb1 = self.ruclip.encode_text(name1[:, 0, :], name1[:, 1, :])\n",
    "        name_emb2 = self.ruclip.encode_text(name2[:, 0, :], name2[:, 1, :])\n",
    "        desc_emb1 = self.encode_description(desc1)\n",
    "        desc_emb2 = self.encode_description(desc2)\n",
    "        first_emb = torch.cat([image_emb1, name_emb1, desc_emb1], dim=1)\n",
    "        second_emb = torch.cat([image_emb2, name_emb2, desc_emb2], dim=1)\n",
    "        x = torch.cat([first_emb, second_emb], dim=1)\n",
    "        out = self.head(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b27995c0784f62849ab4158d625d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download models' weights & text/image datasets\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ID = \"INDEEPA/clip-siamese\"\n",
    "LOCAL_DIR = Path(\"data/train_results\")\n",
    "LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type='dataset',\n",
    "    local_dir='data',\n",
    "    allow_patterns=[\n",
    "        \"train_results/siamese_fitted*.pt\",\n",
    "        TABLE_DATASET_FILE,\n",
    "        f\"{IMG_DATASET_NAME}.zip\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "!unzip -o -q data/{IMG_DATASET_NAME}.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "labeled = pd.read_csv(DATA_PATH + TABLE_DATASET_FILE)\n",
    "images_dir = DATA_PATH + IMG_DATASET_NAME\n",
    "\n",
    "y = labeled.label.values\n",
    "X = labeled.drop(columns='label').copy()\n",
    "\n",
    "def load_data(model_config):\n",
    "    test_ds = SiameseRuCLIPDataset(\n",
    "        images_dir,\n",
    "        model_config['NAME_MODEL_NAME'], \n",
    "        model_config['DESCRIPTION_MODEL_NAME'], \n",
    "        X, y\n",
    "    )\n",
    "    test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "    return test_dl\n",
    "\n",
    "# model_config = model_configs[1]\n",
    "# test_dl = load_data(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def load_model(model_config):\n",
    "    ckpt_name = model_config['MODEL_CKPT']\n",
    "    model_ckpt_path = Path(DATA_PATH) / 'train_results' / ckpt_name\n",
    "    std = torch.load(model_ckpt_path, map_location=DEVICE)\n",
    "\n",
    "    # Initialize the model using the configuration.\n",
    "    model = SiameseRuCLIP(\n",
    "        name_model_name=model_config[\"NAME_MODEL_NAME\"],\n",
    "        description_model_name=model_config[\"DESCRIPTION_MODEL_NAME\"],\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(std)\n",
    "    return model\n",
    "\n",
    "# model = load_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation score\n",
    "\n",
    "def validation(model, valid_loader, score, device='cpu') -> float:\n",
    "    correct_val = 0\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "        for data in tqdm(valid_loader):\n",
    "            im1, name1, desc1, im2, name2, desc2, label = data \n",
    "            im1, name1, desc1, im2, name2, desc2 = im1.to(device), name1.to(device), desc1.to(device), im2.to(device), name2.to(device), desc2.to(device)\n",
    "            out = model(im1, name1, desc1, im2, name2, desc2) \n",
    "            _, predicted = torch.max(out.data, -1)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            correct_val += score(label, predicted)\n",
    "            # break\n",
    "    return correct_val / len(valid_loader)\n",
    "\n",
    "# test_score = validation(model, test_dl, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "def log_to_wandb(model_config, test_score):\n",
    "    wandb.init(\n",
    "        project=\"product-matching\",\n",
    "        entity=\"overfit1010\",\n",
    "        name=f\"test-{model_config['MODEL_CKPT']}\",\n",
    "        config={\n",
    "            \"table_dataset_file\": TABLE_DATASET_FILE,\n",
    "            \"img_dataset_name\": IMG_DATASET_NAME,\n",
    "            \"model_ckpt\": model_config['MODEL_CKPT'],\n",
    "            \"name_model_name\": model_config['NAME_MODEL_NAME'],\n",
    "            \"description_model_name\": model_config['DESCRIPTION_MODEL_NAME'],\n",
    "            \"model_type\": \"siamese\",\n",
    "            \"run_type\": \"test\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Optional: log summary metric\n",
    "    wandb.summary[\"test.f1_score\"] = test_score\n",
    "    wandb.finish()\n",
    "\n",
    "# log_to_wandb(model_config, test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea49e4fb96514f828cdfe33907849941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/anton/marketplace/clip-siamese/wandb/run-20250401_222228-07pmdbqh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/overfit1010/product-matching/runs/07pmdbqh' target=\"_blank\">test-siamese_fitted_10epochs_bert_turbo.pt</a></strong> to <a href='https://wandb.ai/overfit1010/product-matching' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/overfit1010/product-matching' target=\"_blank\">https://wandb.ai/overfit1010/product-matching</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/overfit1010/product-matching/runs/07pmdbqh' target=\"_blank\">https://wandb.ai/overfit1010/product-matching/runs/07pmdbqh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test.f1_score</td><td>0.00146</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test-siamese_fitted_10epochs_bert_turbo.pt</strong> at: <a href='https://wandb.ai/overfit1010/product-matching/runs/07pmdbqh' target=\"_blank\">https://wandb.ai/overfit1010/product-matching/runs/07pmdbqh</a><br> View project at: <a href='https://wandb.ai/overfit1010/product-matching' target=\"_blank\">https://wandb.ai/overfit1010/product-matching</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250401_222228-07pmdbqh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff10e637033d442b9e5950fb7052652e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/anton/marketplace/clip-siamese/wandb/run-20250401_222235-czwatx2k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/overfit1010/product-matching/runs/czwatx2k' target=\"_blank\">test-siamese_fitted_10epochs_bert_tiny.pt</a></strong> to <a href='https://wandb.ai/overfit1010/product-matching' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/overfit1010/product-matching' target=\"_blank\">https://wandb.ai/overfit1010/product-matching</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/overfit1010/product-matching/runs/czwatx2k' target=\"_blank\">https://wandb.ai/overfit1010/product-matching/runs/czwatx2k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test.f1_score</td><td>0.00161</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test-siamese_fitted_10epochs_bert_tiny.pt</strong> at: <a href='https://wandb.ai/overfit1010/product-matching/runs/czwatx2k' target=\"_blank\">https://wandb.ai/overfit1010/product-matching/runs/czwatx2k</a><br> View project at: <a href='https://wandb.ai/overfit1010/product-matching' target=\"_blank\">https://wandb.ai/overfit1010/product-matching</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250401_222235-czwatx2k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_config in model_configs:\n",
    "    model = load_model(model_config)\n",
    "    test_dl = load_data(model_config)\n",
    "    test_score = validation(model, test_dl, f1_score)\n",
    "    log_to_wandb(model_config, test_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

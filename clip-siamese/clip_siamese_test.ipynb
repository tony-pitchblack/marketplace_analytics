{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "from timm import create_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import transformers\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer,\\\n",
    "        get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import json\n",
    "from itertools import product\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import argparse\n",
    "import requests\n",
    "\n",
    "from io import BytesIO\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_DATASET_NAME = 'new_labeled.csv'\n",
    "IMG_DATASET_NAME = 'images_7k'\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_CKPT = 'siamese_fitted_10epochs_bert_turbo.pt'\n",
    "# NAME_MODEL_NAME = 'DeepPavlov/distilrubert-tiny-cased-conversational-v1'\n",
    "# DESCRIPTION_MODEL_NAME = 'sergeyzh/rubert-tiny-turbo'\n",
    "\n",
    "MODEL_CKPT = 'siamese_fitted_10epochs_bert_tiny.pt'\n",
    "NAME_MODEL_NAME = 'DeepPavlov/distilrubert-tiny-cased-conversational-v1'\n",
    "DESCRIPTION_MODEL_NAME = 'cointegrated/rubert-tiny'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RuCLIPtiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuCLIPtiny(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.visual = create_model('convnext_tiny',\n",
    "                                   pretrained=False, # TODO: берём претрейн\n",
    "                                   num_classes=0,\n",
    "                                   in_chans=3)  # out 768\n",
    "\n",
    "        self.transformer = AutoModel.from_pretrained(NAME_MODEL_NAME)\n",
    "        name_model_output_shape = self.transformer.config.hidden_size  # dynamically get hidden size\n",
    "        self.final_ln = torch.nn.Linear(name_model_output_shape, 768)  # now uses the transformer hidden size\n",
    "        self.logit_scale = torch.nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.stem[0].weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        x = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = x.last_hidden_state[:, 0, :]\n",
    "        x = self.final_ln(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(input_ids, attention_mask)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return logits_per_image, logits_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        _convert_image_to_rgb,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]), ])\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "class Tokenizers:\n",
    "    def __init__(self):\n",
    "        self.name_tokenizer = AutoTokenizer.from_pretrained(NAME_MODEL_NAME)\n",
    "        self.desc_tokenizer = AutoTokenizer.from_pretrained(DESCRIPTION_MODEL_NAME)\n",
    "\n",
    "    def tokenize_name(self, texts, max_len=77):\n",
    "        tokenized = self.name_tokenizer.batch_encode_plus(texts,\n",
    "                                                     truncation=True,\n",
    "                                                     add_special_tokens=True,\n",
    "                                                     max_length=max_len,\n",
    "                                                     padding='max_length',\n",
    "                                                     return_attention_mask=True,\n",
    "                                                     return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])\n",
    "    \n",
    "    def tokenize_description(self, texts, max_len=77):\n",
    "        tokenized = self.desc_tokenizer(texts,\n",
    "                                        truncation=True,\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=max_len,\n",
    "                                        padding='max_length',\n",
    "                                        return_attention_mask=True,\n",
    "                                        return_tensors='pt')\n",
    "        return torch.stack([tokenized[\"input_ids\"], tokenized[\"attention_mask\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseRuCLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df=None, labels=None, df_path=None,\n",
    "        images_dir=DATA_PATH+'images/',\n",
    "    ):\n",
    "        # loads data either from path using `df_path` or directly from `df` argument\n",
    "        self.df = pd.read_csv(df_path) if df_path is not None else df\n",
    "        self.labels = labels\n",
    "        self.images_dir = images_dir\n",
    "        self.tokenizers = Tokenizers()\n",
    "        self.transform = get_transform()\n",
    "        # \n",
    "        self.max_len = 77\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Process names and descriptions\n",
    "        name_tokens = self.tokenizers.tokenize_name([str(row.name_first), \n",
    "                                               str(row.name_second)], max_len=self.max_len)\n",
    "        name_first = name_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        name_second = name_tokens[:, 1, :]\n",
    "        desc_tokens = self.tokenizers.tokenize_description([str(row.description_first), \n",
    "                                               str(row.description_second)])\n",
    "        desc_first = desc_tokens[:, 0, :] # [input_ids, attention_mask]\n",
    "        desc_second = desc_tokens[:, 1, :]\n",
    "\n",
    "        # Process images\n",
    "        im_first_path = os.path.join(self.images_dir, row.image_name_first)\n",
    "        im_first = cv2.imread(im_first_path)\n",
    "        if im_first is None:\n",
    "            raise FileNotFoundError(f\"Image not found at {im_first_path}\")\n",
    "        im_first = cv2.cvtColor(im_first, cv2.COLOR_BGR2RGB)\n",
    "        im_first = Image.fromarray(im_first)\n",
    "        im_first = self.transform(im_first)\n",
    "\n",
    "        im_second_path = os.path.join(self.images_dir, row.image_name_second)\n",
    "        im_second = cv2.imread(im_second_path)\n",
    "        if im_second is None:\n",
    "            raise FileNotFoundError(f\"Image not found at {im_second_path}\")\n",
    "        im_second = cv2.cvtColor(im_second, cv2.COLOR_BGR2RGB)\n",
    "        im_second = Image.fromarray(im_second)\n",
    "        im_second = self.transform(im_second)\n",
    "        label = self.labels[idx]\n",
    "        return im_first, name_first, desc_first, im_second, name_second, desc_second, label\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SiameseRuCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        last_hidden = last_hidden_states.masked_fill(\n",
    "            ~attention_mask[..., None].bool(), 0.0\n",
    "        )\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "class SiameseRuCLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 preload_ruclip=False,\n",
    "                 preload_model_name='cc12m_rubert_tiny_ep_1.pt', # 'cc12m_ddp_4mill_ep_4.pt'\n",
    "                 device='cpu',\n",
    "                 models_dir=DATA_PATH + 'train_results/'):\n",
    "        super().__init__()\n",
    "        self.ruclip = RuCLIPtiny()\n",
    "\n",
    "        if preload_ruclip:\n",
    "            std = torch.load(\n",
    "                models_dir + preload_model_name,\n",
    "                weights_only=True,\n",
    "                map_location=device\n",
    "            )\n",
    "            self.ruclip.load_state_dict(std)\n",
    "            self.ruclip = self.ruclip.to(device)\n",
    "            self.ruclip.eval()\n",
    "\n",
    "        self.description_transformer = AutoModel.from_pretrained(DESCRIPTION_MODEL_NAME)\n",
    "\n",
    "        # Automatically infer dimensions:\n",
    "        # For the vision encoder, we assume the timm model provides 'num_features'\n",
    "        vision_dim = self.ruclip.visual.num_features  # e.g. 768 for convnext_tiny\n",
    "        \n",
    "        # For the name branch, use the output dimension of the final linear layer.\n",
    "        name_dim = self.ruclip.final_ln.out_features   # e.g. 768\n",
    "        \n",
    "        # For the description transformer, take the hidden size from its configuration.\n",
    "        desc_dim = self.description_transformer.config.hidden_size  # e.g. 312 for cointegrated/rubert-tiny\n",
    "        \n",
    "        # Compute the per-product embedding as the concatenation of the three modalities.\n",
    "        per_product_dim = vision_dim + name_dim + desc_dim  # e.g. 768 + 768 + 312 = 1848\n",
    "        head_input_dim = 2 * per_product_dim  # for a pair of products\n",
    "        \n",
    "        self.hidden_dim = per_product_dim  # storing the per-product dimension\n",
    "        \n",
    "        # Build the MLP head that takes concatenated features from two products.\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(head_input_dim, head_input_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(head_input_dim // 2, head_input_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(head_input_dim // 4, 2)\n",
    "        )\n",
    "        \n",
    "    def encode_description(self, desc):\n",
    "        # desc is [input_ids, attention_mask]\n",
    "        last_hidden_states = self.description_transformer(desc[:, 0, :], desc[:, 1, :]).last_hidden_state\n",
    "        attention_mask = desc[:, 1, :]\n",
    "        # TODO: нужно ли делать пулинг, посмотреть на результаты\n",
    "        return average_pool(last_hidden_states, attention_mask)\n",
    "    \n",
    "    def forward(self, im1, name1, desc1, im2, name2, desc2):\n",
    "        image_emb1 = self.ruclip.encode_image(im1)\n",
    "        image_emb2 = self.ruclip.encode_image(im2)\n",
    "        name_emb1 = self.ruclip.encode_text(name1[:, 0, :], name1[:, 1, :])\n",
    "        name_emb2 = self.ruclip.encode_text(name2[:, 0, :], name2[:, 1, :])\n",
    "        desc_emb1 = self.encode_description(desc1) \n",
    "        desc_emb2 = self.encode_description(desc2)\n",
    "        first_emb = torch.cat([image_emb1, name_emb1, desc_emb1], dim=1)\n",
    "        second_emb = torch.cat([image_emb2, name_emb2, desc_emb2], dim=1)\n",
    "        x = torch.cat([first_emb, second_emb], dim=1)\n",
    "        out = self.head(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "model_ckpt_path = Path(DATA_PATH) / 'train_results' / MODEL_CKPT\n",
    "std = torch.load(model_ckpt_path, map_location=DEVICE)\n",
    "model = SiameseRuCLIP()\n",
    "model.load_state_dict(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "labeled = pd.read_csv(DATA_PATH + TABLE_DATASET_NAME)\n",
    "images_dir = DATA_PATH + IMG_DATASET_NAME\n",
    "\n",
    "y = labeled.label.values\n",
    "X = labeled.drop(columns='label').copy()\n",
    "\n",
    "test_ds = SiameseRuCLIPDataset(X, y, images_dir=images_dir)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader, score, device='cpu') -> float:\n",
    "    correct_val = 0\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "        for data in tqdm(valid_loader):\n",
    "            im1, name1, desc1, im2, name2, desc2, label = data \n",
    "            im1, name1, desc1, im2, name2, desc2 = im1.to(device), name1.to(device), desc1.to(device), im2.to(device), name2.to(device), desc2.to(device)\n",
    "            out = model(im1, name1, desc1, im2, name2, desc2) \n",
    "            _, predicted = torch.max(out.data, -1)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            correct_val += score(label, predicted)\n",
    "    return correct_val / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fc151f713444f6b8bc55ffaa322aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_score = validation(model, test_dl, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/anton/product-matching/clip-siamese/wandb/run-20250401_192517-u2lvwdf0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/overfit1010/product-matching/runs/u2lvwdf0' target=\"_blank\">test-siamese_fitted_10epochs_bert_tiny.pt</a></strong> to <a href='https://wandb.ai/overfit1010/product-matching' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/overfit1010/product-matching' target=\"_blank\">https://wandb.ai/overfit1010/product-matching</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/overfit1010/product-matching/runs/u2lvwdf0' target=\"_blank\">https://wandb.ai/overfit1010/product-matching/runs/u2lvwdf0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test.f1_score</td><td>0.00161</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test-siamese_fitted_10epochs_bert_tiny.pt</strong> at: <a href='https://wandb.ai/overfit1010/product-matching/runs/u2lvwdf0' target=\"_blank\">https://wandb.ai/overfit1010/product-matching/runs/u2lvwdf0</a><br> View project at: <a href='https://wandb.ai/overfit1010/product-matching' target=\"_blank\">https://wandb.ai/overfit1010/product-matching</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250401_192517-u2lvwdf0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "wandb.init(\n",
    "    project=\"product-matching\",\n",
    "    entity=\"overfit1010\",\n",
    "    name=f\"test-{MODEL_CKPT}\",\n",
    "    config={\n",
    "        \"table_dataset_name\": TABLE_DATASET_NAME,\n",
    "        \"img_dataset_name\": IMG_DATASET_NAME,\n",
    "        \"model_ckpt\": MODEL_CKPT,\n",
    "        \"name_model_name\": NAME_MODEL_NAME,\n",
    "        \"description_model_name\": DESCRIPTION_MODEL_NAME,\n",
    "        \"model_type\": \"siamese\",\n",
    "        \"run_type\": \"test\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Optional: log summary metric\n",
    "wandb.summary[\"test.f1_score\"] = test_score\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
